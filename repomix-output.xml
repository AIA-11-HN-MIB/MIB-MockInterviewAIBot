This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.env.example
.gitignore
.repomixignore
251108-reference-answer-removal-test-report.md
251108-status-value-error-investigation-report.md
251108-test-rerun-after-fixes-report.md
alembic.ini
alembic/env.py
alembic/README
alembic/script.py.mako
alembic/versions/0001_initial_database_schema_with_all_tables.py
alembic/versions/0002_seed_sample_data.py
alembic/versions/0003_add_planning_and_adaptive_fields.py
alembic/versions/0004_seed_data_for_planning_and_adaptive.py
alembic/versions/0005_drop_reference_answer_column.py
CHANGELOG_ENV.md
CLAUDE.md
DATABASE_SETUP.md
ENV_SETUP.md
pyproject.toml
pytest.ini
quickstart.bat
README.md
requirements/base.txt
requirements/dev.txt
requirements/prod.txt
src/__init__.py
src/adapters/__init__.py
src/adapters/api/__init__.py
src/adapters/api/rest/__init__.py
src/adapters/api/rest/health_routes.py
src/adapters/api/rest/interview_routes.py
src/adapters/api/websocket/__init__.py
src/adapters/api/websocket/connection_manager.py
src/adapters/api/websocket/interview_handler.py
src/adapters/cv_processing/__init__.py
src/adapters/llm/__init__.py
src/adapters/llm/openai_adapter.py
src/adapters/mock/__init__.py
src/adapters/mock/mock_analytics.py
src/adapters/mock/mock_cv_analyzer.py
src/adapters/mock/mock_llm_adapter.py
src/adapters/mock/mock_stt_adapter.py
src/adapters/mock/mock_tts_adapter.py
src/adapters/mock/mock_vector_search_adapter.py
src/adapters/persistence/__init__.py
src/adapters/persistence/answer_repository.py
src/adapters/persistence/candidate_repository.py
src/adapters/persistence/cv_analysis_repository.py
src/adapters/persistence/follow_up_question_repository.py
src/adapters/persistence/interview_repository.py
src/adapters/persistence/mappers.py
src/adapters/persistence/models.py
src/adapters/persistence/question_repository.py
src/adapters/speech/__init__.py
src/adapters/vector_db/__init__.py
src/adapters/vector_db/pinecone_adapter.py
src/application/__init__.py
src/application/dto/answer_dto.py
src/application/dto/interview_dto.py
src/application/dto/websocket_dto.py
src/application/use_cases/__init__.py
src/application/use_cases/analyze_cv.py
src/application/use_cases/complete_interview.py
src/application/use_cases/get_next_question.py
src/application/use_cases/plan_interview.py
src/application/use_cases/process_answer_adaptive.py
src/domain/__init__.py
src/domain/models/__init__.py
src/domain/models/answer.py
src/domain/models/candidate.py
src/domain/models/cv_analysis.py
src/domain/models/follow_up_question.py
src/domain/models/interview.py
src/domain/models/question.py
src/domain/ports/__init__.py
src/domain/ports/analytics_port.py
src/domain/ports/answer_repository_port.py
src/domain/ports/candidate_repository_port.py
src/domain/ports/cv_analysis_repository_port.py
src/domain/ports/cv_analyzer_port.py
src/domain/ports/follow_up_question_repository_port.py
src/domain/ports/interview_repository_port.py
src/domain/ports/llm_port.py
src/domain/ports/question_repository_port.py
src/domain/ports/speech_to_text_port.py
src/domain/ports/text_to_speech_port.py
src/domain/ports/vector_search_port.py
src/domain/services/__init__.py
src/infrastructure/__init__.py
src/infrastructure/config/__init__.py
src/infrastructure/config/settings.py
src/infrastructure/database/__init__.py
src/infrastructure/database/base.py
src/infrastructure/database/session.py
src/infrastructure/dependency_injection/__init__.py
src/infrastructure/dependency_injection/container.py
src/main.py
test_basic.py
tests/__init__.py
tests/conftest.py
tests/integration/api/test_planning_endpoints.py
tests/unit/adapters/test_mock_analytics.py
tests/unit/adapters/test_mock_cv_analyzer.py
tests/unit/domain/test_adaptive_models.py
tests/unit/use_cases/test_plan_interview.py
tests/unit/use_cases/test_process_answer_adaptive.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".repomixignore">
docs/*
plans/*
assets/*
dist/*
coverage/*
build/*
ios/*
android/*

.claude/*
.serena/*
.pnpm-store/*
.github/*
.dart_tool/*
.idea/*
</file>

<file path="251108-reference-answer-removal-test-report.md">
# Test Report: reference_answer Column Removal

**Date**: 2025-11-08
**Tester**: QA Agent
**Test Scope**: Remove redundant `reference_answer` column, use `ideal_answer` instead

---

## Executive Summary

**Status**: âš ï¸ PARTIAL SUCCESS - Migration successful, but mapper and test issues found

### Key Metrics
- **Tests Run**: 87 total
- **Tests Passed**: 67 (77%)
- **Tests Failed**: 9 (10%)
- **Tests Error**: 11 (13%)
- **Coverage**: 27% overall (domain layer tested)

### Critical Issues Found
1. **AnswerMapper missing fields** - `similarity_score` and `gaps` not mapped (TYPE ERROR)
2. **ExtractedSkill alias issue** - Tests using wrong field name `name` instead of `skill`
3. **Answer.has_gaps() logic** - Returns True for empty dict, should check `confirmed` field
4. **Test assertion errors** - 2 domain model tests failing due to gap detection logic

---

## 1. Migration Status

### âœ… Database Migration: SUCCESS

**Migration Applied**: `251108_1200_drop_reference_answer_column.py`

```
INFO  [alembic.runtime.migration] Running upgrade 251106_2300 -> 251108_1200
```

**Current Migration**: `251108_1200` (head)

**Migration Chain**:
```
a4047ce5a909 (initial)
  â†’ 525593eca676 (seed)
    â†’ 251106_2300 (add planning fields)
      â†’ d0078872a49a (seed planning data)
      â†’ 251108_1200 (drop reference_answer) âœ…
```

**Schema Verification**:
```python
QuestionModel fields:
  - created_at: DATETIME
  - difficulty: VARCHAR(50)
  - embedding: ARRAY
  - evaluation_criteria: TEXT
  - id: UUID
  - ideal_answer: TEXT          âœ… Present
  - question_type: VARCHAR(50)
  - rationale: TEXT             âœ… Present
  - skills: ARRAY
  - tags: ARRAY
  - text: TEXT
  - updated_at: DATETIME
  - version: INTEGER

Has reference_answer: False     âœ… Removed
Has ideal_answer: True          âœ…
Has rationale: True             âœ…
```

---

## 2. Code Changes Verification

### âœ… Domain Model (src/domain/models/question.py)

**Status**: CORRECT

```python
# Removed reference_answer field
# Kept ideal_answer and rationale

ideal_answer: str | None = None
rationale: str | None = None
```

**Methods using ideal_answer**:
- `has_ideal_answer()` - checks if ideal_answer exists and has 10+ chars
- `is_planned` property - returns True if has ideal_answer + rationale

### âœ… Database Model (src/adapters/persistence/models.py)

**Status**: CORRECT

```python
# QuestionModel - No reference_answer column
ideal_answer: Mapped[str | None] = mapped_column(Text, nullable=True)  âœ…
rationale: Mapped[str | None] = mapped_column(Text, nullable=True)     âœ…
```

### âœ… QuestionMapper (src/adapters/persistence/mappers.py)

**Status**: CORRECT

All three mapper methods properly handle `ideal_answer` and `rationale`:
- âœ… `to_domain()` - maps ideal_answer, rationale
- âœ… `to_db_model()` - maps ideal_answer, rationale
- âœ… `update_db_model()` - maps ideal_answer, rationale

### âŒ AnswerMapper (src/adapters/persistence/mappers.py)

**Status**: INCOMPLETE - TYPE ERROR

**Issue**: Missing `similarity_score` and `gaps` fields in all mapper methods

**Mypy Error**:
```
src\adapters\persistence\mappers.py:195: error: Missing named argument "similarity_score" for "Answer"  [call-arg]
```

**Root Cause**: Answer domain model has required adaptive fields but mapper doesn't include them:

```python
# Domain Model (Answer)
similarity_score: float | None = Field(None, ge=0.0, le=1.0)
gaps: dict[str, Any] | None = None

# Database Model (AnswerModel)
similarity_score: Mapped[float | None] = mapped_column(Float, nullable=True)  âœ…
gaps: Mapped[dict | None] = mapped_column(JSONB, nullable=True)                âœ…

# Mapper - MISSING in to_domain(), to_db_model(), update_db_model()
# Lines 195-251 need to add these fields
```

**Fix Required**: Add to all three methods in AnswerMapper:
1. `to_domain()` - add `similarity_score=db_model.similarity_score, gaps=dict(db_model.gaps) if db_model.gaps else None`
2. `to_db_model()` - add `similarity_score=domain_model.similarity_score, gaps=domain_model.gaps`
3. `update_db_model()` - add `db_model.similarity_score = domain_model.similarity_score; db_model.gaps = domain_model.gaps`

### âœ… OpenAI Adapter (src/adapters/llm/openai_adapter.py)

**Status**: CORRECT

```python
# Line 107 - using ideal_answer
{"Ideal Answer: " + question.ideal_answer if question.ideal_answer else ""}

# Line 276 - generate_ideal_answer method exists
# Line 330 - ideal_answer parameter used
# Line 349 - ideal_answer in prompt
```

---

## 3. Test Results

### 3.1 Import Tests

**Status**: âœ… PASS

```bash
Import check: OK
```

All modified modules import successfully without errors.

### 3.2 Type Checking (mypy)

**Status**: âš ï¸ WARNINGS + 1 CRITICAL ERROR

**Critical Error**:
```
src\adapters\persistence\mappers.py:195: error: Missing named argument "similarity_score" for "Answer"  [call-arg]
```

**Other Type Warnings** (pre-existing, not related to this change):
- Missing type parameters for generic `dict` types
- Missing return type annotations
- These don't block functionality but should be addressed

### 3.3 Unit Tests

#### âœ… Passing Tests (67 total)

**Integration Tests** (14/14 passed):
- âœ… Planning endpoints
- âœ… Adaptive interview flow
- âœ… Follow-up question delivery
- âœ… Evaluation enhancement
- âœ… Backward compatibility

**Mock Adapter Tests** (21/21 passed):
- âœ… MockAnalytics (13 tests)
- âœ… MockCVAnalyzer (8 tests)

**Domain Model Tests** (11/13 passed):
- âœ… Question adaptive fields (3/3)
- âœ… Interview adaptive fields (3/3)
- âœ… Answer adaptive fields (3/5) - 2 failures
- âœ… FollowUpQuestion (3/3)

#### âŒ Failed Tests (9 total)

**1. Domain Model Test Failures (2 tests)**

**Test**: `test_answer_without_gaps`
**File**: `tests/unit/domain/test_adaptive_models.py:171`
**Status**: FAILED
**Error**: `AssertionError: assert True is False`

```python
# Test code
answer = Answer(
    text="Complete answer",
    gaps={"concepts": [], "confirmed": False},  # Empty concepts array
)
assert answer.has_gaps() is False  # Expected False, got True

# has_gaps() implementation (line 117)
return self.gaps is not None and len(self.gaps) > 0
# Bug: Returns True if dict exists (len=2), should check if concepts array has items OR confirmed=True
```

**Root Cause**: `has_gaps()` checks `len(self.gaps) > 0` which returns True for `{"concepts": [], "confirmed": False}` (dict length = 2 keys).

**Fix Required**: Update logic to check if concepts array has items:
```python
def has_gaps(self) -> bool:
    if self.gaps is None:
        return False
    concepts = self.gaps.get("concepts", [])
    confirmed = self.gaps.get("confirmed", False)
    return len(concepts) > 0 and confirmed
```

**Test**: `test_is_adaptive_complete_no_gaps`
**File**: `tests/unit/domain/test_adaptive_models.py:198`
**Status**: FAILED
**Error**: `AssertionError: assert False is True`

```python
answer = Answer(
    similarity_score=0.75,  # Below 0.8 threshold
    gaps={"concepts": [], "confirmed": False},  # No actual gaps
)
assert answer.is_adaptive_complete() is True  # Should be True (no gaps)

# is_adaptive_complete() logic (line 137)
similarity_ok = self.similarity_score and self.similarity_score >= 0.8  # False
no_gaps = not self.has_gaps()  # False (but should be True)
return similarity_ok or no_gaps  # False or False = False
```

**Root Cause**: Same as above - `has_gaps()` incorrectly returns True for empty gaps.

**2. ExtractedSkill ValidationError (7 tests + 4 errors)**

**Tests Affected**:
- `test_plan_interview_with_2_skills`
- `test_plan_interview_with_4_skills`
- `test_plan_interview_with_7_skills`
- `test_plan_interview_with_10_skills_max_5`
- `test_calculate_n_for_various_skill_counts`
- `test_calculate_n_ignores_experience_years`
- Plus 4 tests in `test_process_answer_adaptive.py`
- Plus errors in `test_plan_interview.py` fixtures

**Error**:
```
pydantic_core._pydantic_core.ValidationError: 1 validation error for ExtractedSkill
skill
  Field required [type=missing, input_value={'name': 'Python', 'category': 'technical', 'proficiency': 'expert'}, input_type=dict]
```

**Root Cause**: ExtractedSkill model uses alias:
```python
# Domain model (line 16)
name: str = Field(alias="skill")  # Expects "skill" key, not "name"
```

**Tests using wrong key**:
```python
# tests/conftest.py:24 and test files
ExtractedSkill(name="Python", category="technical", proficiency="expert")
# Should be:
ExtractedSkill(skill="Python", category="technical", proficiency="expert")
```

**Fix Required**: Update all test fixtures to use `skill` instead of `name`:
- `tests/conftest.py` (sample_cv_analysis fixture)
- `tests/unit/use_cases/test_plan_interview.py` (multiple instances)
- `tests/unit/use_cases/test_process_answer_adaptive.py` (fixtures)

**3. Gap Detection Test Failure (1 test)**

**Test**: `test_keyword_gap_detection`
**File**: `tests/unit/use_cases/test_process_answer_adaptive.py`
**Error**: `AssertionError: assert 8 <= 3`

This test expects max 3 keywords in gaps but got 8. Likely related to LLM output parsing or test expectations.

---

## 4. Coverage Analysis

**Overall Coverage**: 27%

**Domain Layer** (well tested):
- âœ… domain/models: High coverage
- âœ… domain/ports: 100% coverage (interfaces)

**Application Layer** (partially tested):
- âš ï¸ use_cases: Some coverage, but errors blocking full test runs

**Infrastructure Layer** (not tested):
- âŒ config: 0% coverage
- âŒ database: 0% coverage
- âŒ dependency_injection: 0% coverage
- âŒ main.py: 0% coverage

**Note**: Infrastructure typically tested via integration tests, not unit tests.

---

## 5. Grep Analysis: reference_answer Usage

**Files Still Mentioning reference_answer** (8 files found):

1. âœ… `alembic/versions/251108_1200_drop_reference_answer_column.py` - Migration file (expected)
2. âš ï¸ `tests/unit/adapters/test_mock_analytics.py` - Test may need update
3. âœ… `plans/*.md` - Documentation (historical, OK)
4. âœ… `docs/system-architecture.md` - Documentation needs update
5. âœ… `repomix-output.xml` - Generated file (ignore)
6. âœ… `alembic/versions/525593eca676_seed_sample_data.py` - Old migration (OK)
7. âœ… `alembic/versions/a4047ce5a909_initial_database_schema.py` - Old migration (OK)

**Action Required**:
- Check `test_mock_analytics.py` for any `reference_answer` usage
- Update `docs/system-architecture.md` to reflect `ideal_answer`

---

## 6. Performance Metrics

**Test Execution Time**: 1.27 seconds (87 tests)

**Fast Tests**:
- Domain model tests: < 0.5s
- Mock adapter tests: < 0.3s

**Integration Tests**: Slower due to database operations

**No performance degradation** from the migration.

---

## 7. Recommendations

### ðŸ”´ Critical (Must Fix Before Production)

1. **Fix AnswerMapper** (BLOCKS TYPE CHECKING)
   - Add `similarity_score` and `gaps` to all three mapper methods
   - Priority: HIGH
   - Impact: Type errors, potential runtime failures

2. **Fix Answer.has_gaps() Logic** (2 TEST FAILURES)
   - Update to check `concepts` array length and `confirmed` flag
   - Priority: HIGH
   - Impact: Incorrect gap detection, wrong follow-up logic

### ðŸŸ¡ Important (Should Fix Soon)

3. **Fix ExtractedSkill Test Fixtures** (11 TEST ERRORS)
   - Replace `name=` with `skill=` in all test fixtures
   - Priority: MEDIUM
   - Impact: 11 tests blocked, coverage incomplete

4. **Fix Gap Detection Test**
   - Review `test_keyword_gap_detection` expectations
   - Priority: MEDIUM
   - Impact: 1 test failure

### ðŸŸ¢ Optional (Nice to Have)

5. **Update Documentation**
   - `docs/system-architecture.md` - replace `reference_answer` with `ideal_answer`
   - Priority: LOW
   - Impact: Documentation accuracy

6. **Address Type Warnings**
   - Add type parameters for generic `dict` types
   - Add return type annotations
   - Priority: LOW
   - Impact: Code quality, type safety

---

## 8. Unresolved Questions

1. **Migration Heads**: Why are there two migration heads (`d0078872a49a` and `251108_1200`)?
   - Current status shows both as head
   - Should they be merged or is this intentional branching?

2. **Gap Detection Algorithm**: What is the expected behavior for keyword extraction?
   - Test expects <= 3 keywords but got 8
   - Is this a test issue or implementation issue?

3. **Coverage Target**: What is the project's coverage goal?
   - Current: 27%
   - Infrastructure not tested - is this acceptable?

---

## 9. Next Steps

### Immediate Actions (Before Merge)

1. âœ… Migration applied successfully
2. âŒ Fix AnswerMapper (add similarity_score, gaps)
3. âŒ Fix Answer.has_gaps() logic
4. âŒ Fix ExtractedSkill test fixtures
5. âš ï¸ Re-run full test suite
6. âš ï¸ Verify all tests pass

### Post-Merge Actions

1. Update documentation (system-architecture.md)
2. Address type warnings
3. Investigate gap detection test failure
4. Consider increasing test coverage for infrastructure layer

---

## 10. Conclusion

**Migration Status**: âœ… SUCCESS - Database schema updated correctly

**Code Status**: âš ï¸ PARTIAL - Domain model correct, mapper incomplete

**Test Status**: âŒ FAILING - 20 tests blocked by 3 issues:
1. AnswerMapper missing fields (type error)
2. has_gaps() logic incorrect (2 failures)
3. ExtractedSkill fixtures wrong (11 errors, 7 failures)

**Recommendation**: **DO NOT MERGE** until critical fixes applied.

**Estimated Fix Time**: 30-60 minutes for all three issues.

---

**Report Generated**: 2025-11-08 16:30 UTC
**Next Review**: After mapper fixes applied
</file>

<file path="251108-status-value-error-investigation-report.md">
# Investigation Report: status.value AttributeError

**Date:** 2025-11-08
**Investigator:** Debug Agent
**Issue:** AttributeError at `src/adapters/persistence/mappers.py:157`
**Context:** Post-fix for Question model `use_enum_values` removal

---

## Executive Summary

**Root Cause:** Interview model has `use_enum_values = True` (line 48), causing `status` field to serialize as string instead of `InterviewStatus` enum. Mapper code inconsistently calls `.value` on a string.

**Business Impact:**
- Interview creation/update operations fail with AttributeError
- Database persistence layer broken for Interview entity
- API endpoints returning interview data are affected

**Affected Components:**
- Interview domain model (1 instance)
- InterviewMapper (2 locations)
- Interview REST API routes (7 locations)

**Recommended Action:** Remove `use_enum_values = True` from Interview model Config (same fix as Question model)

---

## Technical Analysis

### 1. Root Cause Identification

**Interview Model Config (src/domain/models/interview.py:45-49):**
```python
class Config:
    """Pydantic configuration."""

    use_enum_values = True  # â† PROBLEM
    frozen = False
```

**Effect:** When `use_enum_values = True`, Pydantic automatically converts:
- `InterviewStatus.PREPARING` â†’ `"preparing"` (string)
- `interview.status` becomes `str`, NOT `InterviewStatus` enum

**Mapper Assumption:** Code at line 157 assumes `status` is enum:
```python
status=domain_model.status.value,  # â† Calls .value on string
```

**Error:** `str` has no `.value` attribute â†’ AttributeError

### 2. Pattern Analysis

**Comparison with Question Model Fix:**
- Question model previously had `use_enum_values = True`
- Removed in recent fix â†’ now returns enum types
- Mappers correctly call `.value` on enums
- Interview model NOT updated â†’ still has config flag

**Mapper Inconsistency:**
- Line 157 (`to_db_model`): Uses `domain_model.status.value` âœ—
- Line 171 (`update_db_model`): Uses `domain_model.status` directly âœ“
- Line 140 (`to_domain`): Correctly wraps `InterviewStatus(db_model.status)` âœ“

### 3. Evidence from Codebase

**Domain Models with use_enum_values:**
```bash
$ grep -r "use_enum_values = True" src/domain/models/
src/domain/models/interview.py:48:        use_enum_values = True
```
Only Interview model affected (Question model already fixed).

**All .status.value Usage:**
1. `src/adapters/persistence/mappers.py:157` - InterviewMapper.to_db_model() âœ—
2. `src/adapters/persistence/mappers.py:171` - InterviewMapper.update_db_model() - INCONSISTENT (no .value)
3. `src/adapters/api/rest/interview_routes.py:285` - PlanningStatusResponse âœ—
4. `src/adapters/api/rest/interview_routes.py:329-338` - Status comparisons (6 instances) âœ—
5. `src/adapters/api/rest/interview_routes.py:342` - PlanningStatusResponse âœ—

**Database Model Expectations:**
- SQLAlchemy models store enums as strings in DB
- Mappers must convert enum â†’ string via `.value`
- BUT only when domain model has enum type (NOT when `use_enum_values = True`)

### 4. Timeline of Events

1. **Original State:** Interview model had `use_enum_values = True`, mappers called `.value`
2. **Question Fix:** Question model removed `use_enum_values`, mappers updated
3. **Current State:** Interview model still has flag, mappers still call `.value` â†’ CONFLICT
4. **Error Trigger:** Any interview save/update operation calls line 157

---

## Affected File Locations

### Critical (Breaks Persistence):
1. **src/domain/models/interview.py:48**
   - `use_enum_values = True` in Config
   - Causes status to serialize as string

2. **src/adapters/persistence/mappers.py:157**
   - `status=domain_model.status.value,`
   - Assumes enum, gets string â†’ AttributeError

3. **src/adapters/persistence/mappers.py:171**
   - `db_model.status = domain_model.status`
   - No `.value` call (inconsistent with line 157)

### Secondary (API Layer Issues):
4. **src/adapters/api/rest/interview_routes.py:285**
   - `status=interview.status.value,`
   - Returns PlanningStatusResponse

5. **src/adapters/api/rest/interview_routes.py:329-338**
   - `if interview.status.value == "PREPARING":` (6 comparisons)
   - Status string matching logic

6. **src/adapters/api/rest/interview_routes.py:342**
   - `status=interview.status.value,`
   - Returns PlanningStatusResponse

---

## Solution Approach

### Recommended Fix: Remove use_enum_values (Align with Question Model)

**Primary Change:**
```python
# src/domain/models/interview.py:45-49
class Config:
    """Pydantic configuration."""

    # use_enum_values = True  # â† REMOVE THIS LINE
    frozen = False
```

**Consequences:**
- `interview.status` becomes `InterviewStatus` enum (not string)
- Mapper calls to `.value` become VALID
- API routes calling `.value` become VALID
- Aligns with Question model pattern

**Files Requiring NO Changes:**
- `src/adapters/persistence/mappers.py:157` - Already calls `.value` âœ“
- `src/adapters/api/rest/interview_routes.py` - Already calls `.value` âœ“

**Files Requiring Update:**
- `src/adapters/persistence/mappers.py:171` - Should add `.value`:
  ```python
  db_model.status = domain_model.status.value  # Add .value
  ```

### Alternative Fix: Keep use_enum_values (NOT Recommended)

If keeping `use_enum_values = True`, must REMOVE all `.value` calls:
- Lines: 157, 171, 285, 329-338, 342
- Makes code inconsistent with Question model
- Loses type safety benefits

---

## Supporting Evidence

### Mapper Code Context (Lines 152-179)

```python
@staticmethod
def to_db_model(domain_model: Interview) -> InterviewModel:
    """Convert domain model to database model."""
    return InterviewModel(
        id=domain_model.id,
        candidate_id=domain_model.candidate_id,
        status=domain_model.status.value,  # LINE 157 - ERROR HERE
        cv_analysis_id=domain_model.cv_analysis_id,
        question_ids=domain_model.question_ids,
        answer_ids=domain_model.answer_ids,
        current_question_index=domain_model.current_question_index,
        started_at=domain_model.started_at,
        completed_at=domain_model.completed_at,
        created_at=domain_model.created_at,
        updated_at=domain_model.updated_at,
    )

@staticmethod
def update_db_model(db_model: InterviewModel, domain_model: Interview) -> None:
    """Update database model from domain model."""
    db_model.status = domain_model.status  # LINE 171 - INCONSISTENT (no .value)
    db_model.cv_analysis_id = domain_model.cv_analysis_id
    # ... rest of fields
```

### Question Model Pattern (Post-Fix)

```python
# src/domain/models/question.py:51-54
class Config:
    """Pydantic configuration."""

    pass  # â† No use_enum_values

# src/adapters/persistence/mappers.py:104-105
question_type=domain_model.question_type.value,  # âœ“ Works because enum
difficulty=domain_model.difficulty.value,        # âœ“ Works because enum
```

---

## Risk Assessment

**Immediate Risk (High):**
- All interview create/update operations FAIL
- API endpoints `/interview/plan` and `/{id}/plan` broken
- Database persistence completely blocked

**Fix Risk (Low):**
- Removing `use_enum_values` is safe (proven by Question model fix)
- Existing code already expects enum + `.value` pattern
- Only line 171 needs update (add `.value`)

**Rollback Risk (None):**
- Config change is reversible
- No database migration required (still stores strings)

---

## Actionable Recommendations

### Priority 1 (Critical - Fix Immediately):
1. Remove `use_enum_values = True` from `src/domain/models/interview.py:48`
2. Update `src/adapters/persistence/mappers.py:171`:
   - Change: `db_model.status = domain_model.status`
   - To: `db_model.status = domain_model.status.value`

### Priority 2 (Verification):
3. Run unit tests for InterviewMapper
4. Test interview creation via API
5. Verify status transitions (start, complete, cancel methods)

### Priority 3 (Code Quality):
6. Add type hints to catch this earlier:
   ```python
   def to_db_model(domain_model: Interview) -> InterviewModel:
       status: str = domain_model.status.value  # Explicit type
   ```

### Priority 4 (Prevention):
7. Document enum handling pattern in `docs/code-standards.md`
8. Add linter rule to detect `use_enum_values` in domain models
9. Create test suite for all mappers with enum fields

---

## Unresolved Questions

1. **Migration Path:** Are there any database records with malformed status values?
2. **API Contracts:** Do external clients expect string status or enum object?
3. **Serialization:** How does FastAPI serialize InterviewStatus enums in responses?
4. **Test Coverage:** Why didn't tests catch this inconsistency between line 157 and 171?

---

## Appendix: Full Context

### InterviewStatus Enum Definition
```python
# src/domain/models/interview.py:11-18
class InterviewStatus(str, Enum):
    """Interview status enumeration."""

    PREPARING = "preparing"
    READY = "ready"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
```

### API Route Usage Example
```python
# src/adapters/api/rest/interview_routes.py:329-338
if interview.status.value == "PREPARING":
    message = "Interview planning in progress..."
elif interview.status.value == "READY":
    message = f"Interview ready with {interview.planned_question_count} questions"
# ... etc
```

---

**End of Report**
</file>

<file path="251108-test-rerun-after-fixes-report.md">
# Test Rerun After Critical Fixes - QA Report
**Date**: 2025-11-08
**Reporter**: QA Engineer
**Branch**: feat/EA-6-start-interview

---

## Executive Summary

**Status**: âŒ **PARTIALLY SUCCESSFUL** - 3 original issues fixed, 3 NEW issues discovered

**Test Results**: 69 PASSED / 7 FAILED / 11 ERRORS (87 total)
**Type Checking**: âŒ FAILED (53 mypy errors)
**Pass Rate**: 79.3%

---

## âœ… Original Fixes - SUCCESSFUL

All 3 critical issues from previous report have been resolved:

### 1. AnswerMapper - similarity_score & gaps âœ…
- **Fixed**: Added `similarity_score` and `gaps` to all 3 mapper methods
- **Status**: No mapper-related errors in tests
- **Files**: `src/adapters/persistence/mappers.py`

### 2. Answer.has_gaps() Logic âœ…
- **Fixed**: Changed from `len(self.concepts) >= 5` to `len(self.concepts) < 5`
- **Fixed**: Changed from `not self.confirmed` to `self.confirmed`
- **Status**: Logic now correctly identifies gaps
- **Files**: `src/domain/models/answer.py`

### 3. ExtractedSkill Fixtures âœ…
- **Fixed**: Changed all `name=` to `skill=` in test fixtures
- **Status**: No more Pydantic validation errors for ExtractedSkill
- **Files**: Multiple test files

---

## âŒ NEW Issues Discovered

### CRITICAL Issue 1: Missing similarity_score in process_answer.py

**Location**: `src/application/use_cases/process_answer.py:64`

**Error**:
```
Missing named argument "similarity_score" for "Answer" [call-arg]
```

**Problem**: Legacy `ProcessAnswerUseCase` (non-adaptive) creates Answer without required `similarity_score` field

**Code** (line 64-69):
```python
answer = Answer(
    interview_id=interview_id,
    question_id=question_id,
    candidate_id=interview.candidate_id,
    text=answer_text,
    is_voice=bool(audio_file_path),
    # MISSING: similarity_score=0.0
)
```

**Impact**:
- Type checker fails
- Legacy flow broken (non-adaptive interviews)

**Fix Required**: Add `similarity_score=0.0` parameter

---

### CRITICAL Issue 2: Missing extracted_text in CVAnalysis Fixtures

**Affected Tests**: 11 tests in `test_plan_interview.py` and `test_process_answer_adaptive.py`

**Error**:
```
pydantic_core._pydantic_core.ValidationError: 1 validation error for CVAnalysis
extracted_text
  Field required [type=missing, ...]
```

**Problem**: Test fixtures create CVAnalysis without required `extracted_text` field

**Sample Fixture** (needs fixing):
```python
cv_analysis = CVAnalysis(
    candidate_id=UUID(...),
    extracted_skills=[...],
    education_level="Bachelor's Degree"
    # MISSING: extracted_text="..."
)
```

**Impact**: 11 tests erroring out with validation failures

**Fix Required**: Add `extracted_text="Sample CV text"` to all CVAnalysis fixtures

---

### MAJOR Issue 3: Gap Detection Test Logic Mismatch

**Location**: `tests/unit/use_cases/test_process_answer_adaptive.py::TestGapDetection::test_keyword_gap_detection`

**Error**:
```python
AssertionError: assert 8 <= 3
 +  where 8 = len(['itself.', 'condition.', 'case,', 'calling', 'stack,', 'termination', 'concepts:', 'function'])
```

**Problem**: Test expects â‰¤3 gaps, but implementation returns 8

**Root Cause**: Gap detection extracts punctuation/artifacts as "gaps":
- `'itself.'` â† includes period
- `'case,'` â† includes comma
- `'concepts:'` â† includes colon
- `'stack,'` â† includes comma

**Actual Missing Concepts**: ~3-4 real keywords (termination, calling, stack, function)
**Detected "Gaps"**: 8 tokens (including punctuation artifacts)

**Impact**: Test assertion doesn't match implementation behavior

**Fix Options**:
1. **Fix Implementation**: Strip punctuation from detected gaps
2. **Fix Test**: Adjust assertion to `assert len(gaps) <= 10`
3. **Hybrid**: Improve tokenization + relax test threshold

---

## MINOR Issues

### Type Checking Warnings (53 errors)

**Categories**:
1. **Missing type annotations**: 22 errors (functions without return types)
2. **Generic dict warnings**: 12 errors (use `dict[str, Any]`)
3. **Untyped function calls**: 8 errors
4. **None attribute access**: 7 errors (Pinecone index initialization)
5. **Other**: 4 errors (unions, deprecated Pydantic config)

**Impact**: Non-blocking but reduces type safety

**Files Most Affected**:
- `src/adapters/api/websocket/connection_manager.py` (7 errors)
- `src/adapters/persistence/models.py` (7 errors)
- `src/adapters/vector_db/pinecone_adapter.py` (7 errors)
- `src/infrastructure/config/settings.py` (4 errors)

---

## Test Suite Breakdown

### âœ… Passing Test Suites (69 tests)

1. **Integration Tests** - 14/14 PASSED
   - Planning endpoints âœ…
   - Adaptive interview flow âœ…
   - Follow-up delivery âœ…
   - Backward compatibility âœ…

2. **Mock Adapter Tests** - 38/38 PASSED
   - MockAnalyticsAdapter (14 tests) âœ…
   - MockCVAnalyzerAdapter (24 tests) âœ…

3. **Use Case Tests** - 17/35 PASSED
   - Gap detection (partial) âš ï¸
   - Adaptive processing (partial) âš ï¸

### âŒ Failing Tests (18 total)

**Category A: Gap Detection Issues** (7 tests)
- `test_keyword_gap_detection` - Assertion mismatch
- `test_synonym_concept_match` - 6 != 5
- `test_case_insensitive_match` - 6 != 5
- `test_punctuation_ignored` - 7 != 5
- `test_partial_word_not_matched` - 9 != 5
- `test_empty_answer_all_gaps` - 6 != 5
- `test_concept_confirmation_threshold` - 5 != 3

**Category B: CVAnalysis Validation** (11 tests)
- All in `test_plan_interview.py` and `test_process_answer_adaptive.py`
- Missing `extracted_text` field in fixtures

---

## Performance Metrics

**Test Execution Time**: 1.69s (excellent)
**Coverage**: 15% (low - most adapters/infrastructure untested)

**Coverage by Layer**:
- Domain Models: 50-88% âœ…
- Use Cases: 17-47% âš ï¸
- Adapters: 0-5% âŒ
- Infrastructure: 0% âŒ

---

## Priority Action Items

### P0 - CRITICAL (Block Merge)

1. **Fix process_answer.py Answer Creation**
   - Add `similarity_score=0.0` to line 64
   - **ETA**: 2 minutes
   - **File**: `src/application/use_cases/process_answer.py`

2. **Fix CVAnalysis Test Fixtures**
   - Add `extracted_text="Sample CV text"` to 11 fixtures
   - **ETA**: 10 minutes
   - **Files**:
     - `tests/unit/use_cases/test_plan_interview.py`
     - `tests/unit/use_cases/test_process_answer_adaptive.py`

3. **Fix Gap Detection Test Logic**
   - Option A: Strip punctuation in implementation
   - Option B: Relax test assertions
   - **ETA**: 15 minutes
   - **File**: `tests/unit/use_cases/test_process_answer_adaptive.py`

### P1 - HIGH (Type Safety)

4. **Fix Type Checking Errors**
   - Add return type annotations (22 errors)
   - Fix generic dict types (12 errors)
   - **ETA**: 30 minutes

---

## Recommended Next Steps

1. **Immediate**: Fix P0 items (3 issues)
2. **Short-term**: Resolve mypy errors for type safety
3. **Long-term**: Increase test coverage (target 80%)

---

## Questions/Blockers

1. **Gap Detection Behavior**: Should gaps include punctuation artifacts or be cleaned?
2. **Test Strategy**: Should we prioritize integration tests over unit tests for use cases?
3. **Coverage Goals**: What's acceptable coverage threshold for this feature branch?

---

## Files Modified (3 Fixes Applied)

1. `src/adapters/persistence/mappers.py` - Added similarity_score/gaps mapping
2. `src/domain/models/answer.py` - Fixed has_gaps() logic
3. Multiple test files - Fixed ExtractedSkill fixtures

## Files Requiring Changes (3 New Issues)

1. `src/application/use_cases/process_answer.py` - Add similarity_score
2. `tests/unit/use_cases/test_plan_interview.py` - Add extracted_text to fixtures
3. `tests/unit/use_cases/test_process_answer_adaptive.py` - Fix gap detection tests

---

**Report Generated**: 2025-11-08
**Next Review**: After P0 fixes applied
</file>

<file path="alembic/README">
Generic single-database configuration.
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="alembic/versions/0001_initial_database_schema_with_all_tables.py">
"""Initial database schema with all tables

Revision ID: 0001
Revises:
Create Date: 2025-10-31 14:29:38.298134

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0001'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema - create all tables."""

    # Create candidates table
    op.create_table(
        'candidates',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('email', sa.String(255), nullable=False, unique=True),
        sa.Column('cv_file_path', sa.String(500), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
    )
    op.create_index('idx_candidates_email', 'candidates', ['email'])
    op.create_index('idx_candidates_created_at', 'candidates', ['created_at'])

    # Create questions table
    op.create_table(
        'questions',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('text', sa.Text(), nullable=False),
        sa.Column('question_type', sa.String(50), nullable=False),
        sa.Column('difficulty', sa.String(50), nullable=False),
        sa.Column('skills', postgresql.ARRAY(sa.String(100)), nullable=False, server_default='{}'),
        sa.Column('tags', postgresql.ARRAY(sa.String(100)), nullable=False, server_default='{}'),
        sa.Column('reference_answer', sa.Text(), nullable=True),
        sa.Column('evaluation_criteria', sa.Text(), nullable=True),
        sa.Column('version', sa.Integer(), nullable=False, server_default='1'),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
    )
    op.create_index('idx_questions_type', 'questions', ['question_type'])
    op.create_index('idx_questions_difficulty', 'questions', ['difficulty'])
    op.create_index('idx_questions_skills', 'questions', ['skills'], postgresql_using='gin')
    op.create_index('idx_questions_tags', 'questions', ['tags'], postgresql_using='gin')

    # Create cv_analyses table
    op.create_table(
        'cv_analyses',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('cv_file_path', sa.String(500), nullable=False),
        sa.Column('extracted_text', sa.Text(), nullable=False),
        sa.Column('skills', postgresql.JSONB(), nullable=False, server_default='[]'),
        sa.Column('work_experience_years', sa.Float(), nullable=True),
        sa.Column('education_level', sa.String(100), nullable=True),
        sa.Column('suggested_topics', postgresql.ARRAY(sa.String(200)), nullable=False, server_default='{}'),
        sa.Column('suggested_difficulty', sa.String(50), nullable=False, server_default="'medium'"),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('summary', sa.Text(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=False, server_default='{}'),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
    )
    op.create_index('idx_cv_analyses_candidate_id', 'cv_analyses', ['candidate_id'])
    op.create_index('idx_cv_analyses_created_at', 'cv_analyses', ['created_at'])

    # Create interviews table
    op.create_table(
        'interviews',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('status', sa.String(50), nullable=False),
        sa.Column('cv_analysis_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('question_ids', postgresql.ARRAY(postgresql.UUID(as_uuid=True)), nullable=False, server_default='{}'),
        sa.Column('answer_ids', postgresql.ARRAY(postgresql.UUID(as_uuid=True)), nullable=False, server_default='{}'),
        sa.Column('current_question_index', sa.Integer(), nullable=False, server_default='0'),
        sa.Column('started_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['cv_analysis_id'], ['cv_analyses.id'], ondelete='SET NULL'),
    )
    op.create_index('idx_interviews_candidate_id', 'interviews', ['candidate_id'])
    op.create_index('idx_interviews_status', 'interviews', ['status'])
    op.create_index('idx_interviews_created_at', 'interviews', ['created_at'])

    # Create answers table
    op.create_table(
        'answers',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('interview_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('question_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('text', sa.Text(), nullable=False),
        sa.Column('is_voice', sa.Boolean(), nullable=False, server_default='false'),
        sa.Column('audio_file_path', sa.String(500), nullable=True),
        sa.Column('duration_seconds', sa.Float(), nullable=True),
        sa.Column('evaluation', postgresql.JSONB(), nullable=True),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=False, server_default='{}'),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('evaluated_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['interview_id'], ['interviews.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['question_id'], ['questions.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
    )
    op.create_index('idx_answers_interview_id', 'answers', ['interview_id'])
    op.create_index('idx_answers_question_id', 'answers', ['question_id'])
    op.create_index('idx_answers_candidate_id', 'answers', ['candidate_id'])
    op.create_index('idx_answers_created_at', 'answers', ['created_at'])


def downgrade() -> None:
    """Downgrade schema - drop all tables."""
    op.drop_table('answers')
    op.drop_table('interviews')
    op.drop_table('cv_analyses')
    op.drop_table('questions')
    op.drop_table('candidates')
</file>

<file path="alembic/versions/0002_seed_sample_data.py">
"""seed_sample_data

Revision ID: 0002
Revises: 0001
Create Date: 2025-10-31 23:46:29.587545

"""
from typing import Sequence, Union
from datetime import datetime, timedelta
import uuid

from alembic import op
import sqlalchemy as sa
from sqlalchemy import Table, Column, MetaData
from sqlalchemy.dialects.postgresql import UUID, ARRAY, JSONB
from sqlalchemy import String, Text, Integer, Float, Boolean, DateTime


# revision identifiers, used by Alembic.
revision: str = '0002'
down_revision: Union[str, Sequence[str], None] = '0001'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Seed sample data for development and testing."""

    # Get connection
    conn = op.get_bind()
    metadata = MetaData()
    now = datetime.utcnow()

    # Define table schemas for bulk insert
    candidates_table = Table(
        'candidates', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('name', String),
        Column('email', String),
        Column('cv_file_path', String),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    questions_table = Table(
        'questions', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('text', Text),
        Column('question_type', String),
        Column('difficulty', String),
        Column('skills', ARRAY(String)),
        Column('tags', ARRAY(String)),
        Column('reference_answer', Text),
        Column('evaluation_criteria', Text),
        Column('version', Integer),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    cv_analyses_table = Table(
        'cv_analyses', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('cv_file_path', String),
        Column('extracted_text', Text),
        Column('skills', JSONB),
        Column('work_experience_years', Float),
        Column('education_level', String),
        Column('suggested_topics', ARRAY(String)),
        Column('suggested_difficulty', String),
        Column('summary', Text),
        Column('metadata', JSONB),
        Column('created_at', DateTime),
    )

    interviews_table = Table(
        'interviews', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('status', String),
        Column('cv_analysis_id', UUID(as_uuid=True)),
        Column('question_ids', ARRAY(UUID(as_uuid=True))),
        Column('answer_ids', ARRAY(UUID(as_uuid=True))),
        Column('current_question_index', Integer),
        Column('started_at', DateTime),
        Column('completed_at', DateTime),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    answers_table = Table(
        'answers', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('interview_id', UUID(as_uuid=True)),
        Column('question_id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('text', Text),
        Column('is_voice', Boolean),
        Column('audio_file_path', String),
        Column('duration_seconds', Float),
        Column('evaluation', JSONB),
        Column('metadata', JSONB),
        Column('created_at', DateTime),
        Column('evaluated_at', DateTime),
    )

    # =============================================
    # SEED DATA
    # =============================================

    # 1. Candidates
    op.bulk_insert(candidates_table, [
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'name': 'John Doe',
            'email': 'john.doe@example.com',
            'cv_file_path': '/uploads/cvs/john_doe_cv.pdf',
            'created_at': now - timedelta(days=30),
            'updated_at': now - timedelta(days=30),
        },
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'name': 'Jane Smith',
            'email': 'jane.smith@example.com',
            'cv_file_path': '/uploads/cvs/jane_smith_cv.pdf',
            'created_at': now - timedelta(days=25),
            'updated_at': now - timedelta(days=25),
        },
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'name': 'Bob Johnson',
            'email': 'bob.johnson@example.com',
            'cv_file_path': '/uploads/cvs/bob_johnson_cv.pdf',
            'created_at': now - timedelta(days=20),
            'updated_at': now - timedelta(days=20),
        },
    ])

    # 2. Questions
    op.bulk_insert(questions_table, [
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'text': 'What is the difference between var, let, and const in JavaScript?',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['JavaScript', 'ES6', 'Variables'],
            'tags': ['javascript', 'basics', 'es6'],
            'reference_answer': 'var is function-scoped and can be redeclared, let is block-scoped and cannot be redeclared, const is block-scoped and cannot be reassigned.',
            'evaluation_criteria': 'Check understanding of scope, hoisting, and immutability concepts',
            'version': 1,
            'created_at': now - timedelta(days=60),
            'updated_at': now - timedelta(days=60),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440002'),
            'text': 'Explain what REST API is and its main HTTP methods.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['API', 'REST', 'HTTP'],
            'tags': ['api', 'rest', 'http'],
            'reference_answer': 'REST is an architectural style for APIs using standard HTTP methods: GET (retrieve), POST (create), PUT/PATCH (update), DELETE (remove).',
            'evaluation_criteria': 'Evaluate understanding of RESTful principles and HTTP verbs',
            'version': 1,
            'created_at': now - timedelta(days=60),
            'updated_at': now - timedelta(days=60),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'text': 'How does async/await work in JavaScript? Compare it with Promises.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['JavaScript', 'Async', 'Promises'],
            'tags': ['javascript', 'async', 'promises'],
            'reference_answer': 'async/await is syntactic sugar over Promises, making asynchronous code look synchronous.',
            'evaluation_criteria': 'Assess understanding of asynchronous programming',
            'version': 1,
            'created_at': now - timedelta(days=55),
            'updated_at': now - timedelta(days=55),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
            'text': 'What is a closure in Python? Provide an example.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Python', 'Closures', 'Functional Programming'],
            'tags': ['python', 'closures', 'functional'],
            'reference_answer': 'A closure is a nested function that remembers values from its enclosing scope even after the outer function has finished execution.',
            'evaluation_criteria': 'Check understanding of lexical scoping and closure mechanics',
            'version': 1,
            'created_at': now - timedelta(days=58),
            'updated_at': now - timedelta(days=58),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440005'),
            'text': 'Describe the difference between list comprehension and generator expressions in Python.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Python', 'List Comprehension', 'Generators'],
            'tags': ['python', 'list-comprehension', 'generators'],
            'reference_answer': 'List comprehensions create lists in memory, while generator expressions create iterators that yield values on demand, saving memory.',
            'evaluation_criteria': 'Evaluate understanding of memory efficiency and iteration patterns',
            'version': 1,
            'created_at': now - timedelta(days=57),
            'updated_at': now - timedelta(days=57),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
            'text': 'What is the difference between SQL JOIN types: INNER, LEFT, RIGHT, and FULL OUTER?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['SQL', 'Database', 'JOIN'],
            'tags': ['sql', 'database', 'join'],
            'reference_answer': 'INNER returns matching rows, LEFT returns all left table rows, RIGHT returns all right table rows, FULL OUTER returns all rows from both tables.',
            'evaluation_criteria': 'Assess knowledge of SQL JOIN operations and their use cases',
            'version': 1,
            'created_at': now - timedelta(days=56),
            'updated_at': now - timedelta(days=56),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
            'text': 'Explain what is a React Hook and name three common hooks you have used.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['React', 'Hooks', 'Frontend'],
            'tags': ['react', 'hooks', 'frontend'],
            'reference_answer': 'React Hooks are functions that let you use state and lifecycle features in functional components. Common hooks: useState, useEffect, useContext.',
            'evaluation_criteria': 'Evaluate understanding of React Hooks and their practical application',
            'version': 1,
            'created_at': now - timedelta(days=54),
            'updated_at': now - timedelta(days=54),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            'text': 'How would you design a system to handle 1 million requests per second?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['System Design', 'Scalability', 'Architecture'],
            'tags': ['system-design', 'scalability', 'architecture'],
            'reference_answer': 'Use load balancers, horizontal scaling, caching layers, database sharding, CDN, message queues, and microservices architecture.',
            'evaluation_criteria': 'Assess system design thinking, scalability patterns, and trade-off considerations',
            'version': 1,
            'created_at': now - timedelta(days=52),
            'updated_at': now - timedelta(days=52),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440009'),
            'text': 'What is the difference between process and thread? When would you use each?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Operating Systems', 'Concurrency', 'Threading'],
            'tags': ['operating-systems', 'concurrency', 'threading'],
            'reference_answer': 'Processes have separate memory spaces, threads share memory. Use processes for isolation, threads for shared state and performance.',
            'evaluation_criteria': 'Evaluate understanding of concurrency models and their trade-offs',
            'version': 1,
            'created_at': now - timedelta(days=51),
            'updated_at': now - timedelta(days=51),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            'text': 'Explain the SOLID principles in object-oriented programming.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['OOP', 'Design Patterns', 'SOLID'],
            'tags': ['oop', 'design-patterns', 'solid'],
            'reference_answer': 'SOLID: Single Responsibility, Open/Closed, Liskov Substitution, Interface Segregation, Dependency Inversion. These principles guide good OOP design.',
            'evaluation_criteria': 'Check understanding of OOP principles and their practical application',
            'version': 1,
            'created_at': now - timedelta(days=50),
            'updated_at': now - timedelta(days=50),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440011'),
            'text': 'What is Docker and how does it differ from a virtual machine?',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Docker', 'Containers', 'DevOps'],
            'tags': ['docker', 'containers', 'devops'],
            'reference_answer': 'Docker uses containerization to package applications with dependencies. VMs virtualize hardware, containers virtualize the OS, making containers lighter and faster.',
            'evaluation_criteria': 'Assess understanding of containerization vs virtualization',
            'version': 1,
            'created_at': now - timedelta(days=49),
            'updated_at': now - timedelta(days=49),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440012'),
            'text': 'Explain the difference between time complexity and space complexity with examples.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Algorithms', 'Big O', 'Complexity Analysis'],
            'tags': ['algorithms', 'big-o', 'complexity'],
            'reference_answer': 'Time complexity measures execution time growth, space complexity measures memory usage growth. Both use Big O notation (O(n), O(log n), etc.).',
            'evaluation_criteria': 'Evaluate algorithmic thinking and complexity analysis skills',
            'version': 1,
            'created_at': now - timedelta(days=48),
            'updated_at': now - timedelta(days=48),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440013'),
            'text': 'What is the difference between REST and GraphQL? When would you choose one over the other?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['API', 'REST', 'GraphQL'],
            'tags': ['api', 'rest', 'graphql'],
            'reference_answer': 'REST uses multiple endpoints with fixed responses, GraphQL uses a single endpoint with flexible queries. Choose GraphQL for complex queries, REST for simplicity.',
            'evaluation_criteria': 'Assess understanding of API design patterns and trade-offs',
            'version': 1,
            'created_at': now - timedelta(days=47),
            'updated_at': now - timedelta(days=47),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440014'),
            'text': 'Describe how Node.js handles asynchronous operations. What is the event loop?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['Node.js', 'Event Loop', 'Asynchronous'],
            'tags': ['nodejs', 'event-loop', 'async'],
            'reference_answer': 'Node.js uses an event loop with a single-threaded event-driven architecture. The event loop processes callbacks, timers, and I/O operations in phases.',
            'evaluation_criteria': 'Evaluate deep understanding of Node.js internals and asynchronous execution',
            'version': 1,
            'created_at': now - timedelta(days=46),
            'updated_at': now - timedelta(days=46),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440015'),
            'text': 'What is unit testing and why is it important? Name a testing framework you have used.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Testing', 'Unit Testing', 'QA'],
            'tags': ['testing', 'unit-testing', 'qa'],
            'reference_answer': 'Unit testing tests individual components in isolation. It ensures code quality, catches bugs early, and enables refactoring confidence. Examples: Jest, pytest, JUnit.',
            'evaluation_criteria': 'Check understanding of testing principles and practical experience',
            'version': 1,
            'created_at': now - timedelta(days=45),
            'updated_at': now - timedelta(days=45),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            'text': 'Tell me about a time when you had to work under pressure to meet a deadline.',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Time Management', 'Stress Management', 'Communication'],
            'tags': ['behavioral', 'deadlines', 'pressure'],
            'reference_answer': 'Candidate should describe a specific situation, explain the challenge, detail actions taken, and reflect on outcomes using STAR method.',
            'evaluation_criteria': 'Assess ability to handle pressure, prioritize tasks, and communicate effectively under stress',
            'version': 1,
            'created_at': now - timedelta(days=44),
            'updated_at': now - timedelta(days=44),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440017'),
            'text': 'Describe a situation where you had to learn a new technology quickly for a project.',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'EASY',
            'skills': ['Learning', 'Adaptability', 'Problem Solving'],
            'tags': ['behavioral', 'learning', 'adaptability'],
            'reference_answer': 'Candidate should demonstrate self-directed learning, resource utilization, and ability to apply new knowledge effectively.',
            'evaluation_criteria': 'Evaluate learning agility, initiative, and ability to adapt to new technologies',
            'version': 1,
            'created_at': now - timedelta(days=43),
            'updated_at': now - timedelta(days=43),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440018'),
            'text': 'Give an example of a time when you disagreed with a team member. How did you resolve it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Conflict Resolution', 'Teamwork', 'Communication'],
            'tags': ['behavioral', 'conflict', 'teamwork'],
            'reference_answer': 'Candidate should show emotional intelligence, active listening, and collaborative problem-solving approach.',
            'evaluation_criteria': 'Assess conflict resolution skills and ability to work collaboratively',
            'version': 1,
            'created_at': now - timedelta(days=42),
            'updated_at': now - timedelta(days=42),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440019'),
            'text': 'What is your biggest weakness as a developer, and how are you working to improve it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Self-Awareness', 'Growth Mindset', 'Honesty'],
            'tags': ['behavioral', 'self-reflection', 'growth'],
            'reference_answer': 'Candidate should be honest, show self-awareness, and demonstrate proactive steps toward improvement.',
            'evaluation_criteria': 'Evaluate self-awareness, growth mindset, and honesty',
            'version': 1,
            'created_at': now - timedelta(days=41),
            'updated_at': now - timedelta(days=41),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440020'),
            'text': 'If you discovered a critical bug in production right before a major release, what would you do?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'MEDIUM',
            'skills': ['Problem Solving', 'Risk Management', 'Decision Making'],
            'tags': ['situational', 'bug', 'production'],
            'reference_answer': 'Assess severity, inform stakeholders, evaluate options (hotfix vs delay), prioritize user impact, and document the decision process.',
            'evaluation_criteria': 'Check ability to make decisions under pressure while considering business and technical implications',
            'version': 1,
            'created_at': now - timedelta(days=40),
            'updated_at': now - timedelta(days=40),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440021'),
            'text': 'You have limited time to complete a feature. How do you decide what to prioritize?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'EASY',
            'skills': ['Prioritization', 'Time Management', 'Analytical Thinking'],
            'tags': ['situational', 'prioritization', 'time-management'],
            'reference_answer': 'Evaluate business value, user impact, dependencies, and risks. Focus on MVP first, then iterate.',
            'evaluation_criteria': 'Assess prioritization skills and ability to make trade-off decisions',
            'version': 1,
            'created_at': now - timedelta(days=39),
            'updated_at': now - timedelta(days=39),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440022'),
            'text': 'How would you handle a situation where a client requests a feature that conflicts with your technical recommendations?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'HARD',
            'skills': ['Client Communication', 'Technical Leadership', 'Negotiation'],
            'tags': ['situational', 'client-management', 'leadership'],
            'reference_answer': 'Listen to client needs, explain technical concerns clearly, propose alternatives, and find a collaborative solution that balances requirements.',
            'evaluation_criteria': 'Evaluate communication skills, technical credibility, and ability to navigate stakeholder relationships',
            'version': 1,
            'created_at': now - timedelta(days=38),
            'updated_at': now - timedelta(days=38),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
            'text': 'Explain what is garbage collection in Java and how it works.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Java', 'Memory Management', 'JVM'],
            'tags': ['java', 'memory-management', 'jvm'],
            'reference_answer': 'Garbage collection automatically reclaims memory by identifying and removing unused objects. JVM uses generational GC with Eden, Survivor, and Old Generation spaces.',
            'evaluation_criteria': 'Assess understanding of memory management and JVM internals',
            'version': 1,
            'created_at': now - timedelta(days=37),
            'updated_at': now - timedelta(days=37),
        },
    ])

    # 3. CV Analyses
    op.bulk_insert(cv_analyses_table, [
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'cv_file_path': '/uploads/cvs/john_doe_cv.pdf',
            'extracted_text': 'John Doe - Senior Full Stack Developer. 5+ years experience in React, Node.js, PostgreSQL.',
            'skills': [
                {"skill": "React", "proficiency": "expert", "years": 5},
                {"skill": "Node.js", "proficiency": "expert", "years": 5},
            ],
            'work_experience_years': 5.5,
            'education_level': 'Bachelor',
            'suggested_topics': ['Microservices', 'React Hooks', 'Database Design'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Experienced full-stack developer with strong React and Node.js skills.',
            'metadata': {"keywords": ["React", "Node.js"]},
            'created_at': now - timedelta(days=29),
        },
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'cv_file_path': '/uploads/cvs/jane_smith_cv.pdf',
            'extracted_text': 'Jane Smith - Backend Engineer. 3+ years experience in Python, Java, SQL, REST APIs, and microservices architecture.',
            'skills': [
                {"skill": "Python", "proficiency": "advanced", "years": 3},
                {"skill": "Java", "proficiency": "advanced", "years": 3},
                {"skill": "SQL", "proficiency": "intermediate", "years": 2},
                {"skill": "REST API", "proficiency": "advanced", "years": 3},
            ],
            'work_experience_years': 3.5,
            'education_level': 'Master',
            'suggested_topics': ['System Design', 'SOLID Principles', 'Database Optimization', 'API Design'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Backend engineer with strong Python and Java skills, experienced in building scalable APIs.',
            'metadata': {"keywords": ["Python", "Java", "Backend", "API"]},
            'created_at': now - timedelta(days=24),
        },
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'cv_file_path': '/uploads/cvs/bob_johnson_cv.pdf',
            'extracted_text': 'Bob Johnson - Full Stack Developer. 7+ years experience in JavaScript, React, Node.js, Docker, and cloud technologies.',
            'skills': [
                {"skill": "JavaScript", "proficiency": "expert", "years": 7},
                {"skill": "React", "proficiency": "expert", "years": 6},
                {"skill": "Node.js", "proficiency": "expert", "years": 6},
                {"skill": "Docker", "proficiency": "advanced", "years": 4},
            ],
            'work_experience_years': 7.5,
            'education_level': 'Bachelor',
            'suggested_topics': ['Event Loop', 'System Design', 'Microservices', 'Async Programming'],
            'suggested_difficulty': 'HARD',
            'summary': 'Senior full-stack developer with extensive JavaScript ecosystem expertise and system design experience.',
            'metadata': {"keywords": ["JavaScript", "React", "Node.js", "System Design"]},
            'created_at': now - timedelta(days=19),
        },
    ])

    # 4. Interviews
    op.bulk_insert(interviews_table, [
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'status': 'COMPLETED',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440001'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440002'),
            ],
            'current_question_index': 2,
            'started_at': now - timedelta(days=28),
            'completed_at': now - timedelta(days=28) + timedelta(minutes=30),
            'created_at': now - timedelta(days=28),
            'updated_at': now - timedelta(days=28) + timedelta(minutes=30),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'status': 'COMPLETED',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440003'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440006'),
            ],
            'current_question_index': 4,
            'started_at': now - timedelta(days=23),
            'completed_at': now - timedelta(days=23) + timedelta(minutes=45),
            'created_at': now - timedelta(days=23),
            'updated_at': now - timedelta(days=23) + timedelta(minutes=45),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'status': 'IN_PROGRESS',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440003'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440014'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440018'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440007'),
            ],
            'current_question_index': 1,
            'started_at': now - timedelta(days=18),
            'completed_at': None,
            'created_at': now - timedelta(days=18),
            'updated_at': now - timedelta(days=18) + timedelta(minutes=20),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'status': 'READY',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440009'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440013'),
            ],
            'answer_ids': [],
            'current_question_index': 0,
            'started_at': None,
            'completed_at': None,
            'created_at': now - timedelta(days=15),
            'updated_at': now - timedelta(days=15),
        },
    ])

    # 5. Answers
    op.bulk_insert(answers_table, [
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440001'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'text': 'var is function-scoped, let and const are block-scoped. const cannot be reassigned.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 85,
                "feedback": "Good understanding of scope and hoisting.",
                "strengths": ["Clear explanation"],
            },
            'metadata': {"response_time_seconds": 45},
            'created_at': now - timedelta(days=28),
            'evaluated_at': now - timedelta(days=28) + timedelta(minutes=5),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440002'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'text': 'async/await makes async code more readable. It is built on top of Promises.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 80,
                "feedback": "Solid understanding.",
            },
            'metadata': {"response_time_seconds": 60},
            'created_at': now - timedelta(days=28) + timedelta(minutes=15),
            'evaluated_at': now - timedelta(days=28) + timedelta(minutes=20),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440003'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'A closure in Python is a nested function that captures variables from its enclosing scope. For example, a counter function can maintain state between calls.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 88,
                "feedback": "Good understanding of closures with practical example.",
                "strengths": ["Clear explanation", "Provided example"],
            },
            'metadata': {"response_time_seconds": 50},
            'created_at': now - timedelta(days=23) + timedelta(minutes=5),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=8),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440004'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'INNER JOIN returns matching rows from both tables. LEFT JOIN returns all rows from left table. RIGHT JOIN returns all rows from right table. FULL OUTER JOIN returns all rows from both tables.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 92,
                "feedback": "Excellent understanding of SQL JOIN operations.",
                "strengths": ["Complete coverage of all JOIN types", "Clear explanation"],
            },
            'metadata': {"response_time_seconds": 55},
            'created_at': now - timedelta(days=23) + timedelta(minutes=15),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=18),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440005'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'SOLID principles are: Single Responsibility - one reason to change, Open/Closed - open for extension, Liskov Substitution - derived classes must be substitutable, Interface Segregation - no forced implementation, Dependency Inversion - depend on abstractions.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 85,
                "feedback": "Solid grasp of SOLID principles with clear explanations.",
                "strengths": ["Covered all principles", "Practical understanding"],
            },
            'metadata': {"response_time_seconds": 75},
            'created_at': now - timedelta(days=23) + timedelta(minutes=25),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=30),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440006'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'Last month I had to deliver a feature under tight deadline. I broke it down into smaller tasks, communicated blockers early, and worked extra hours when needed. We delivered on time by prioritizing the MVP.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 78,
                "feedback": "Good example showing problem-solving under pressure.",
                "strengths": ["Clear situation", "Practical approach"],
                "areas_for_improvement": ["Could elaborate more on communication strategies"],
            },
            'metadata': {"response_time_seconds": 90},
            'created_at': now - timedelta(days=23) + timedelta(minutes=35),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=40),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440007'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'text': 'To handle 1M requests per second, I would use horizontal scaling with load balancers, implement caching at multiple layers (CDN, Redis), use database replication and sharding, implement message queues for async processing, and design with microservices for independent scaling.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 90,
                "feedback": "Excellent system design thinking covering key scalability patterns.",
                "strengths": ["Comprehensive approach", "Mentioned key technologies", "Thought about multiple layers"],
            },
            'metadata': {"response_time_seconds": 120},
            'created_at': now - timedelta(days=18) + timedelta(minutes=5),
            'evaluated_at': now - timedelta(days=18) + timedelta(minutes=12),
        },
    ])

    print("[OK] Seeded 3 candidates")
    print("[OK] Seeded 23 questions")
    print("[OK] Seeded 3 CV analyses")
    print("[OK] Seeded 4 interviews")
    print("[OK] Seeded 7 answers")


def downgrade() -> None:
    """Remove seeded data."""
    conn = op.get_bind()

    # Delete in reverse order of dependencies
    # Answers
    conn.execute(sa.text("""
        DELETE FROM answers WHERE id IN (
            '950e8400-e29b-41d4-a716-446655440001',
            '950e8400-e29b-41d4-a716-446655440002',
            '950e8400-e29b-41d4-a716-446655440003',
            '950e8400-e29b-41d4-a716-446655440004',
            '950e8400-e29b-41d4-a716-446655440005',
            '950e8400-e29b-41d4-a716-446655440006',
            '950e8400-e29b-41d4-a716-446655440007'
        )
    """))

    # Interviews
    conn.execute(sa.text("""
        DELETE FROM interviews WHERE id IN (
            '850e8400-e29b-41d4-a716-446655440001',
            '850e8400-e29b-41d4-a716-446655440002',
            '850e8400-e29b-41d4-a716-446655440003',
            '850e8400-e29b-41d4-a716-446655440004'
        )
    """))

    # CV Analyses
    conn.execute(sa.text("""
        DELETE FROM cv_analyses WHERE id IN (
            '750e8400-e29b-41d4-a716-446655440001',
            '750e8400-e29b-41d4-a716-446655440002',
            '750e8400-e29b-41d4-a716-446655440003'
        )
    """))

    # Questions
    conn.execute(sa.text("""
        DELETE FROM questions WHERE id IN (
            '650e8400-e29b-41d4-a716-446655440001',
            '650e8400-e29b-41d4-a716-446655440002',
            '650e8400-e29b-41d4-a716-446655440003',
            '650e8400-e29b-41d4-a716-446655440004',
            '650e8400-e29b-41d4-a716-446655440005',
            '650e8400-e29b-41d4-a716-446655440006',
            '650e8400-e29b-41d4-a716-446655440007',
            '650e8400-e29b-41d4-a716-446655440008',
            '650e8400-e29b-41d4-a716-446655440009',
            '650e8400-e29b-41d4-a716-446655440010',
            '650e8400-e29b-41d4-a716-446655440011',
            '650e8400-e29b-41d4-a716-446655440012',
            '650e8400-e29b-41d4-a716-446655440013',
            '650e8400-e29b-41d4-a716-446655440014',
            '650e8400-e29b-41d4-a716-446655440015',
            '650e8400-e29b-41d4-a716-446655440016',
            '650e8400-e29b-41d4-a716-446655440017',
            '650e8400-e29b-41d4-a716-446655440018',
            '650e8400-e29b-41d4-a716-446655440019',
            '650e8400-e29b-41d4-a716-446655440020',
            '650e8400-e29b-41d4-a716-446655440021',
            '650e8400-e29b-41d4-a716-446655440022',
            '650e8400-e29b-41d4-a716-446655440023'
        )
    """))

    # Candidates
    conn.execute(sa.text("""
        DELETE FROM candidates WHERE id IN (
            '550e8400-e29b-41d4-a716-446655440001',
            '550e8400-e29b-41d4-a716-446655440002',
            '550e8400-e29b-41d4-a716-446655440003'
        )
    """))

    print("[OK] Removed seed data")
</file>

<file path="alembic/versions/0003_add_planning_and_adaptive_fields.py">
"""add planning and adaptive evaluation fields

Revision ID: 0003
Revises: 0002
Create Date: 2025-11-06 23:00:00

Changes:
- questions: ideal_answer, rationale
- interviews: plan_metadata, adaptive_follow_ups
- answers: similarity_score, gaps
- Add new follow_up_questions table
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = "0003"
down_revision = "0002"
branch_labels = None
depends_on = None


def upgrade():
    # === Questions table ===
    op.add_column(
        "questions",
        sa.Column("ideal_answer", sa.Text(), nullable=True),
    )
    op.add_column(
        "questions",
        sa.Column("rationale", sa.Text(), nullable=True),
    )

    # === Interviews table ===
    op.add_column(
        "interviews",
        sa.Column(
            "plan_metadata",
            postgresql.JSONB(astext_type=sa.Text()),
            server_default="{}",
            nullable=False,
        ),
    )
    op.add_column(
        "interviews",
        sa.Column(
            "adaptive_follow_ups",
            postgresql.ARRAY(postgresql.UUID(as_uuid=True)),
            server_default="{}",
            nullable=False,
        ),
    )

    # === Answers table ===
    op.add_column(
        "answers",
        sa.Column("similarity_score", sa.Float(), nullable=True),
    )
    op.add_column(
        "answers",
        sa.Column(
            "gaps", postgresql.JSONB(astext_type=sa.Text()), nullable=True
        ),
    )

    # === Create follow_up_questions table ===
    op.create_table(
        "follow_up_questions",
        sa.Column("id", postgresql.UUID(as_uuid=True), primary_key=True, nullable=False),
        sa.Column(
            "parent_question_id",
            postgresql.UUID(as_uuid=True),
            sa.ForeignKey("questions.id", ondelete="CASCADE"),
            nullable=False,
        ),
        sa.Column(
            "interview_id",
            postgresql.UUID(as_uuid=True),
            sa.ForeignKey("interviews.id", ondelete="CASCADE"),
            nullable=False,
        ),
        sa.Column("text", sa.Text(), nullable=False),
        sa.Column("generated_reason", sa.Text(), nullable=False),
        sa.Column("order_in_sequence", sa.Integer(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
    )

    # === Indexes for follow_up_questions ===
    op.create_index(
        "idx_follow_up_questions_parent_question_id",
        "follow_up_questions",
        ["parent_question_id"],
    )
    op.create_index(
        "idx_follow_up_questions_interview_id",
        "follow_up_questions",
        ["interview_id"],
    )
    op.create_index(
        "idx_follow_up_questions_created_at",
        "follow_up_questions",
        ["created_at"],
    )

    # === Constraints for answers ===
    op.create_check_constraint(
        "check_similarity_score_bounds",
        "answers",
        "similarity_score IS NULL OR (similarity_score >= 0 AND similarity_score <= 1)",
    )

    # === Indexes for answers ===
    op.create_index(
        "idx_answers_similarity_score",
        "answers",
        ["similarity_score"],
        postgresql_where=sa.text("similarity_score IS NOT NULL"),
    )
    op.create_index(
        "idx_answers_gaps",
        "answers",
        ["gaps"],
        postgresql_using="gin",
        postgresql_where=sa.text("gaps IS NOT NULL"),
    )


def downgrade():
    # Drop indexes
    op.drop_index("idx_answers_gaps", table_name="answers")
    op.drop_index("idx_answers_similarity_score", table_name="answers")
    op.drop_constraint("check_similarity_score_bounds", "answers", type_="check")

    # Drop follow_up_questions table and its indexes
    op.drop_index("idx_follow_up_questions_created_at", table_name="follow_up_questions")
    op.drop_index("idx_follow_up_questions_interview_id", table_name="follow_up_questions")
    op.drop_index("idx_follow_up_questions_parent_question_id", table_name="follow_up_questions")
    op.drop_table("follow_up_questions")

    # Drop columns (answers)
    op.drop_column("answers", "gaps")
    op.drop_column("answers", "similarity_score")

    # Drop columns (interviews)
    op.drop_column("interviews", "adaptive_follow_ups")
    op.drop_column("interviews", "plan_metadata")

    # Drop columns (questions)
    op.drop_column("questions", "rationale")
    op.drop_column("questions", "ideal_answer")
</file>

<file path="alembic/versions/0004_seed_data_for_planning_and_adaptive.py">
"""seed data for planning and adaptive fields

Revision ID: 0004
Revises: 0003
Create Date: 2025-11-08 23:01:57.944002

Seed sample data for new columns:
- questions: ideal_answer, rationale
- interviews: plan_metadata (already has default {})
- answers: similarity_score, gaps
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy import text


# revision identifiers, used by Alembic.
revision: str = '0004'
down_revision: Union[str, Sequence[str], None] = '0003'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Seed data for new planning and adaptive fields."""

    conn = op.get_bind()

    # === Seed Questions table ===
    # Update existing questions with ideal_answer and rationale
    conn.execute(text("""
        UPDATE questions
        SET
            ideal_answer = CASE
                WHEN question_type = 'technical' THEN
                    'A comprehensive answer should cover: ' ||
                    CASE
                        WHEN text ILIKE '%OOP%' OR text ILIKE '%object%oriented%' THEN 'classes, objects, inheritance, polymorphism, encapsulation, abstraction'
                        WHEN text ILIKE '%database%' OR text ILIKE '%sql%' THEN 'data modeling, ACID properties, indexing, query optimization'
                        WHEN text ILIKE '%algorithm%' THEN 'time complexity, space complexity, trade-offs, real-world applications'
                        ELSE 'technical concepts, best practices, and practical implementation'
                    END
                WHEN question_type = 'behavioral' THEN
                    'Use STAR method: Situation, Task, Action, Result. Provide specific examples with measurable outcomes.'
                WHEN question_type = 'situational' THEN
                    'Demonstrate problem-solving approach: understand problem, consider options, make decision, explain rationale.'
                ELSE 'Provide detailed, structured response with examples.'
            END,
            rationale = CASE
                WHEN question_type = 'technical' THEN
                    'Assesses technical knowledge and ability to explain complex concepts clearly.'
                WHEN question_type = 'behavioral' THEN
                    'Evaluates past behavior as predictor of future performance and cultural fit.'
                WHEN question_type = 'situational' THEN
                    'Tests problem-solving skills and decision-making process under hypothetical scenarios.'
                ELSE 'Evaluates candidate competency in relevant domain.'
            END
        WHERE ideal_answer IS NULL OR rationale IS NULL
    """))

    # === Seed Answers table ===
    # Calculate similarity scores for existing answers (mock values for demo)
    conn.execute(text("""
        UPDATE answers
        SET
            similarity_score = CASE
                WHEN LENGTH(text) > 200 THEN 0.85 + (RANDOM() * 0.15)
                WHEN LENGTH(text) > 100 THEN 0.70 + (RANDOM() * 0.15)
                WHEN LENGTH(text) > 50 THEN 0.60 + (RANDOM() * 0.10)
                ELSE 0.40 + (RANDOM() * 0.20)
            END,
            gaps = CASE
                WHEN LENGTH(text) < 100 THEN
                    '{"missing_concepts": ["detail", "examples"], "improvement_areas": ["depth", "clarity"]}'::jsonb
                WHEN LENGTH(text) < 200 THEN
                    '{"missing_concepts": ["advanced_details"], "improvement_areas": ["specificity"]}'::jsonb
                ELSE
                    '{"missing_concepts": [], "improvement_areas": []}'::jsonb
            END
        WHERE similarity_score IS NULL OR gaps IS NULL
    """))

    print("[OK] Seeded ideal answers and rationale for questions")
    print("[OK] Seeded similarity scores and gaps for answers")


def downgrade() -> None:
    """Clear seeded data."""

    conn = op.get_bind()

    # Clear seeded data (set to NULL)
    conn.execute(text("""
        UPDATE questions
        SET ideal_answer = NULL, rationale = NULL
    """))

    conn.execute(text("""
        UPDATE answers
        SET similarity_score = NULL, gaps = NULL
    """))

    print("[OK] Cleared seeded data for new columns")
</file>

<file path="alembic/versions/0005_drop_reference_answer_column.py">
"""drop reference_answer column from questions table

Revision ID: 0005
Revises: 0004
Create Date: 2025-11-08 12:00:00

Changes:
- questions: Remove redundant reference_answer column (replaced by ideal_answer)
"""
from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision = "0005"
down_revision = "0004"
branch_labels = None
depends_on = None


def upgrade():
    """Drop reference_answer column from questions table."""
    op.drop_column("questions", "reference_answer")


def downgrade():
    """Re-add reference_answer column to questions table."""
    op.add_column(
        "questions",
        sa.Column("reference_answer", sa.Text(), nullable=True),
    )
</file>

<file path="CHANGELOG_ENV.md">
# Environment Configuration Update

## Summary

Updated all migration scripts and configuration to support `.env.local` for local development overrides. This follows security best practices by keeping sensitive credentials out of version control.

## Changes Made

### 1. Updated Scripts

All Python scripts now check for `.env.local` first, then fallback to `.env`:

#### `scripts/setup_db.py`
- Added environment file detection logic
- Prints which file is being loaded
- Tries `.env.local` â†’ `.env` â†’ warns if neither exists

#### `scripts/verify_db.py`
- Same environment loading pattern as setup_db.py
- Consistent behavior across all scripts

#### `scripts/test_env.py` (NEW)
- Test script to verify environment configuration
- Shows which file is active
- Displays loaded settings (with password masking)

### 2. Updated Shell Scripts

#### `scripts/setup_and_migrate.sh` (Linux/macOS)
- Checks for `.env.local` first
- Falls back to `.env`
- Shows which file is being used
- Exports variables from the active file

#### `scripts/setup_and_migrate.bat` (Windows)
- Same logic as bash script
- Windows-compatible syntax
- Sets `ENV_FILE` variable to track active file

### 3. Updated Documentation

#### `.env.example`
- Added header explaining the `.env` vs `.env.local` pattern
- Documents that `.env.local` takes precedence
- Notes that `.env.local` is gitignored

#### `.gitignore`
- Added explicit `.env.local` entry
- Ensures local credentials are never committed

#### `DATABASE_SETUP.md`
- Updated configuration section
- Explains file priority order
- Recommends using `.env.local` for credentials
- Links to new `ENV_SETUP.md`

#### `ENV_SETUP.md` (NEW)
- Comprehensive guide to environment configuration
- Explains priority order
- Best practices for security
- Examples of shared vs. local config
- Troubleshooting guide

## File Priority Order

The application and all scripts now follow this order:

```
1. .env.local (highest priority)
   â†“ if not found
2. .env (fallback)
   â†“ if not found
3. System environment variables
   â†“ if not found
4. Pydantic defaults (from settings.py)
```

## Migration Guide

### For Existing Developers

If you already have a `.env` file:

```bash
# Option 1: Convert .env to .env.local
mv .env .env.local

# Option 2: Keep .env and create .env.local for overrides
cp .env .env.local
# Then edit .env.local with your actual credentials

# Option 3: Just create .env.local from example
cp .env.example .env.local
# Edit with your credentials
```

### For New Developers

```bash
# Copy example to .env.local
cp .env.example .env.local

# Edit with your credentials
nano .env.local
```

## Benefits

1. **Security**
   - Sensitive credentials stay in `.env.local` (gitignored)
   - Safe default configuration in `.env` (can be committed)
   - No accidental credential commits

2. **Flexibility**
   - Each developer can have different local settings
   - Easy to switch between configurations
   - No merge conflicts on environment files

3. **Team Collaboration**
   - Shared defaults in `.env` (optional)
   - Personal overrides in `.env.local`
   - Clear separation of concerns

4. **Consistency**
   - All scripts use the same loading pattern
   - Predictable behavior across the project
   - Clear feedback on which file is active

## Verification

Test the new configuration:

```bash
# Check which environment file will be used
python scripts/test_env.py

# Run database setup (will show which file it loads)
python scripts/setup_db.py

# Run migration scripts
scripts/setup_and_migrate.bat  # Windows
# or
./scripts/setup_and_migrate.sh  # Linux/macOS
```

## Backward Compatibility

This change is **fully backward compatible**:

- Existing `.env` files continue to work
- No changes required to existing workflows
- Scripts gracefully fallback to `.env` if `.env.local` doesn't exist
- Settings.py already supported this pattern via Pydantic

## Configuration Already Supported

The `settings.py` was already configured to use this pattern:

```python
model_config = SettingsConfigDict(
    env_file=(".env.local", ".env"),  # Try .env.local first
    env_file_encoding="utf-8",
    case_sensitive=False,
)
```

This update ensures **all scripts** follow the same pattern.

## Related Files

**Created:**
- `ENV_SETUP.md` - Comprehensive environment configuration guide
- `scripts/test_env.py` - Test script for verifying configuration

**Modified:**
- `scripts/setup_db.py` - Added .env.local support
- `scripts/verify_db.py` - Added .env.local support
- `scripts/setup_and_migrate.sh` - Added .env.local support
- `scripts/setup_and_migrate.bat` - Added .env.local support
- `.env.example` - Added documentation header
- `.gitignore` - Added explicit .env.local entry
- `DATABASE_SETUP.md` - Updated configuration section

**Unchanged:**
- `src/infrastructure/config/settings.py` - Already supported this pattern
- `alembic/env.py` - Uses settings.py, automatically inherits support

## Security Notes

**DO NOT commit:**
- `.env.local` - Personal credentials and sensitive data
- `.env` (if it contains secrets) - Only commit with placeholder values

**Safe to commit:**
- `.env.example` - Template with no sensitive data
- `.env` (optional) - If it contains only non-sensitive defaults

## Questions?

- Read `ENV_SETUP.md` for detailed explanation
- Run `python scripts/test_env.py` to test your configuration
- Check `DATABASE_SETUP.md` for database-specific setup

## Date

Updated: 2025-01-31
</file>

<file path="pytest.ini">
[pytest]
# Pytest configuration for adaptive interview tests

# Test discovery
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Asyncio mode
asyncio_mode = auto

# Markers
markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (require database, external services)
    slow: Slow tests (may take > 5 seconds)
    adaptive: Tests for adaptive interview features

# Test output
addopts =
    -v
    --strict-markers
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-branch

# Coverage
[coverage:run]
source = src
omit =
    */tests/*
    */migrations/*
    */__pycache__/*
    */site-packages/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
</file>

<file path="quickstart.bat">
@echo off
REM Quick start script for Windows

echo ========================================
echo Elios AI Interview Service - Quick Start
echo ========================================
echo.

REM Check if virtual environment exists
if not exist "venv\" (
    echo [1/4] Creating virtual environment...
    python -m venv venv
    if errorlevel 1 (
        echo ERROR: Failed to create virtual environment
        pause
        exit /b 1
    )
) else (
    echo [1/4] Virtual environment already exists
)

echo [2/4] Activating virtual environment...
call venv\Scripts\activate.bat

echo [3/4] Installing dependencies...
echo This may take a few minutes on first run...
python -m pip install --upgrade pip >nul 2>&1
pip install fastapi uvicorn pydantic pydantic-settings python-dotenv

echo [4/4] Starting server...
echo.
echo ========================================
echo Server starting on http://localhost:8000
echo ========================================
echo.
echo Visit:
echo   - http://localhost:8000 (Welcome page)
echo   - http://localhost:8000/health (Health check)
echo   - http://localhost:8000/docs (API docs)
echo.
echo Press CTRL+C to stop the server
echo.

python src/main.py
</file>

<file path="requirements/base.txt">
# Core Framework
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
pydantic-settings>=2.1.0

# LLM Providers
openai>=1.3.0
anthropic>=0.7.0

# Vector Databases
pinecone-client>=3.0.0

# Database
sqlalchemy[asyncio]>=2.0.0
asyncpg>=0.29.0
alembic>=1.13.0

# NLP & Document Processing
spacy>=3.7.0
langchain>=0.1.0
PyPDF2>=3.0.0
python-docx>=1.1.0

# Utilities
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4
httpx>=0.25.0
</file>

<file path="requirements/dev.txt">
-r base.txt

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0

# Code Quality
ruff>=0.1.6
black>=23.11.0
mypy>=1.7.0

# Development Tools
ipython>=8.18.0
python-dotenv>=1.0.0
</file>

<file path="requirements/prod.txt">
-r base.txt

# Production-specific dependencies
gunicorn>=21.2.0
</file>

<file path="src/__init__.py">
"""Elios AI Interview Service package."""

__version__ = "0.1.0"
</file>

<file path="src/adapters/__init__.py">
"""Adapters package."""
</file>

<file path="src/adapters/api/__init__.py">
"""API adapters package."""
</file>

<file path="src/adapters/api/rest/__init__.py">
"""REST API routes package."""
</file>

<file path="src/adapters/api/websocket/__init__.py">
"""WebSocket handlers for real-time interview communication."""

from .connection_manager import manager
from .interview_handler import handle_interview_websocket

__all__ = ["manager", "handle_interview_websocket"]
</file>

<file path="src/adapters/api/websocket/connection_manager.py">
"""WebSocket connection manager for interview sessions."""

import logging
from uuid import UUID

from fastapi import WebSocket

logger = logging.getLogger(__name__)


class ConnectionManager:
    """Manage WebSocket connections for interviews."""

    def __init__(self):
        # interview_id â†’ websocket
        self.active_connections: dict[UUID, WebSocket] = {}

    async def connect(self, interview_id: UUID, websocket: WebSocket):
        """Accept and register WebSocket connection.

        Args:
            interview_id: Interview UUID
            websocket: WebSocket connection
        """
        await websocket.accept()
        self.active_connections[interview_id] = websocket
        logger.info(f"WebSocket connected for interview {interview_id}")

    def disconnect(self, interview_id: UUID):
        """Remove connection.

        Args:
            interview_id: Interview UUID
        """
        if interview_id in self.active_connections:
            del self.active_connections[interview_id]
            logger.info(f"WebSocket disconnected for interview {interview_id}")

    async def send_message(self, interview_id: UUID, message: dict):
        """Send message to specific interview connection.

        Args:
            interview_id: Interview UUID
            message: Message dictionary to send
        """
        websocket = self.active_connections.get(interview_id)
        if websocket:
            await websocket.send_json(message)

    async def broadcast(self, message: dict):
        """Send message to all connections.

        Args:
            message: Message dictionary to broadcast
        """
        for websocket in self.active_connections.values():
            await websocket.send_json(message)


# Global instance
manager = ConnectionManager()
</file>

<file path="src/adapters/cv_processing/__init__.py">
"""CV processing adapters package."""
</file>

<file path="src/adapters/llm/__init__.py">
"""LLM adapters package."""

from .openai_adapter import OpenAIAdapter

__all__ = ["OpenAIAdapter"]
</file>

<file path="src/adapters/mock/mock_analytics.py">
"""Mock Analytics adapter for development and testing."""

from collections import defaultdict
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.models.question import Question
from ...domain.ports.analytics_port import AnalyticsPort


class MockAnalyticsAdapter(AnalyticsPort):
    """Mock analytics adapter that simulates performance tracking.

    This adapter provides in-memory analytics calculations without
    requiring external analytics services or databases.

    Storage is maintained in memory per instance, so each test
    should use a fresh instance for isolation.
    """

    def __init__(self) -> None:
        """Initialize mock analytics with empty storage."""
        # Store evaluations per interview
        self._evaluations: dict[UUID, list[Answer]] = defaultdict(list)

        # Store candidate history (mock historical data)
        self._candidate_history: dict[UUID, list[dict[str, Any]]] = {}

    async def record_answer_evaluation(
        self,
        interview_id: UUID,
        answer: Answer,
    ) -> None:
        """Record answer evaluation for analytics.

        Args:
            interview_id: Interview identifier
            answer: Answer with evaluation data
        """
        self._evaluations[interview_id].append(answer)

    async def get_interview_statistics(
        self,
        interview_id: UUID,
    ) -> dict[str, Any]:
        """Get statistics for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            Dictionary with interview statistics
        """
        answers = self._evaluations.get(interview_id, [])

        if not answers:
            return {
                "interview_id": str(interview_id),
                "question_count": 0,
                "answers_count": 0,
                "avg_score": 0.0,
                "completion_rate": 0.0,
                "time_spent_minutes": 0,
            }

        # Calculate average score
        scores = [
            a.evaluation.score
            for a in answers
            if a.evaluation is not None
        ]
        avg_score = sum(scores) / len(scores) if scores else 0.0

        # Mock time calculation (2-5 min per question)
        time_spent = len(answers) * 3.5  # Average 3.5 minutes per answer

        return {
            "interview_id": str(interview_id),
            "question_count": len(answers),
            "answers_count": len([a for a in answers if a.text]),
            "avg_score": round(avg_score, 2),
            "completion_rate": round(len([a for a in answers if a.text]) / len(answers) * 100, 2) if answers else 0.0,
            "time_spent_minutes": round(time_spent, 1),
            "highest_score": max(scores) if scores else 0.0,
            "lowest_score": min(scores) if scores else 0.0,
        }

    async def get_candidate_performance_history(
        self,
        candidate_id: UUID,
    ) -> list[dict[str, Any]]:
        """Get candidate's performance across all interviews.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of historical interview performance data
        """
        # Check if we have mock history for this candidate
        if candidate_id in self._candidate_history:
            return self._candidate_history[candidate_id]

        # Generate mock historical data (0-3 past interviews)
        # Simulate improvement over time
        history = []
        num_past_interviews = 2  # Mock: 2 past interviews

        for i in range(num_past_interviews):
            history.append({
                "interview_date": f"2024-{10-i:02d}-15",
                "avg_score": round(65.0 + (i * 8.0), 2),  # Improving scores
                "questions_answered": 5 + i,
                "completion_rate": round(80.0 + (i * 10.0), 2),
                "strong_skills": ["Python", "SQL"] if i == 0 else ["Python", "FastAPI", "PostgreSQL"],
                "weak_skills": ["System Design", "Architecture"] if i == 0 else ["System Design"],
            })

        self._candidate_history[candidate_id] = history
        return history

    async def generate_improvement_recommendations(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[Answer],
    ) -> list[str]:
        """Generate improvement recommendations based on performance.

        Args:
            interview_id: Interview identifier
            questions: Questions asked
            answers: Answers with evaluations

        Returns:
            List of improvement recommendations
        """
        if not answers:
            return ["Complete the interview to receive personalized recommendations"]

        # Calculate average score
        scores = [
            a.evaluation.score
            for a in answers
            if a.evaluation is not None
        ]
        avg_score = sum(scores) / len(scores) if scores else 0.0

        # Collect weaknesses from evaluations
        all_weaknesses = []
        weak_skills = []

        for answer, question in zip(answers, questions, strict=True):
            if answer.evaluation:
                all_weaknesses.extend(answer.evaluation.weaknesses)

                # Track skills with low scores (<70)
                if answer.evaluation.score < 70.0:
                    weak_skills.extend(question.skills)

        # Deduplicate and count weaknesses
        weakness_counts: dict[str, int] = defaultdict(int)
        for weakness in all_weaknesses:
            weakness_counts[weakness] += 1

        # Sort by frequency
        top_weaknesses = sorted(
            weakness_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]

        # Generate recommendations based on score ranges
        recommendations = []

        if avg_score < 60:
            recommendations.append("Focus on understanding fundamental concepts before diving into advanced topics")
            recommendations.append("Practice explaining technical concepts clearly and concisely")
            recommendations.append("Review and strengthen basic knowledge in your core skills")

            # Add specific weakness-based recommendations
            for weakness, _ in top_weaknesses[:2]:
                if "depth" in weakness.lower():
                    recommendations.append("Study topics in more depth with practical examples")
                elif "example" in weakness.lower():
                    recommendations.append("Prepare concrete examples from your experience")

        elif avg_score < 80:
            recommendations.append("Strengthen your understanding of intermediate concepts")
            recommendations.append("Practice providing more detailed and structured answers")

            # Add skill-specific recommendations
            if weak_skills:
                unique_weak_skills = list(set(weak_skills))[:2]
                for skill in unique_weak_skills:
                    recommendations.append(f"Improve your knowledge in {skill} through hands-on projects")

            # Add weakness-based recommendations
            for weakness, _ in top_weaknesses[:2]:
                recommendations.append(f"Work on: {weakness}")

        else:
            recommendations.append("Excellent performance! Continue building expertise in advanced topics")
            recommendations.append("Consider exploring cutting-edge technologies and patterns")

            # Even high performers can improve
            if top_weaknesses:
                recommendations.append(f"Minor area for growth: {top_weaknesses[0][0]}")

        # Limit to 3-5 recommendations
        return recommendations[:5]

    async def calculate_skill_scores(
        self,
        answers: list[Answer],
        questions: list[Question],
    ) -> dict[str, float]:
        """Calculate scores per skill based on answers.

        Args:
            answers: List of evaluated answers
            questions: Corresponding questions

        Returns:
            Dictionary mapping skill names to average scores
        """
        if not answers or not questions or len(answers) != len(questions):
            return {}

        # Group scores by skill
        skill_scores: dict[str, list[float]] = defaultdict(list)

        for answer, question in zip(answers, questions, strict=True):
            if answer.evaluation is None:
                continue

            score = answer.evaluation.score

            # Associate this score with all skills in the question
            for skill in question.skills:
                skill_scores[skill].append(score)

        # Calculate average score per skill
        result = {}
        for skill, scores in skill_scores.items():
            result[skill] = round(sum(scores) / len(scores), 2)

        return result
</file>

<file path="src/adapters/mock/mock_cv_analyzer.py">
"""Mock CV Analyzer adapter for development and testing."""

import random
from pathlib import Path
from uuid import UUID, uuid4

from ...domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from ...domain.ports.cv_analyzer_port import CVAnalyzerPort


class MockCVAnalyzerAdapter(CVAnalyzerPort):
    """Mock CV analyzer that returns realistic but simulated analysis.

    This adapter simulates CV parsing and skill extraction without
    requiring actual document processing or LLM API calls.

    The mock provides deterministic results based on filename patterns:
    - Files with "junior" â†’ 2-3 skills, 1-2 years exp, EASY difficulty
    - Files with "senior" â†’ 5-6 skills, 6-10 years exp, HARD difficulty
    - Default (mid-level) â†’ 4-5 skills, 3-5 years exp, MEDIUM difficulty
    """

    # Skill database organized by experience level
    JUNIOR_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="intermediate", years=1.5),
        ExtractedSkill(skill="Git", category="technical", proficiency="beginner", years=1.0),
        ExtractedSkill(skill="SQL", category="technical", proficiency="beginner", years=0.5),
        ExtractedSkill(skill="Communication", category="soft", proficiency="intermediate"),
        ExtractedSkill(skill="Team Collaboration", category="soft", proficiency="beginner"),
    ]

    MID_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="advanced", years=3.5),
        ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate", years=2.0),
        ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate", years=2.5),
        ExtractedSkill(skill="Docker", category="technical", proficiency="intermediate", years=1.5),
        ExtractedSkill(skill="REST APIs", category="technical", proficiency="advanced", years=3.0),
        ExtractedSkill(skill="Problem Solving", category="soft", proficiency="advanced"),
        ExtractedSkill(skill="Leadership", category="soft", proficiency="intermediate"),
    ]

    SENIOR_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="expert", years=7.0),
        ExtractedSkill(skill="System Design", category="technical", proficiency="expert", years=5.0),
        ExtractedSkill(skill="Microservices", category="technical", proficiency="advanced", years=4.0),
        ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="expert", years=6.0),
        ExtractedSkill(skill="AWS", category="technical", proficiency="advanced", years=4.5),
        ExtractedSkill(skill="Architecture", category="technical", proficiency="expert", years=5.5),
        ExtractedSkill(skill="Leadership", category="soft", proficiency="expert"),
        ExtractedSkill(skill="Mentoring", category="soft", proficiency="advanced"),
    ]

    SUPPORTED_EXTENSIONS = {".pdf", ".doc", ".docx"}

    async def extract_text_from_file(self, file_path: str) -> str:
        """Extract mock text from file.

        Args:
            file_path: Path to CV file

        Returns:
            Mock CV text content

        Raises:
            ValueError: If file extension not supported
        """
        path = Path(file_path)
        if path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
            raise ValueError(
                f"Unsupported file format: {path.suffix}. "
                f"Supported formats: {', '.join(self.SUPPORTED_EXTENSIONS)}"
            )

        # Return realistic mock CV text
        return """
        John Doe
        Software Engineer
        Email: john.doe@example.com
        Phone: +1-555-0100

        PROFESSIONAL SUMMARY
        Experienced software engineer with strong background in backend development,
        API design, and cloud infrastructure. Passionate about clean code and scalable
        systems.

        EXPERIENCE
        Senior Software Engineer at Tech Corp (2020-Present)
        - Designed and implemented microservices architecture serving 1M+ users
        - Led team of 5 engineers in migrating monolith to event-driven architecture
        - Reduced API latency by 40% through optimization and caching strategies
        - Mentored junior developers and conducted code reviews

        Software Engineer at StartupXYZ (2018-2020)
        - Developed REST APIs using Python and FastAPI
        - Implemented PostgreSQL database schema and query optimization
        - Built CI/CD pipelines with Docker and GitHub Actions
        - Collaborated with frontend team on API integration

        TECHNICAL SKILLS
        Languages: Python, SQL, JavaScript
        Frameworks: FastAPI, Django, Flask
        Databases: PostgreSQL, Redis, MongoDB
        Cloud: AWS (EC2, S3, Lambda), Docker, Kubernetes
        Tools: Git, GitHub Actions, Terraform

        EDUCATION
        Bachelor of Science in Computer Science
        University of Technology, 2018

        CERTIFICATIONS
        - AWS Certified Solutions Architect
        - Python Professional Certification
        """

    async def analyze_cv(
        self,
        cv_file_path: str,
        candidate_id: str,
    ) -> CVAnalysis:
        """Analyze CV and extract structured information.

        Args:
            cv_file_path: Path to CV file
            candidate_id: UUID of the candidate

        Returns:
            CVAnalysis with extracted skills and metadata
        """
        # Extract text from file
        extracted_text = await self.extract_text_from_file(cv_file_path)

        # Detect experience level from filename
        filename = Path(cv_file_path).stem.lower()
        if "junior" in filename:
            experience_level = "junior"
            skills = self.JUNIOR_SKILLS[:3]  # 2-3 skills
            years = random.uniform(1.0, 2.0)
            difficulty = "easy"
            education = "Bachelor's"
        elif "senior" in filename:
            experience_level = "senior"
            skills = self.SENIOR_SKILLS[:6]  # 5-6 skills
            years = random.uniform(6.0, 10.0)
            difficulty = "hard"
            education = "Master's"
        else:
            experience_level = "mid"
            skills = self.MID_SKILLS[:5]  # 4-5 skills
            years = random.uniform(3.0, 5.0)
            difficulty = "medium"
            education = "Bachelor's"

        # Generate suggested topics from skills
        technical_skills = [s for s in skills if s.category == "technical"]
        suggested_topics = [skill.name for skill in technical_skills]

        # Add some topic variations
        if any(s.name in ["Python", "FastAPI"] for s in skills):
            suggested_topics.append("Backend Development")
        if any(s.name in ["PostgreSQL", "SQL"] for s in skills):
            suggested_topics.append("Database Design")
        if any(s.name in ["System Design", "Architecture"] for s in skills):
            suggested_topics.append("System Architecture")

        # Create CV analysis
        return CVAnalysis(
            id=uuid4(),
            candidate_id=UUID(candidate_id),
            cv_file_path=cv_file_path,
            extracted_text=extracted_text,
            skills=skills,
            work_experience_years=round(years, 1),
            education_level=education,
            suggested_topics=suggested_topics[:5],  # Limit to 5 topics
            suggested_difficulty=difficulty,
            embedding=None,  # Mock doesn't generate embeddings
            summary=f"Mock CV analysis: {experience_level.title()}-level candidate with {years:.1f} years of experience",
            metadata={
                "experience_level": experience_level,
                "file_name": Path(cv_file_path).name,
                "mock_adapter": True,
            },
        )
</file>

<file path="src/adapters/mock/mock_stt_adapter.py">
"""Mock Speech-to-Text adapter for development and testing."""


from ...domain.ports.speech_to_text_port import SpeechToTextPort


class MockSTTAdapter(SpeechToTextPort):
    """Mock STT adapter that returns placeholder transcriptions.

    This adapter simulates speech-to-text behavior for development
    and testing without requiring actual STT service calls.
    """

    async def transcribe_audio(
        self,
        audio_file_path: str,
        language: str = "en-US",
    ) -> str:
        """Mock transcription from file."""
        return f"[Mock transcription from {audio_file_path}]"

    async def transcribe_stream(
        self,
        audio_stream: bytes,
        language: str = "en-US",
    ) -> str:
        """Mock stream transcription."""
        stream_size = len(audio_stream)
        return f"[Mock transcription from audio stream ({stream_size} bytes)]"

    async def detect_language(
        self,
        audio_file_path: str,
    ) -> str | None:
        """Mock language detection."""
        return "en-US"
</file>

<file path="src/adapters/mock/mock_tts_adapter.py">
"""Mock Text-to-Speech adapter for development and testing."""


from ...domain.ports.text_to_speech_port import TextToSpeechPort


class MockTTSAdapter(TextToSpeechPort):
    """Mock TTS adapter that returns minimal audio data.

    This adapter simulates text-to-speech behavior for development
    and testing without requiring actual TTS service calls.
    """

    async def synthesize_speech(
        self,
        text: str,
        language: str = "en-US",
        voice: str | None = None,
    ) -> bytes:
        """Mock speech synthesis with minimal WAV structure."""
        # Return mock audio bytes (minimal WAV header + data)
        # Real WAV would have proper RIFF structure
        wav_header = b"RIFF"
        wav_header += (100).to_bytes(4, "little")  # File size
        wav_header += b"WAVE"
        wav_header += b"fmt "
        wav_header += (16).to_bytes(4, "little")  # Format chunk size
        wav_header += b"\x00" * 16  # Format data
        wav_header += b"data"
        wav_header += (64).to_bytes(4, "little")  # Data chunk size
        wav_header += b"\x00" * 64  # Audio data (silence)

        return wav_header

    async def save_speech_to_file(
        self,
        text: str,
        output_path: str,
        language: str = "en-US",
        voice: str | None = None,
    ) -> str:
        """Mock save to file."""
        # In real implementation, would save actual audio
        audio_bytes = await self.synthesize_speech(text, language, voice)
        with open(output_path, "wb") as f:
            f.write(audio_bytes)
        return output_path

    async def list_available_voices(
        self,
        language: str | None = None,
    ) -> list[dict]:
        """Mock voice list."""
        voices = [
            {
                "name": "mock-en-US-male-1",
                "language": "en-US",
                "gender": "male",
                "quality": "standard",
            },
            {
                "name": "mock-en-US-female-1",
                "language": "en-US",
                "gender": "female",
                "quality": "standard",
            },
            {
                "name": "mock-en-GB-male-1",
                "language": "en-GB",
                "gender": "male",
                "quality": "premium",
            },
        ]

        if language:
            return [v for v in voices if v["language"] == language]
        return voices
</file>

<file path="src/adapters/mock/mock_vector_search_adapter.py">
"""Mock vector search adapter for development and testing."""

import random
from typing import Any
from uuid import UUID

from ...domain.models.question import DifficultyLevel
from ...domain.ports.vector_search_port import VectorSearchPort


class MockVectorSearchAdapter(VectorSearchPort):
    """Mock vector search adapter that returns fake data.

    This adapter simulates vector database behavior without requiring
    external services like Pinecone, Weaviate, or ChromaDB.
    """

    def __init__(self):
        """Initialize mock adapter with seeded question IDs from database."""
        # Use actual question IDs from seed_data.sql
        # These IDs will exist in the database after running seed script
        self._mock_question_ids = [
            UUID("650e8400-e29b-41d4-a716-446655440001"),  # JS var/let/const - EASY
            UUID("650e8400-e29b-41d4-a716-446655440002"),  # REST API - EASY
            UUID("650e8400-e29b-41d4-a716-446655440003"),  # async/await - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440004"),  # SOLID principles - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440005"),  # Database normalization - HARD
            UUID("650e8400-e29b-41d4-a716-446655440006"),  # Microservices - HARD
            UUID("650e8400-e29b-41d4-a716-446655440007"),  # Behavioral - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440008"),  # Behavioral teamwork - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440009"),  # Reverse string - EASY
            UUID("650e8400-e29b-41d4-a716-446655440010"),  # Non-repeating char - MEDIUM
        ]

    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Mock storing question embedding (no-op)."""
        # In mock mode, we just acknowledge the storage
        pass

    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Mock storing CV embedding (no-op)."""
        # In mock mode, we just acknowledge the storage
        pass

    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Return mock similar questions.

        Args:
            query_embedding: Query vector (ignored in mock)
            top_k: Number of results to return
            filters: Optional filters (difficulty, skills, etc.)

        Returns:
            List of mock question matches with metadata
        """
        # Generate mock results
        results = []
        difficulty = filters.get("difficulty", DifficultyLevel.MEDIUM) if filters else DifficultyLevel.MEDIUM

        for i in range(min(top_k, len(self._mock_question_ids))):
            # Generate realistic similarity scores (higher for first results)
            similarity = random.uniform(0.75, 0.95) - (i * 0.02)

            results.append({
                "question_id": str(self._mock_question_ids[i]),
                "similarity_score": similarity,
                "metadata": {
                    "difficulty": difficulty,
                    "skills": ["Python", "FastAPI", "PostgreSQL"][: i % 3 + 1],
                    "tags": ["backend", "api", "database"][: i % 3 + 1],
                },
            })

        return results

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Return mock similarity score between 0.7 and 0.95."""
        return random.uniform(0.7, 0.95)

    async def get_embedding(
        self,
        text: str,
    ) -> list[float]:
        """Generate mock embedding vector.

        Args:
            text: Text to embed

        Returns:
            Mock 1536-dimensional vector (OpenAI embedding size)
        """
        # Return a mock embedding with realistic dimensions
        # OpenAI embeddings are 1536-dimensional
        return [random.uniform(-1.0, 1.0) for _ in range(1536)]

    async def delete_embeddings(
        self,
        ids: list[UUID],
    ) -> None:
        """Mock deleting embeddings (no-op)."""
        # In mock mode, we just acknowledge the deletion
        pass
</file>

<file path="src/adapters/persistence/follow_up_question_repository.py">
"""PostgreSQL implementation of FollowUpQuestionRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.follow_up_question import FollowUpQuestion
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from .mappers import FollowUpQuestionMapper
from .models import FollowUpQuestionModel


class PostgreSQLFollowUpQuestionRepository(FollowUpQuestionRepositoryPort):
    """PostgreSQL implementation of follow-up question repository.

    This adapter implements the FollowUpQuestionRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, follow_up_question: FollowUpQuestion) -> FollowUpQuestion:
        """Save a new follow-up question to the database.

        Args:
            follow_up_question: FollowUpQuestion to save

        Returns:
            Saved follow-up question
        """
        db_model = FollowUpQuestionMapper.to_db_model(follow_up_question)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return FollowUpQuestionMapper.to_domain(db_model)

    async def get_by_id(self, question_id: UUID) -> FollowUpQuestion | None:
        """Retrieve a follow-up question by ID.

        Args:
            question_id: Follow-up question identifier

        Returns:
            FollowUpQuestion if found, None otherwise
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(FollowUpQuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()
        return FollowUpQuestionMapper.to_domain(db_model) if db_model else None

    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            List of follow-up questions ordered by order_in_sequence
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel)
            .where(FollowUpQuestionModel.parent_question_id == parent_question_id)
            .order_by(FollowUpQuestionModel.order_in_sequence)
        )
        db_models = result.scalars().all()
        return [FollowUpQuestionMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_interview_id(self, interview_id: UUID) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of follow-up questions ordered by created_at
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel)
            .where(FollowUpQuestionModel.interview_id == interview_id)
            .order_by(FollowUpQuestionModel.created_at)
        )
        db_models = result.scalars().all()
        return [FollowUpQuestionMapper.to_domain(db_model) for db_model in db_models]

    async def count_by_parent_question_id(self, parent_question_id: UUID) -> int:
        """Count follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            Count of follow-up questions
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(
                FollowUpQuestionModel.parent_question_id == parent_question_id
            )
        )
        db_models = result.scalars().all()
        return len(db_models)

    async def delete(self, question_id: UUID) -> bool:
        """Delete a follow-up question.

        Args:
            question_id: Follow-up question identifier

        Returns:
            True if deleted, False if not found
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(FollowUpQuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()

        if db_model:
            await self.session.delete(db_model)
            await self.session.commit()
            return True
        return False
</file>

<file path="src/adapters/speech/__init__.py">
"""Speech adapters package."""
</file>

<file path="src/adapters/vector_db/__init__.py">
"""Vector database adapters package."""

from .pinecone_adapter import PineconeAdapter

__all__ = ["PineconeAdapter"]
</file>

<file path="src/application/__init__.py">
"""Application layer package."""
</file>

<file path="src/application/dto/answer_dto.py">
"""Answer DTOs for REST API request/response."""

from uuid import UUID

from pydantic import BaseModel


class SubmitAnswerRequest(BaseModel):
    """Request to submit an answer to a question."""
    question_id: UUID
    answer_text: str
    audio_file_path: str | None = None


class AnswerEvaluationResponse(BaseModel):
    """Response with answer evaluation results."""
    answer_id: UUID
    question_id: UUID
    score: float
    feedback: str
    strengths: list[str]
    weaknesses: list[str]
    improvement_suggestions: list[str]
    next_question_available: bool
</file>

<file path="src/application/dto/websocket_dto.py">
"""WebSocket message DTOs for real-time interview communication."""

from typing import Any, Literal
from uuid import UUID

from pydantic import BaseModel


# Base message
class WebSocketMessage(BaseModel):
    """Base WebSocket message format."""
    type: str
    payload: dict[str, Any]


# Client â†’ Server messages
class TextAnswerMessage(BaseModel):
    """Client sends text answer."""
    type: Literal["text_answer"]
    question_id: UUID
    answer_text: str


class AudioChunkMessage(BaseModel):
    """Client sends audio chunk for voice answer."""
    type: Literal["audio_chunk"]
    chunk_data: str  # base64 encoded
    is_final: bool


class GetNextQuestionMessage(BaseModel):
    """Client requests next question."""
    type: Literal["get_next_question"]


# Server â†’ Client messages
class QuestionMessage(BaseModel):
    """Server sends question to client."""
    type: Literal["question"]
    question_id: UUID
    text: str
    question_type: str
    difficulty: str
    index: int
    total: int
    audio_data: str | None = None  # base64 encoded TTS


class TranscriptionMessage(BaseModel):
    """Server sends transcription of audio."""
    type: Literal["transcription"]
    text: str
    is_final: bool


class EvaluationMessage(BaseModel):
    """Server sends answer evaluation."""
    type: Literal["evaluation"]
    answer_id: UUID
    score: float
    feedback: str
    strengths: list[str]
    weaknesses: list[str]


class InterviewCompleteMessage(BaseModel):
    """Server notifies interview completion."""
    type: Literal["interview_complete"]
    interview_id: UUID
    overall_score: float
    total_questions: int
    feedback_url: str


class ErrorMessage(BaseModel):
    """Server sends error message."""
    type: Literal["error"]
    code: str
    message: str
</file>

<file path="src/application/use_cases/complete_interview.py">
"""Complete interview use case."""

from uuid import UUID

from ...domain.models.interview import Interview, InterviewStatus
from ...domain.ports.interview_repository_port import InterviewRepositoryPort


class CompleteInterviewUseCase:
    """Complete interview and mark as finished."""

    def __init__(self, interview_repository: InterviewRepositoryPort):
        self.interview_repo = interview_repository

    async def execute(self, interview_id: UUID) -> Interview:
        """Mark interview as completed.

        Args:
            interview_id: The interview UUID

        Returns:
            Completed interview

        Raises:
            ValueError: If interview not found or invalid state
        """
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        if interview.status != InterviewStatus.IN_PROGRESS:
            raise ValueError(
                f"Cannot complete interview with status: {interview.status}"
            )

        interview.complete()
        return await self.interview_repo.update(interview)
</file>

<file path="src/application/use_cases/get_next_question.py">
"""Get next question use case."""

from uuid import UUID

from ...domain.models.question import Question
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort


class GetNextQuestionUseCase:
    """Get next question in interview sequence."""

    def __init__(
        self,
        interview_repository: InterviewRepositoryPort,
        question_repository: QuestionRepositoryPort,
    ):
        self.interview_repo = interview_repository
        self.question_repo = question_repository

    async def execute(self, interview_id: UUID) -> Question | None:
        """Get next unanswered question.

        Args:
            interview_id: The interview UUID

        Returns:
            Next question or None if interview complete

        Raises:
            ValueError: If interview not found
        """
        # Get interview
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        # Check if more questions available
        if not interview.has_more_questions():
            return None

        # Get current question
        question_id = interview.get_current_question_id()
        if not question_id:
            return None

        question = await self.question_repo.get_by_id(question_id)
        return question
</file>

<file path="src/application/use_cases/plan_interview.py">
"""Plan interview use case."""

import logging
from datetime import datetime
from typing import Any
from uuid import UUID

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.models.interview import Interview, InterviewStatus
from ...domain.models.question import DifficultyLevel, Question, QuestionType
from ...domain.ports.cv_analysis_repository_port import CVAnalysisRepositoryPort
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort

logger = logging.getLogger(__name__)


class PlanInterviewUseCase:
    """Use case for planning interview questions with ideal answers.

    This orchestrates the pre-planning phase:
    1. Load CV analysis
    2. Calculate n based on skill diversity (max 5)
    3. Generate n questions with ideal answers + rationale
    4. Store questions and mark interview as READY
    """

    def __init__(
        self,
        llm: LLMPort,
        cv_analysis_repo: CVAnalysisRepositoryPort,
        interview_repo: InterviewRepositoryPort,
        question_repo: QuestionRepositoryPort,
    ):
        """Initialize use case with required ports.

        Args:
            llm: LLM service for question generation
            cv_analysis_repo: CV analysis storage
            interview_repo: Interview storage
            question_repo: Question storage
        """
        self.llm = llm
        self.cv_analysis_repo = cv_analysis_repo
        self.interview_repo = interview_repo
        self.question_repo = question_repo

    async def execute(
        self,
        cv_analysis_id: UUID,
        candidate_id: UUID,
    ) -> Interview:
        """Plan interview by generating n questions with ideal answers.

        Args:
            cv_analysis_id: CV analysis to base questions on
            candidate_id: Candidate being interviewed

        Returns:
            Interview entity with status=READY

        Raises:
            ValueError: If CV analysis not found
            Exception: If question generation fails
        """
        logger.info(
            "Starting interview planning",
            extra={
                "cv_analysis_id": str(cv_analysis_id),
                "candidate_id": str(candidate_id),
            },
        )

        # Step 1: Load CV analysis
        cv_analysis = await self.cv_analysis_repo.get_by_id(cv_analysis_id)
        if not cv_analysis:
            raise ValueError(f"CV analysis {cv_analysis_id} not found")

        # Step 2: Calculate n based on skill diversity
        n = self._calculate_question_count(cv_analysis)
        logger.info(f"Calculated n={n} questions based on CV complexity")

        # Step 3: Create interview (status=PREPARING)
        interview = Interview(
            candidate_id=candidate_id,
            status=InterviewStatus.PREPARING,
            cv_analysis_id=cv_analysis_id,
        )
        await self.interview_repo.save(interview)

        # Step 4: Generate n questions (sequential for MVP)
        question_ids = []
        try:
            for i in range(n):
                question = await self._generate_question_with_ideal_answer(cv_analysis, i, n)
                await self.question_repo.save(question)
                question_ids.append(question.id)
                logger.info(f"Generated question {i + 1}/{n}: {question.id}")

        except Exception as e:
            logger.error(f"Failed to generate questions: {e}")
            # Rollback: Delete partially created questions
            for qid in question_ids:
                try:
                    await self.question_repo.delete(qid)
                except Exception:
                    pass  # Best effort cleanup
            raise

        # Step 5: Update interview (status=READY)
        interview.question_ids = question_ids
        interview.plan_metadata = {
            "n": n,
            "generated_at": datetime.utcnow().isoformat(),
            "strategy": "adaptive_planning_v1",
            "cv_summary": cv_analysis.summary or "No summary",
        }
        interview.mark_ready(cv_analysis_id)
        await self.interview_repo.update(interview)

        logger.info(
            "Interview planning complete",
            extra={
                "interview_id": str(interview.id),
                "question_count": n,
            },
        )

        return interview

    def _calculate_question_count(self, cv_analysis: CVAnalysis) -> int:
        """Calculate question count based on skill diversity only.

        Args:
            cv_analysis: Analyzed CV data

        Returns:
            Question count (2-5)
        """
        skill_count = len(cv_analysis.skills)

        # Skill-only calculation (ignore experience years)
        if skill_count <= 2:
            n = 2
        elif skill_count <= 4:
            n = 3
        elif skill_count <= 7:
            n = 4
        else:
            n = 5  # Max 5 questions

        return n

    async def _generate_question_with_ideal_answer(
        self,
        cv_analysis: CVAnalysis,
        index: int,
        total: int,
    ) -> Question:
        """Generate single question with ideal answer + rationale.

        Args:
            cv_analysis: CV data for context
            index: Current question index (0-based)
            total: Total questions to generate

        Returns:
            Question entity with ideal_answer + rationale populated
        """
        # Determine question type/difficulty based on index
        question_type, difficulty = self._get_question_distribution(index, total)

        # Select skill to test
        skills = cv_analysis.get_top_skills(limit=5)
        skill = skills[index % len(skills)].name if skills else "general knowledge"

        # Generate question
        context = {
            "summary": cv_analysis.summary or "No summary",
            "skills": [s.name for s in skills],
            "experience": cv_analysis.work_experience_years or 0,
        }

        # Use existing generate_question method
        question_text = await self.llm.generate_question(
            context=context,
            skill=skill,
            difficulty=difficulty.value,
        )

        # Generate ideal answer using new method
        ideal_answer = await self.llm.generate_ideal_answer(
            question_text=question_text,
            context=context,
        )

        # Generate rationale using new method
        rationale = await self.llm.generate_rationale(
            question_text=question_text,
            ideal_answer=ideal_answer,
        )

        # Create Question entity
        question = Question(
            text=question_text,
            question_type=question_type,
            difficulty=difficulty,
            skills=[skill],
            ideal_answer=ideal_answer,
            rationale=rationale,
        )

        return question

    def _get_question_distribution(
        self, index: int, total: int
    ) -> tuple[QuestionType, DifficultyLevel]:
        """Determine question type and difficulty based on index.

        Distribution:
        - 60% technical, 30% behavioral, 10% situational
        - 50% easy, 30% medium, 20% hard

        Args:
            index: Current question index
            total: Total questions

        Returns:
            (QuestionType, DifficultyLevel)
        """
        # Question type distribution
        technical_count = int(total * 0.6)
        behavioral_count = int(total * 0.3)

        if index < technical_count:
            q_type = QuestionType.TECHNICAL
        elif index < technical_count + behavioral_count:
            q_type = QuestionType.BEHAVIORAL
        else:
            q_type = QuestionType.SITUATIONAL

        # Difficulty distribution
        easy_count = int(total * 0.5)
        medium_count = int(total * 0.3)

        if index < easy_count:
            difficulty = DifficultyLevel.EASY
        elif index < easy_count + medium_count:
            difficulty = DifficultyLevel.MEDIUM
        else:
            difficulty = DifficultyLevel.HARD

        return q_type, difficulty
</file>

<file path="src/domain/__init__.py">
"""Domain layer package."""
</file>

<file path="src/domain/ports/follow_up_question_repository_port.py">
"""Follow-up question repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.follow_up_question import FollowUpQuestion


class FollowUpQuestionRepositoryPort(ABC):
    """Interface for follow-up question persistence operations.

    This port abstracts database operations for follow-up questions,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, follow_up_question: FollowUpQuestion) -> FollowUpQuestion:
        """Save a follow-up question.

        Args:
            follow_up_question: FollowUpQuestion to save

        Returns:
            Saved follow-up question with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, question_id: UUID) -> FollowUpQuestion | None:
        """Retrieve a follow-up question by ID.

        Args:
            question_id: Follow-up question identifier

        Returns:
            FollowUpQuestion if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            List of follow-up questions ordered by order_in_sequence
        """
        pass

    @abstractmethod
    async def get_by_interview_id(self, interview_id: UUID) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of follow-up questions ordered by created_at
        """
        pass

    @abstractmethod
    async def count_by_parent_question_id(self, parent_question_id: UUID) -> int:
        """Count follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            Count of follow-up questions
        """
        pass

    @abstractmethod
    async def delete(self, question_id: UUID) -> bool:
        """Delete a follow-up question.

        Args:
            question_id: Follow-up question identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/services/__init__.py">
"""Domain services package."""
</file>

<file path="src/infrastructure/__init__.py">
"""Infrastructure layer package."""
</file>

<file path="src/infrastructure/config/__init__.py">
"""Configuration package."""

from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"]
</file>

<file path="src/infrastructure/database/base.py">
"""SQLAlchemy declarative base and common utilities."""

from sqlalchemy.orm import DeclarativeBase


class Base(DeclarativeBase):
    """Base class for all SQLAlchemy models.

    All database models should inherit from this class.
    This provides the declarative base for SQLAlchemy 2.0+.
    """

    pass
</file>

<file path="src/infrastructure/dependency_injection/__init__.py">
"""Dependency injection package."""

from .container import Container, get_container

__all__ = ["Container", "get_container"]
</file>

<file path="test_basic.py">
"""Quick test script to verify setup."""

import asyncio
from src.domain.models import Candidate, Interview, Question, DifficultyLevel, QuestionType
from src.infrastructure.config import get_settings


async def main():
    print("=== Testing Elios AI Service Setup ===\n")

    # Test 1: Configuration
    print("1. Testing Configuration...")
    settings = get_settings()
    print(f"   âœ“ App Name: {settings.app_name}")
    print(f"   âœ“ Environment: {settings.environment}")
    print(f"   âœ“ API Port: {settings.api_port}\n")

    # Test 2: Domain Models
    print("2. Testing Domain Models...")
    candidate = Candidate(name="Test User", email="test@example.com")
    print(f"   âœ“ Created Candidate: {candidate.name} ({candidate.id})")

    interview = Interview(candidate_id=candidate.id)
    print(f"   âœ“ Created Interview: {interview.id}")
    print(f"   âœ“ Interview Status: {interview.status}\n")

    question = Question(
        text="What is dependency injection?",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Software Architecture"]
    )
    print(f"   âœ“ Created Question: {question.text[:40]}...")
    print(f"   âœ“ Question Difficulty: {question.difficulty}\n")

    # Test 3: Interview Flow
    print("3. Testing Interview Flow...")
    interview.mark_ready(candidate.id)
    print(f"   âœ“ Interview marked ready")

    interview.start()
    print(f"   âœ“ Interview started: {interview.status}")

    interview.add_question(question.id)
    print(f"   âœ“ Added question to interview")
    print(f"   âœ“ Progress: {interview.get_progress_percentage()}%\n")

    print("=== All Tests Passed! ===")
    print("\nYou can now start the server:")
    print("  python src/main.py")
    print("\nThen visit:")
    print("  http://localhost:8000")
    print("  http://localhost:8000/docs")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="tests/__init__.py">
"""Tests package."""
</file>

<file path="tests/unit/adapters/test_mock_analytics.py">
"""Unit tests for MockAnalyticsAdapter."""

import pytest
from uuid import uuid4

from src.adapters.mock.mock_analytics import MockAnalyticsAdapter
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.question import Question, QuestionType, DifficultyLevel


@pytest.fixture
def analytics():
    """Create analytics adapter instance."""
    return MockAnalyticsAdapter()


@pytest.fixture
def sample_question():
    """Create sample question."""
    return Question(
        id=uuid4(),
        text="What is Python?",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Python", "Programming"],
        reference_answer="Python is a high-level programming language...",
        evaluation_criteria="Understanding Python fundamentals",
    )


def _create_answer(question_id):
    """Helper to create sample answer."""
    evaluation = AnswerEvaluation(
        score=85.0,
        semantic_similarity=0.9,
        completeness=0.85,
        relevance=0.95,
        sentiment="confident",
        reasoning="Good answer with examples",
        strengths=["Clear explanation", "Good examples"],
        weaknesses=["Could add more depth"],
        improvement_suggestions=["Add advanced use cases"],
    )

    return Answer(
        id=uuid4(),
        question_id=question_id,
        candidate_id=uuid4(),
        interview_id=uuid4(),
        text="Python is a versatile programming language known for readability",
        evaluation=evaluation,
    )



@pytest.fixture
def sample_answer(sample_question):
    """Create sample answer with evaluation."""
    return _create_answer(sample_question.id)


class TestRecordAnswerEvaluation:
    """Test record_answer_evaluation method."""

    @pytest.mark.asyncio
    async def test_record_single_answer(self, analytics, sample_answer):
        """Test recording single answer."""
        interview_id = uuid4()

        await analytics.record_answer_evaluation(interview_id, sample_answer)

        # Verify it was stored
        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["answers_count"] == 1

    @pytest.mark.asyncio
    async def test_record_multiple_answers(self, analytics, sample_answer):
        """Test recording multiple answers."""
        interview_id = uuid4()

        # Record 3 answers
        for _ in range(3):
            await analytics.record_answer_evaluation(interview_id, sample_answer)

        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["answers_count"] == 3


class TestGetInterviewStatistics:
    """Test get_interview_statistics method."""

    @pytest.mark.asyncio
    async def test_empty_interview(self, analytics):
        """Test statistics for interview with no answers."""
        interview_id = uuid4()

        stats = await analytics.get_interview_statistics(interview_id)

        assert stats["interview_id"] == str(interview_id)
        assert stats["question_count"] == 0
        assert stats["answers_count"] == 0
        assert stats["avg_score"] == 0.0
        assert stats["completion_rate"] == 0.0

    @pytest.mark.asyncio
    async def test_statistics_calculation(self, analytics, sample_question):
        """Test statistics are calculated correctly."""
        interview_id = uuid4()

        # Create answers with different scores
        scores = [80.0, 90.0, 70.0]
        for score in scores:
            evaluation = AnswerEvaluation(
                score=score,
                semantic_similarity=0.8,
                completeness=0.8,
                relevance=0.8,
                sentiment="positive",
                reasoning="Test",
                strengths=["Test"],
                weaknesses=[],
                improvement_suggestions=[],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                candidate_id=uuid4(),
                interview_id=interview_id,
                text="Test answer",
                evaluation=evaluation,
            )
            await analytics.record_answer_evaluation(interview_id, answer)

        stats = await analytics.get_interview_statistics(interview_id)

        assert stats["question_count"] == 3
        assert stats["answers_count"] == 3
        assert stats["avg_score"] == 80.0  # (80+90+70)/3
        assert stats["completion_rate"] == 100.0
        assert stats["highest_score"] == 90.0
        assert stats["lowest_score"] == 70.0
        assert stats["time_spent_minutes"] > 0

    @pytest.mark.asyncio
    async def test_completion_rate(self, analytics, sample_question):
        """Test completion rate calculation."""
        interview_id = uuid4()

        # Create 2 answers with text and 1 without
        for i in range(3):
            evaluation = AnswerEvaluation(
                score=80.0,
                semantic_similarity=0.8,
                completeness=0.8,
                relevance=0.8,
                sentiment="positive",
                reasoning="Test",
                strengths=[],
                weaknesses=[],
                improvement_suggestions=[],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                interview_id=interview_id,
                candidate_id=uuid4(),
                text="Answer" if i < 2 else "",  # 2 with text, 1 empty
                evaluation=evaluation,
            )
            await analytics.record_answer_evaluation(interview_id, answer)

        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["completion_rate"] == pytest.approx(66.67, rel=0.1)


class TestGetCandidatePerformanceHistory:
    """Test get_candidate_performance_history method."""

    @pytest.mark.asyncio
    async def test_first_time_candidate(self, analytics):
        """Test history for first-time candidate."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        assert isinstance(history, list)
        assert len(history) >= 0  # Mock generates 0-3 past interviews

    @pytest.mark.asyncio
    async def test_history_structure(self, analytics):
        """Test history entries have correct structure."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        if history:
            entry = history[0]
            assert "interview_date" in entry
            assert "avg_score" in entry
            assert "questions_answered" in entry
            assert "completion_rate" in entry
            assert "strong_skills" in entry
            assert "weak_skills" in entry

    @pytest.mark.asyncio
    async def test_history_shows_improvement(self, analytics):
        """Test that mock history shows score improvement."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        if len(history) >= 2:
            # Scores should improve over time
            assert history[1]["avg_score"] >= history[0]["avg_score"]


class TestGenerateImprovementRecommendations:
    """Test generate_improvement_recommendations method."""

    @pytest.mark.asyncio
    async def test_no_answers(self, analytics):
        """Test recommendations with no answers."""
        interview_id = uuid4()
        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, [], []
        )

        assert isinstance(recommendations, list)
        assert len(recommendations) > 0
        assert any("Complete the interview" in r for r in recommendations)

    @pytest.mark.asyncio
    async def test_low_score_recommendations(self, analytics, sample_question):
        """Test recommendations for low scores (<60)."""
        interview_id = uuid4()

        # Create low-scoring answers
        questions = [sample_question, sample_question, sample_question]  # 3 questions for 3 answers
        answers = []
        for _ in range(3):
            evaluation = AnswerEvaluation(
                score=50.0,
                semantic_similarity=0.5,
                completeness=0.5,
                relevance=0.6,
                sentiment="uncertain",
                reasoning="Lacks depth",
                strengths=[],
                weaknesses=["Lacks depth", "Missing examples"],
                improvement_suggestions=["Study fundamentals"],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                interview_id=interview_id,
                candidate_id=uuid4(),
                text="Brief answer",
                evaluation=evaluation,
            )
            answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) >= 3
        assert len(recommendations) <= 5
        assert any("fundamental" in r.lower() for r in recommendations)

    @pytest.mark.asyncio
    async def test_mid_score_recommendations(self, analytics, sample_question):
        """Test recommendations for mid-range scores (60-80)."""
        interview_id = uuid4()

        questions = [sample_question]
        answers = []
        evaluation = AnswerEvaluation(
            score=70.0,
            semantic_similarity=0.7,
            completeness=0.7,
            relevance=0.75,
            sentiment="positive",
            reasoning="Good but could improve",
            strengths=["Clear"],
            weaknesses=["Could add more detail"],
            improvement_suggestions=["Add examples"],
        )
        answer = Answer(
            id=uuid4(),
            question_id=sample_question.id,
            interview_id=interview_id,
            candidate_id=uuid4(),
            text="Decent answer",
            evaluation=evaluation,
        )
        answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) >= 2
        assert len(recommendations) <= 5

    @pytest.mark.asyncio
    async def test_high_score_recommendations(self, analytics, sample_question):
        """Test recommendations for high scores (>80)."""
        interview_id = uuid4()

        questions = [sample_question]
        answers = []
        evaluation = AnswerEvaluation(
            score=90.0,
            semantic_similarity=0.95,
            completeness=0.9,
            relevance=0.95,
            sentiment="confident",
            reasoning="Excellent answer",
            strengths=["Comprehensive", "Clear", "Good examples"],
            weaknesses=["Minor formatting"],
            improvement_suggestions=["Consider edge cases"],
        )
        answer = Answer(
            id=uuid4(),
            question_id=sample_question.id,
            interview_id=interview_id,
            candidate_id=uuid4(),
            text="Excellent detailed answer",
            evaluation=evaluation,
        )
        answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) > 0
        assert len(recommendations) <= 5
        assert any("excellent" in r.lower() or "continue" in r.lower() for r in recommendations)


class TestCalculateSkillScores:
    """Test calculate_skill_scores method."""

    @pytest.mark.asyncio
    async def test_empty_data(self, analytics):
        """Test with no answers."""
        scores = await analytics.calculate_skill_scores([], [])
        assert scores == {}

    @pytest.mark.asyncio
    async def test_single_skill(self, analytics):
        """Test scoring for single skill."""
        question = Question(
            id=uuid4(),
            text="What is Python?",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python"],
        )

        evaluation = AnswerEvaluation(
            score=85.0,
            semantic_similarity=0.85,
            completeness=0.85,
            relevance=0.9,
            sentiment="confident",
            reasoning="Good",
            strengths=[],
            weaknesses=[],
            improvement_suggestions=[],
        )
        answer = Answer(
            id=uuid4(),
            question_id=question.id,
            interview_id=uuid4(),
            candidate_id=uuid4(),
            text="Answer",
            evaluation=evaluation,
        )

        scores = await analytics.calculate_skill_scores([answer], [question])

        assert "Python" in scores
        assert scores["Python"] == 85.0

    @pytest.mark.asyncio
    async def test_multiple_skills(self, analytics):
        """Test scoring across multiple skills."""
        questions = [
            Question(
                id=uuid4(),
                text="Q1",
                question_type=QuestionType.TECHNICAL,
                difficulty=DifficultyLevel.MEDIUM,
                skills=["Python", "FastAPI"],
            ),
            Question(
                id=uuid4(),
                text="Q2",
                question_type=QuestionType.TECHNICAL,
                difficulty=DifficultyLevel.MEDIUM,
                skills=["Python"],
            ),
        ]

        answers = [
            Answer(
                id=uuid4(),
                question_id=questions[0].id,
                interview_id=uuid4(),
                candidate_id=uuid4(),
                text="A1",
                evaluation=AnswerEvaluation(
                    score=80.0,
                    semantic_similarity=0.8,
                    completeness=0.8,
                    relevance=0.8,
                    sentiment="positive",
                    reasoning="",
                    strengths=[],
                    weaknesses=[],
                    improvement_suggestions=[],
                ),
            ),
            Answer(
                id=uuid4(),
                question_id=questions[1].id,
                interview_id=uuid4(),
                candidate_id=uuid4(),
                text="A2",
                evaluation=AnswerEvaluation(
                    score=90.0,
                    semantic_similarity=0.9,
                    completeness=0.9,
                    relevance=0.9,
                    sentiment="confident",
                    reasoning="",
                    strengths=[],
                    weaknesses=[],
                    improvement_suggestions=[],
                ),
            ),
        ]

        scores = await analytics.calculate_skill_scores(answers, questions)

        assert "Python" in scores
        assert "FastAPI" in scores
        assert scores["Python"] == 85.0  # Average of 80 and 90
        assert scores["FastAPI"] == 80.0

    @pytest.mark.asyncio
    async def test_mismatched_lengths(self, analytics, sample_question):
        """Test with mismatched answer/question counts."""
        answer = _create_answer(sample_question.id)
        scores = await analytics.calculate_skill_scores([answer], [])
        assert scores == {}
</file>

<file path="tests/unit/adapters/test_mock_cv_analyzer.py">
"""Unit tests for MockCVAnalyzerAdapter."""

import pytest
from uuid import UUID, uuid4

from src.adapters.mock.mock_cv_analyzer import MockCVAnalyzerAdapter
from src.domain.models.cv_analysis import CVAnalysis


@pytest.fixture
def cv_analyzer():
    """Create CV analyzer instance."""
    return MockCVAnalyzerAdapter()


class TestExtractTextFromFile:
    """Test extract_text_from_file method."""

    @pytest.mark.asyncio
    async def test_extract_text_pdf(self, cv_analyzer):
        """Test extracting text from PDF file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.pdf")

        assert isinstance(text, str)
        assert len(text) > 200
        assert "John Doe" in text
        assert "Software Engineer" in text

    @pytest.mark.asyncio
    async def test_extract_text_doc(self, cv_analyzer):
        """Test extracting text from DOC file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.doc")

        assert isinstance(text, str)
        assert len(text) > 0

    @pytest.mark.asyncio
    async def test_extract_text_docx(self, cv_analyzer):
        """Test extracting text from DOCX file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.docx")

        assert isinstance(text, str)
        assert len(text) > 0

    @pytest.mark.asyncio
    async def test_unsupported_format(self, cv_analyzer):
        """Test error for unsupported file format."""
        with pytest.raises(ValueError, match="Unsupported file format"):
            await cv_analyzer.extract_text_from_file("test_cv.txt")

    @pytest.mark.asyncio
    async def test_unsupported_format_xlsx(self, cv_analyzer):
        """Test error for XLSX file."""
        with pytest.raises(ValueError, match="Unsupported file format"):
            await cv_analyzer.extract_text_from_file("test_cv.xlsx")


class TestAnalyzeCV:
    """Test analyze_cv method."""

    @pytest.mark.asyncio
    async def test_analyze_junior_cv(self, cv_analyzer):
        """Test analyzing junior-level CV."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="junior_developer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert str(cv_analysis.candidate_id) == candidate_id
        assert len(cv_analysis.skills) >= 2
        assert len(cv_analysis.skills) <= 3
        assert cv_analysis.work_experience_years is not None
        assert 1.0 <= cv_analysis.work_experience_years <= 2.0
        assert cv_analysis.suggested_difficulty == "easy"
        assert cv_analysis.education_level == "Bachelor's"

    @pytest.mark.asyncio
    async def test_analyze_senior_cv(self, cv_analyzer):
        """Test analyzing senior-level CV."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="senior_engineer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert len(cv_analysis.skills) >= 5
        assert len(cv_analysis.skills) <= 6
        assert cv_analysis.work_experience_years is not None
        assert 6.0 <= cv_analysis.work_experience_years <= 10.0
        assert cv_analysis.suggested_difficulty == "hard"
        assert cv_analysis.education_level == "Master's"

    @pytest.mark.asyncio
    async def test_analyze_mid_level_cv(self, cv_analyzer):
        """Test analyzing mid-level CV (default)."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="developer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert len(cv_analysis.skills) >= 4
        assert len(cv_analysis.skills) <= 5
        assert cv_analysis.work_experience_years is not None
        assert 3.0 <= cv_analysis.work_experience_years <= 5.0
        assert cv_analysis.suggested_difficulty == "medium"
        assert cv_analysis.education_level == "Bachelor's"

    @pytest.mark.asyncio
    async def test_cv_analysis_structure(self, cv_analyzer):
        """Test CV analysis has all required fields."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="test.pdf",
            candidate_id=candidate_id
        )

        assert cv_analysis.id is not None
        assert cv_analysis.cv_file_path == "test.pdf"
        assert cv_analysis.extracted_text is not None
        assert len(cv_analysis.extracted_text) > 0
        assert isinstance(cv_analysis.skills, list)
        assert len(cv_analysis.skills) > 0
        assert isinstance(cv_analysis.suggested_topics, list)
        assert len(cv_analysis.suggested_topics) > 0
        assert cv_analysis.summary is not None
        assert "Mock CV analysis" in cv_analysis.summary

    @pytest.mark.asyncio
    async def test_skills_are_technical(self, cv_analyzer):
        """Test that extracted skills include technical skills."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="senior_engineer.pdf",
            candidate_id=candidate_id
        )

        technical_skills = cv_analysis.get_technical_skills()
        assert len(technical_skills) > 0

        # Check skill structure
        for skill in cv_analysis.skills:
            assert skill.name is not None
            assert skill.category in ["technical", "soft"]

    @pytest.mark.asyncio
    async def test_suggested_topics_from_skills(self, cv_analyzer):
        """Test that suggested topics are derived from skills."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="python_developer.pdf",
            candidate_id=candidate_id
        )

        # Topics should be related to skills
        assert len(cv_analysis.suggested_topics) > 0
        assert len(cv_analysis.suggested_topics) <= 5

    @pytest.mark.asyncio
    async def test_metadata_included(self, cv_analyzer):
        """Test that metadata is included in analysis."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="junior_dev.pdf",
            candidate_id=candidate_id
        )

        assert "experience_level" in cv_analysis.metadata
        assert "file_name" in cv_analysis.metadata
        assert "mock_adapter" in cv_analysis.metadata
        assert cv_analysis.metadata["mock_adapter"] is True
        assert cv_analysis.metadata["experience_level"] == "junior"

    @pytest.mark.asyncio
    async def test_consistent_results(self, cv_analyzer):
        """Test that same filename produces consistent experience level."""
        candidate_id = str(uuid4())

        # Call twice with same filename
        result1 = await cv_analyzer.analyze_cv("junior_dev.pdf", candidate_id)
        result2 = await cv_analyzer.analyze_cv("junior_dev.pdf", candidate_id)

        assert result1.suggested_difficulty == result2.suggested_difficulty
        assert len(result1.skills) == len(result2.skills)
        assert result1.education_level == result2.education_level
</file>

<file path="tests/unit/domain/test_adaptive_models.py">
"""Tests for Phase 01: Domain model updates for adaptive interviews."""

from datetime import datetime
from uuid import uuid4

import pytest

from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


class TestQuestionAdaptiveFields:
    """Test Question model adaptive fields (ideal_answer, rationale)."""

    def test_question_with_ideal_answer(self):
        """Test question with ideal answer and rationale."""
        question = Question(
            text="Explain recursion",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python"],
            ideal_answer="Recursion is a function calling itself with a base case...",
            rationale="This answer demonstrates mastery by covering base case and examples",
        )

        assert question.has_ideal_answer() is True
        assert question.is_planned is True
        assert question.ideal_answer is not None
        assert question.rationale is not None

    def test_question_without_ideal_answer(self):
        """Test legacy question without ideal answer."""
        question = Question(
            text="Tell me about yourself",
            question_type=QuestionType.BEHAVIORAL,
            difficulty=DifficultyLevel.EASY,
            skills=["Communication"],
        )

        assert question.has_ideal_answer() is False
        assert question.is_planned is False
        assert question.ideal_answer is None
        assert question.rationale is None

    def test_has_ideal_answer_empty_string(self):
        """Test has_ideal_answer with empty or short string."""
        question = Question(
            text="Test",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.EASY,
            skills=["Test"],
            ideal_answer="   ",  # Only whitespace
        )

        assert question.has_ideal_answer() is False

        question.ideal_answer = "Short"  # < 10 chars
        assert question.has_ideal_answer() is False


class TestInterviewAdaptiveFields:
    """Test Interview model adaptive fields (plan_metadata, adaptive_follow_ups)."""

    def test_interview_with_planning_metadata(self):
        """Test interview with plan_metadata."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.READY,
        )
        interview.plan_metadata = {
            "n": 4,
            "generated_at": datetime.utcnow().isoformat(),
            "strategy": "adaptive_planning_v1",
        }

        assert interview.planned_question_count == 4
        assert "strategy" in interview.plan_metadata

    def test_interview_without_planning_metadata(self):
        """Test legacy interview without plan_metadata."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.IN_PROGRESS,
        )

        assert interview.plan_metadata == {}
        assert interview.planned_question_count == 0

    def test_add_adaptive_followup(self):
        """Test adding adaptive follow-up questions."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.IN_PROGRESS,
        )

        follow_up_id = uuid4()
        interview.add_adaptive_followup(follow_up_id)

        assert len(interview.adaptive_follow_ups) == 1
        assert follow_up_id in interview.adaptive_follow_ups

    def test_mark_ready_with_cv_analysis(self):
        """Test marking interview as READY after planning."""
        cv_analysis_id = uuid4()
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.PREPARING,
        )

        interview.mark_ready(cv_analysis_id)

        assert interview.status == InterviewStatus.READY
        assert interview.cv_analysis_id == cv_analysis_id


class TestAnswerAdaptiveFields:
    """Test Answer model adaptive fields (similarity_score, gaps)."""

    def test_answer_with_similarity_score(self):
        """Test answer with similarity score."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Recursion calls itself with base case",
            is_voice=False,
            similarity_score=0.85,
        )

        assert answer.has_similarity_score() is True
        assert answer.similarity_score == 0.85
        assert answer.meets_threshold(0.8) is True
        assert answer.meets_threshold(0.9) is False

    def test_answer_without_similarity_score(self):
        """Test legacy answer without similarity score."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Some answer",
            is_voice=False,
        )

        assert answer.has_similarity_score() is False
        assert answer.similarity_score is None
        assert answer.meets_threshold(0.8) is False

    def test_answer_with_gaps(self):
        """Test answer with detected gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Brief answer",
            is_voice=False,
            gaps={
                "concepts": ["base case", "recursive case"],
                "keywords": ["base", "recursive"],
                "confirmed": True,
                "severity": "moderate",
            },
        )

        assert answer.has_gaps() is True
        assert len(answer.gaps["concepts"]) == 2  # type: ignore
        assert answer.gaps["confirmed"] is True  # type: ignore

    def test_answer_without_gaps(self):
        """Test answer without gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Complete answer",
            is_voice=False,
            gaps={"concepts": [], "confirmed": False},
        )

        assert answer.has_gaps() is False

    def test_is_adaptive_complete_high_similarity(self):
        """Test adaptive completion with high similarity (>= 80%)."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Good answer",
            is_voice=False,
            similarity_score=0.85,
            gaps={"concepts": ["minor"], "confirmed": True},
        )

        assert answer.is_adaptive_complete() is True

    def test_is_adaptive_complete_no_gaps(self):
        """Test adaptive completion with no confirmed gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Good answer",
            is_voice=False,
            similarity_score=0.75,
            gaps={"concepts": [], "confirmed": False},
        )

        assert answer.is_adaptive_complete() is True

    def test_is_adaptive_incomplete(self):
        """Test answer that needs follow-up."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Brief answer",
            is_voice=False,
            similarity_score=0.60,
            gaps={"concepts": ["base case"], "confirmed": True},
        )

        assert answer.is_adaptive_complete() is False


class TestFollowUpQuestion:
    """Test FollowUpQuestion model."""

    def test_follow_up_question_creation(self):
        """Test creating a follow-up question."""
        parent_id = uuid4()
        interview_id = uuid4()

        follow_up = FollowUpQuestion(
            parent_question_id=parent_id,
            interview_id=interview_id,
            text="Can you explain the base case in recursion?",
            generated_reason="Missing concepts: base case",
            order_in_sequence=1,
        )

        assert follow_up.parent_question_id == parent_id
        assert follow_up.interview_id == interview_id
        assert follow_up.order_in_sequence == 1
        assert follow_up.is_last_allowed() is False

    def test_is_last_allowed(self):
        """Test detection of 3rd follow-up (max allowed)."""
        follow_up_third = FollowUpQuestion(
            parent_question_id=uuid4(),
            interview_id=uuid4(),
            text="Third follow-up",
            generated_reason="Still missing concepts",
            order_in_sequence=3,
        )

        assert follow_up_third.is_last_allowed() is True

    def test_follow_up_has_created_at(self):
        """Test follow-up question has timestamp."""
        follow_up = FollowUpQuestion(
            parent_question_id=uuid4(),
            interview_id=uuid4(),
            text="Follow-up",
            generated_reason="Gaps detected",
            order_in_sequence=1,
        )

        assert follow_up.created_at is not None
        assert isinstance(follow_up.created_at, datetime)


class TestSimilarityScoreValidation:
    """Test similarity_score field validation (0-1 range)."""

    def test_valid_similarity_scores(self):
        """Test valid similarity scores (0.0 to 1.0)."""
        for score in [0.0, 0.5, 0.8, 1.0]:
            answer = Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=score,
            )
            assert answer.similarity_score == score

    def test_invalid_similarity_score_too_high(self):
        """Test invalid similarity score > 1.0."""
        with pytest.raises(Exception):  # Pydantic validation error
            Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=1.5,
            )

    def test_invalid_similarity_score_negative(self):
        """Test invalid negative similarity score."""
        with pytest.raises(Exception):  # Pydantic validation error
            Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=-0.1,
            )
</file>

<file path="alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the tzdata library which can be installed by adding
# `alembic[tz]` to the pip requirements.
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# use sequential revision IDs (0001, 0002, etc.) instead of hash-based IDs
# set to 'true' to enable automatic sequential numbering for new migrations
use_sequential_revisions = true

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
# Database URL is loaded from environment variables in env.py
# sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="pyproject.toml">
[project]
name = "elios-ai-service"
version = "0.1.0"
description = "AI-powered mock interview platform with CV analysis and real-time feedback"
readme = "README.md"
requires-python = ">=3.11"
license = {text = "MIT"}
authors = [
    {name = "Elios Team"},
]
keywords = ["ai", "interview", "nlp", "fastapi"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    # Core Framework
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.1.0",

    # LLM Providers
    "openai>=1.3.0",
    "anthropic>=0.7.0",

    # Vector Databases
    "pinecone-client>=3.0.0",

    # Database
    "sqlalchemy[asyncio]>=2.0.0",
    "asyncpg>=0.29.0",
    "alembic>=1.13.0",

    # NLP & Document Processing
    "spacy>=3.7.0",
    "langchain>=0.1.0",
    "PyPDF2>=3.0.0",
    "python-docx>=1.1.0",

    # Utilities
    "python-multipart>=0.0.6",
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
    "httpx>=0.25.0",
]

[project.optional-dependencies]
dev = [
    # Testing
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",

    # Code Quality
    "ruff>=0.1.6",
    "black>=23.11.0",
    "mypy>=1.7.0",

    # Development Tools
    "ipython>=8.18.0",
    "python-dotenv>=1.0.0",
]

[project.urls]
Homepage = "https://github.com/elios/elios-ai-service"
Repository = "https://github.com/elios/elios-ai-service"
Documentation = "https://github.com/elios/elios-ai-service#readme"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
addopts = "-v --cov=src --cov-report=term-missing --cov-report=html"
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"

[tool.coverage.run]
source = ["src"]
omit = ["tests/*", "**/__pycache__/*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "@abstractmethod",
]

[tool.ruff]
line-length = 100
target-version = "py311"
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
    "UP",  # pyupgrade
]
ignore = [
    "E501",  # line too long (handled by black)
    "B008",  # do not perform function calls in argument defaults
]
exclude = [
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    "build",
    "dist",
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]  # Unused imports in __init__.py

[tool.black]
line-length = 100
target-version = ["py311"]
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.venv
  | venv
  | __pycache__
  | build
  | dist
)/
'''

[tool.mypy]
 python_version = "3.12"
  # Enable stricter checks
  disallow_untyped_defs = true
  disallow_incomplete_defs = true
  strict = true
  warn_return_any = true
  warn_unused_configs = true
  check_untyped_defs = true
  no_implicit_optional = true
  warn_redundant_casts = true
  warn_unused_ignores = true
  warn_no_return = true
  strict_equality = true
  ignore_missing_imports = false

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
</file>

<file path="src/adapters/api/rest/health_routes.py">
"""Health check routes."""

from datetime import datetime

from fastapi import APIRouter
from pydantic import BaseModel

from ....infrastructure.config import get_settings

router = APIRouter()


class HealthResponse(BaseModel):
    """Health check response model."""
    status: str
    version: str
    environment: str
    timestamp: datetime


@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse:
    """Health check endpoint.

    Returns:
        Health status information
    """
    settings = get_settings()

    return HealthResponse(
        status="healthy",
        version=settings.app_version,
        environment=settings.environment,
        timestamp=datetime.utcnow(),
    )


@router.get("/")
async def root():
    """Root endpoint.

    Returns:
        Welcome message
    """
    settings = get_settings()

    return {
        "message": f"Welcome to {settings.app_name}",
        "version": settings.app_version,
        "docs": "/docs",
        "health": "/health",
    }
</file>

<file path="src/adapters/mock/__init__.py">
"""Mock adapters for development and testing."""

from .mock_analytics import MockAnalyticsAdapter
from .mock_cv_analyzer import MockCVAnalyzerAdapter
from .mock_llm_adapter import MockLLMAdapter
from .mock_stt_adapter import MockSTTAdapter
from .mock_tts_adapter import MockTTSAdapter
from .mock_vector_search_adapter import MockVectorSearchAdapter

__all__ = [
    "MockAnalyticsAdapter",
    "MockCVAnalyzerAdapter",
    "MockLLMAdapter",
    "MockSTTAdapter",
    "MockTTSAdapter",
    "MockVectorSearchAdapter",
]
</file>

<file path="src/adapters/persistence/answer_repository.py">
"""PostgreSQL implementation of AnswerRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.answer import Answer
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from .mappers import AnswerMapper
from .models import AnswerModel


class PostgreSQLAnswerRepository(AnswerRepositoryPort):
    """PostgreSQL implementation of answer repository.

    This adapter implements the AnswerRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, answer: Answer) -> Answer:
        """Save a new answer to the database."""
        db_model = AnswerMapper.to_db_model(answer)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return AnswerMapper.to_domain(db_model)

    async def get_by_id(self, answer_id: UUID) -> Answer | None:
        """Retrieve an answer by ID."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer_id)
        )
        db_model = result.scalar_one_or_none()
        return AnswerMapper.to_domain(db_model) if db_model else None

    async def get_by_ids(self, answer_ids: list[UUID]) -> list[Answer]:
        """Retrieve multiple answers by IDs."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id.in_(answer_ids))
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        """Retrieve all answers for an interview."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.interview_id == interview_id)
            .order_by(AnswerModel.created_at.asc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_question_id(self, question_id: UUID) -> list[Answer]:
        """Retrieve all answers for a question."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.question_id == question_id)
            .order_by(AnswerModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_candidate_id(self, candidate_id: UUID) -> list[Answer]:
        """Retrieve all answers by a candidate."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.candidate_id == candidate_id)
            .order_by(AnswerModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, answer: Answer) -> Answer:
        """Update an existing answer."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Answer with id {answer.id} not found")

        AnswerMapper.update_db_model(db_model, answer)
        await self.session.commit()
        await self.session.refresh(db_model)
        return AnswerMapper.to_domain(db_model)

    async def delete(self, answer_id: UUID) -> bool:
        """Delete an answer by ID."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True
</file>

<file path="src/adapters/persistence/candidate_repository.py">
"""PostgreSQL implementation of CandidateRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.candidate import Candidate
from ...domain.ports.candidate_repository_port import CandidateRepositoryPort
from .mappers import CandidateMapper
from .models import CandidateModel


class PostgreSQLCandidateRepository(CandidateRepositoryPort):
    """PostgreSQL implementation of candidate repository.

    This adapter implements the CandidateRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, candidate: Candidate) -> Candidate:
        """Save a new candidate to the database."""
        db_model = CandidateMapper.to_db_model(candidate)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CandidateMapper.to_domain(db_model)

    async def get_by_id(self, candidate_id: UUID) -> Candidate | None:
        """Retrieve a candidate by ID."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate_id)
        )
        db_model = result.scalar_one_or_none()
        return CandidateMapper.to_domain(db_model) if db_model else None

    async def get_by_email(self, email: str) -> Candidate | None:
        """Retrieve a candidate by email address."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.email == email)
        )
        db_model = result.scalar_one_or_none()
        return CandidateMapper.to_domain(db_model) if db_model else None

    async def update(self, candidate: Candidate) -> Candidate:
        """Update an existing candidate."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Candidate with id {candidate.id} not found")

        CandidateMapper.update_db_model(db_model, candidate)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CandidateMapper.to_domain(db_model)

    async def delete(self, candidate_id: UUID) -> bool:
        """Delete a candidate by ID."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Candidate]:
        """List all candidates with pagination."""
        result = await self.session.execute(
            select(CandidateModel)
            .order_by(CandidateModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [CandidateMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/persistence/cv_analysis_repository.py">
"""PostgreSQL implementation of CVAnalysisRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.ports.cv_analysis_repository_port import CVAnalysisRepositoryPort
from .mappers import CVAnalysisMapper
from .models import CVAnalysisModel


class PostgreSQLCVAnalysisRepository(CVAnalysisRepositoryPort):
    """PostgreSQL implementation of CV analysis repository.

    This adapter implements the CVAnalysisRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Save a new CV analysis to the database."""
        db_model = CVAnalysisMapper.to_db_model(cv_analysis)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CVAnalysisMapper.to_domain(db_model)

    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        """Retrieve a CV analysis by ID."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis_id)
        )
        db_model = result.scalar_one_or_none()
        return CVAnalysisMapper.to_domain(db_model) if db_model else None

    async def get_by_candidate_id(self, candidate_id: UUID) -> list[CVAnalysis]:
        """Retrieve all CV analyses for a candidate."""
        result = await self.session.execute(
            select(CVAnalysisModel)
            .where(CVAnalysisModel.candidate_id == candidate_id)
            .order_by(CVAnalysisModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [CVAnalysisMapper.to_domain(db_model) for db_model in db_models]

    async def get_latest_by_candidate_id(
        self,
        candidate_id: UUID,
    ) -> CVAnalysis | None:
        """Retrieve the most recent CV analysis for a candidate."""
        result = await self.session.execute(
            select(CVAnalysisModel)
            .where(CVAnalysisModel.candidate_id == candidate_id)
            .order_by(CVAnalysisModel.created_at.desc())
            .limit(1)
        )
        db_model = result.scalar_one_or_none()
        return CVAnalysisMapper.to_domain(db_model) if db_model else None

    async def update(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Update an existing CV analysis."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"CV Analysis with id {cv_analysis.id} not found")

        CVAnalysisMapper.update_db_model(db_model, cv_analysis)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CVAnalysisMapper.to_domain(db_model)

    async def delete(self, cv_analysis_id: UUID) -> bool:
        """Delete a CV analysis by ID."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True
</file>

<file path="src/adapters/persistence/interview_repository.py">
"""PostgreSQL implementation of InterviewRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.interview import Interview, InterviewStatus
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from .mappers import InterviewMapper
from .models import InterviewModel


class PostgreSQLInterviewRepository(InterviewRepositoryPort):
    """PostgreSQL implementation of interview repository.

    This adapter implements the InterviewRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, interview: Interview) -> Interview:
        """Save a new interview to the database."""
        db_model = InterviewMapper.to_db_model(interview)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return InterviewMapper.to_domain(db_model)

    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        """Retrieve an interview by ID."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview_id)
        )
        db_model = result.scalar_one_or_none()
        return InterviewMapper.to_domain(db_model) if db_model else None

    async def get_by_candidate_id(
        self,
        candidate_id: UUID,
        status: InterviewStatus | None = None,
    ) -> list[Interview]:
        """Retrieve interviews for a candidate with optional status filter."""
        query = select(InterviewModel).where(InterviewModel.candidate_id == candidate_id)

        if status:
            query = query.where(InterviewModel.status == status.value)

        query = query.order_by(InterviewModel.created_at.desc())

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_status(
        self,
        status: InterviewStatus,
        limit: int = 100,
    ) -> list[Interview]:
        """Retrieve interviews by status."""
        result = await self.session.execute(
            select(InterviewModel)
            .where(InterviewModel.status == status.value)
            .order_by(InterviewModel.created_at.desc())
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, interview: Interview) -> Interview:
        """Update an existing interview."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Interview with id {interview.id} not found")

        InterviewMapper.update_db_model(db_model, interview)
        await self.session.commit()
        await self.session.refresh(db_model)
        return InterviewMapper.to_domain(db_model)

    async def delete(self, interview_id: UUID) -> bool:
        """Delete an interview by ID."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Interview]:
        """List all interviews with pagination."""
        result = await self.session.execute(
            select(InterviewModel)
            .order_by(InterviewModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/persistence/question_repository.py">
"""PostgreSQL implementation of QuestionRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.question import DifficultyLevel, Question, QuestionType
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from .mappers import QuestionMapper
from .models import QuestionModel


class PostgreSQLQuestionRepository(QuestionRepositoryPort):
    """PostgreSQL implementation of question repository.

    This adapter implements the QuestionRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, question: Question) -> Question:
        """Save a new question to the database."""
        db_model = QuestionMapper.to_db_model(question)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return QuestionMapper.to_domain(db_model)

    async def get_by_id(self, question_id: UUID) -> Question | None:
        """Retrieve a question by ID."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()
        return QuestionMapper.to_domain(db_model) if db_model else None

    async def get_by_ids(self, question_ids: list[UUID]) -> list[Question]:
        """Retrieve multiple questions by IDs."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id.in_(question_ids))
        )
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_skill(
        self,
        skill: str,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by skill with optional difficulty filter."""
        query = select(QuestionModel).where(
            QuestionModel.skills.contains([skill])  # PostgreSQL array contains
        )

        if difficulty:
            query = query.where(QuestionModel.difficulty == difficulty.value)

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_type(
        self,
        question_type: QuestionType,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by type with optional difficulty filter."""
        query = select(QuestionModel).where(
            QuestionModel.question_type == question_type.value
        )

        if difficulty:
            query = query.where(QuestionModel.difficulty == difficulty.value)

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_tags(
        self,
        tags: list[str],
        match_all: bool = False,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by tags.

        Args:
            tags: List of tags to search for
            match_all: If True, match all tags; if False, match any tag
            limit: Maximum number of results
        """
        if match_all:
            # Match all tags (array contains all elements)
            query = select(QuestionModel).where(
                QuestionModel.tags.contains(tags)  # PostgreSQL @> operator
            )
        else:
            # Match any tag (array overlap)
            query = select(QuestionModel).where(
                QuestionModel.tags.overlap(tags)  # PostgreSQL && operator
            )

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, question: Question) -> Question:
        """Update an existing question."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Question with id {question.id} not found")

        QuestionMapper.update_db_model(db_model, question)
        await self.session.commit()
        await self.session.refresh(db_model)
        return QuestionMapper.to_domain(db_model)

    async def delete(self, question_id: UUID) -> bool:
        """Delete a question by ID."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Question]:
        """List all questions with pagination."""
        result = await self.session.execute(
            select(QuestionModel)
            .order_by(QuestionModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/vector_db/pinecone_adapter.py">
"""Pinecone vector database adapter implementation."""

from typing import Any
from uuid import UUID

from openai import AsyncOpenAI
from pinecone import Pinecone, ServerlessSpec

from ...domain.ports.vector_search_port import VectorSearchPort


class PineconeAdapter(VectorSearchPort):
    """Pinecone implementation of vector search port.

    This adapter encapsulates all Pinecone-specific logic. Switching to
    another vector database (Weaviate, ChromaDB) only requires implementing
    the VectorSearchPort interface.
    """

    def __init__(
        self,
        api_key: str,
        environment: str,
        index_name: str,
        openai_api_key: str,
        embedding_model: str = "text-embedding-3-small",
    ):
        """Initialize Pinecone adapter.

        Args:
            api_key: Pinecone API key
            environment: Pinecone environment
            index_name: Name of the Pinecone index to use
            openai_api_key: OpenAI API key for embeddings
            embedding_model: OpenAI embedding model to use
        """
        self.pc = Pinecone(api_key=api_key)
        self.index_name = index_name
        self.index = None
        self.openai_client = AsyncOpenAI(api_key=openai_api_key)
        self.embedding_model = embedding_model
        self.environment = environment

        # Initialize index if it doesn't exist
        self._ensure_index_exists()

    def _ensure_index_exists(self) -> None:
        """Ensure the Pinecone index exists."""
        existing_indexes = [idx.name for idx in self.pc.list_indexes()]

        if self.index_name not in existing_indexes:
            # Create index with 1536 dimensions (OpenAI text-embedding-3-small)
            self.pc.create_index(
                name=self.index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region=self.environment),
            )

        self.index = self.pc.Index(self.index_name)

    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a question's vector embedding in Pinecone.

        Args:
            question_id: Unique question identifier
            embedding: Vector embedding
            metadata: Additional metadata
        """
        self.index.upsert(
            vectors=[
                {
                    "id": f"question_{str(question_id)}",
                    "values": embedding,
                    "metadata": {
                        **metadata,
                        "type": "question",
                        "question_id": str(question_id),
                    },
                }
            ]
        )

    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a CV analysis vector embedding in Pinecone.

        Args:
            cv_analysis_id: Unique CV analysis identifier
            embedding: Vector embedding
            metadata: Additional metadata
        """
        self.index.upsert(
            vectors=[
                {
                    "id": f"cv_{str(cv_analysis_id)}",
                    "values": embedding,
                    "metadata": {
                        **metadata,
                        "type": "cv",
                        "cv_analysis_id": str(cv_analysis_id),
                    },
                }
            ]
        )

    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Find similar questions using semantic search.

        Args:
            query_embedding: Query vector
            top_k: Number of results to return
            filters: Optional filters

        Returns:
            List of similar questions with similarity scores
        """
        # Build Pinecone filter
        pinecone_filter = {"type": "question"}
        if filters:
            pinecone_filter.update(filters)

        # Query Pinecone
        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            filter=pinecone_filter,
            include_metadata=True,
        )

        # Format results
        similar_questions = []
        for match in results.matches:
            similar_questions.append(
                {
                    "question_id": match.metadata.get("question_id"),
                    "score": match.score,
                    "metadata": match.metadata,
                }
            )

        return similar_questions

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Calculate similarity between answer and reference answers.

        Args:
            answer_embedding: Candidate's answer embedding
            reference_embeddings: Reference answer embeddings

        Returns:
            Similarity score (0-1)
        """
        # For simplicity, calculate cosine similarity with the first reference
        # In production, you might want to average or take max similarity
        if not reference_embeddings:
            return 0.0

        # Store temporary reference embedding
        temp_id = "temp_reference"
        self.index.upsert(
            vectors=[
                {
                    "id": temp_id,
                    "values": reference_embeddings[0],
                    "metadata": {"type": "temp"},
                }
            ]
        )

        # Query for similarity
        results = self.index.query(
            vector=answer_embedding,
            top_k=1,
            filter={"type": "temp"},
        )

        # Clean up temp vector
        self.index.delete(ids=[temp_id])

        return results.matches[0].score if results.matches else 0.0

    async def get_embedding(self, text: str) -> list[float]:
        """Generate embedding for text using OpenAI.

        Args:
            text: Text to embed

        Returns:
            Vector embedding
        """
        response = await self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=text,
        )

        return response.data[0].embedding

    async def delete_embeddings(self, ids: list[UUID]) -> None:
        """Delete embeddings by IDs.

        Args:
            ids: List of IDs to delete
        """
        # Delete both question and CV embeddings
        pinecone_ids = []
        for id_ in ids:
            pinecone_ids.extend([f"question_{str(id_)}", f"cv_{str(id_)}"])

        self.index.delete(ids=pinecone_ids)
</file>

<file path="src/application/use_cases/analyze_cv.py">
"""Analyze CV use case."""

from uuid import UUID

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.ports.cv_analyzer_port import CVAnalyzerPort
from ...domain.ports.vector_search_port import VectorSearchPort


class AnalyzeCVUseCase:
    """Use case for analyzing a candidate's CV.

    This orchestrates the CV analysis process:
    1. Extract text from CV file
    2. Analyze and extract structured information
    3. Generate embeddings for semantic search
    4. Store embeddings in vector database
    """

    def __init__(
        self,
        cv_analyzer: CVAnalyzerPort,
        vector_search: VectorSearchPort,
    ):
        """Initialize use case with required ports.

        Args:
            cv_analyzer: CV analysis service
            vector_search: Vector database service
        """
        self.cv_analyzer = cv_analyzer
        self.vector_search = vector_search

    async def execute(
        self,
        cv_file_path: str,
        candidate_id: UUID,
    ) -> CVAnalysis:
        """Execute CV analysis.

        Args:
            cv_file_path: Path to CV file
            candidate_id: ID of the candidate

        Returns:
            CVAnalysis with extracted information

        Raises:
            ValueError: If CV file is invalid or cannot be processed
        """
        # Step 1: Analyze CV using the CV analyzer port
        cv_analysis = await self.cv_analyzer.analyze_cv(
            cv_file_path=cv_file_path,
            candidate_id=str(candidate_id),
        )

        # Step 2: Generate embedding for the CV
        # Combine key information for embedding
        cv_text_for_embedding = f"""
        Skills: {', '.join([skill.name for skill in cv_analysis.skills])}
        Experience: {cv_analysis.work_experience_years} years
        Education: {cv_analysis.education_level}
        Summary: {cv_analysis.summary}
        """

        embedding = await self.vector_search.get_embedding(cv_text_for_embedding)
        cv_analysis.embedding = embedding

        # Step 3: Store embedding in vector database for future question matching
        await self.vector_search.store_cv_embedding(
            cv_analysis_id=cv_analysis.id,
            embedding=embedding,
            metadata={
                "candidate_id": str(candidate_id),
                "skills": [skill.name for skill in cv_analysis.skills],
                "experience_years": cv_analysis.work_experience_years,
                "education": cv_analysis.education_level,
                "suggested_difficulty": cv_analysis.suggested_difficulty,
            },
        )

        return cv_analysis
</file>

<file path="src/domain/models/candidate.py">
"""Candidate domain model."""

from datetime import datetime
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class Candidate(BaseModel):
    """Represents a candidate participating in an interview.

    This is a rich domain model that encapsulates candidate-related business logic.
    """

    id: UUID = Field(default_factory=uuid4)
    name: str
    email: str
    cv_file_path: str | None = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""
        frozen = False  # Allow updates

    def update_cv(self, cv_file_path: str) -> None:
        """Update candidate's CV file path.

        Args:
            cv_file_path: Path to the CV file
        """
        self.cv_file_path = cv_file_path
        self.updated_at = datetime.utcnow()

    def has_cv(self) -> bool:
        """Check if candidate has uploaded a CV.

        Returns:
            True if CV exists, False otherwise
        """
        return self.cv_file_path is not None
</file>

<file path="src/domain/models/cv_analysis.py">
"""CV Analysis domain model."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class ExtractedSkill(BaseModel):
    """Represents a skill extracted from CV.

    This is a value object within CV analysis.
    """

    name: str = Field(alias="skill")
    category: str = "technical"  # e.g., "technical", "soft", "language"
    proficiency_level: str | None = Field(default=None, alias="proficiency")  # e.g., "beginner", "intermediate", "expert"
    years_of_experience: float | None = Field(default=None, alias="years")
    mentioned_count: int = 1  # How many times mentioned in CV

    def is_technical(self) -> bool:
        """Check if skill is technical.

        Returns:
            True if technical skill, False otherwise
        """
        return self.category.lower() == "technical"


class CVAnalysis(BaseModel):
    """Represents the analysis results of a candidate's CV.

    This is an entity in the interview domain.
    """

    id: UUID = Field(default_factory=uuid4)
    candidate_id: UUID
    cv_file_path: str
    extracted_text: str
    skills: list[ExtractedSkill] = Field(default_factory=list)
    work_experience_years: float | None = None
    education_level: str | None = None  # e.g., "Bachelor's", "Master's"
    suggested_topics: list[str] = Field(default_factory=list)  # Topics to cover
    suggested_difficulty: str = "medium"  # Overall difficulty level
    embedding: list[float] | None = None  # Vector embedding of CV
    summary: str | None = None  # AI-generated summary
    metadata: dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""
        frozen = False

    def get_technical_skills(self) -> list[ExtractedSkill]:
        """Get only technical skills.

        Returns:
            List of technical skills
        """
        return [skill for skill in self.skills if skill.is_technical()]

    def has_skill(self, skill_name: str) -> bool:
        """Check if a specific skill was found in CV.

        Args:
            skill_name: Name of skill to check

        Returns:
            True if skill exists, False otherwise
        """
        return any(
            skill.name.lower() == skill_name.lower()
            for skill in self.skills
        )

    def get_skill_by_name(self, skill_name: str) -> ExtractedSkill | None:
        """Get a skill by name.

        Args:
            skill_name: Name of skill to find

        Returns:
            ExtractedSkill if found, None otherwise
        """
        for skill in self.skills:
            if skill.name.lower() == skill_name.lower():
                return skill
        return None

    def get_top_skills(self, limit: int = 5) -> list[ExtractedSkill]:
        """Get top skills by mention count.

        Args:
            limit: Maximum number of skills to return

        Returns:
            List of top skills
        """
        sorted_skills = sorted(
            self.skills,
            key=lambda s: s.mentioned_count,
            reverse=True
        )
        return sorted_skills[:limit]

    def is_experienced(self, min_years: float = 3.0) -> bool:
        """Check if candidate is experienced.

        Args:
            min_years: Minimum years of experience

        Returns:
            True if experienced, False otherwise
        """
        return (
            self.work_experience_years is not None
            and self.work_experience_years >= min_years
        )
</file>

<file path="src/domain/ports/analytics_port.py">
"""Analytics port interface."""

from abc import ABC, abstractmethod
from typing import Any
from uuid import UUID

from ..models.answer import Answer
from ..models.question import Question


class AnalyticsPort(ABC):
    """Interface for analytics and reporting operations.

    This port abstracts analytics storage and report generation.
    """

    @abstractmethod
    async def record_answer_evaluation(
        self,
        interview_id: UUID,
        answer: Answer,
    ) -> None:
        """Record answer evaluation for analytics.

        Args:
            interview_id: Interview identifier
            answer: Answer with evaluation data
        """
        pass

    @abstractmethod
    async def get_interview_statistics(
        self,
        interview_id: UUID,
    ) -> dict[str, Any]:
        """Get statistics for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            Dictionary with statistics (avg score, completion rate, etc.)
        """
        pass

    @abstractmethod
    async def get_candidate_performance_history(
        self,
        candidate_id: UUID,
    ) -> list[dict[str, Any]]:
        """Get candidate's performance across all interviews.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of interview performance data
        """
        pass

    @abstractmethod
    async def generate_improvement_recommendations(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[Answer],
    ) -> list[str]:
        """Generate improvement recommendations based on performance.

        Args:
            interview_id: Interview identifier
            questions: Questions asked
            answers: Answers with evaluations

        Returns:
            List of improvement recommendations
        """
        pass

    @abstractmethod
    async def calculate_skill_scores(
        self,
        answers: list[Answer],
        questions: list[Question],
    ) -> dict[str, float]:
        """Calculate scores per skill based on answers.

        Args:
            answers: List of evaluated answers
            questions: Corresponding questions

        Returns:
            Dictionary mapping skill names to scores
        """
        pass
</file>

<file path="src/domain/ports/answer_repository_port.py">
"""Answer repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.answer import Answer


class AnswerRepositoryPort(ABC):
    """Interface for answer persistence operations.

    This port abstracts database operations for answers,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, answer: Answer) -> Answer:
        """Save an answer.

        Args:
            answer: Answer to save

        Returns:
            Saved answer with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, answer_id: UUID) -> Answer | None:
        """Retrieve an answer by ID.

        Args:
            answer_id: Answer identifier

        Returns:
            Answer if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_ids(self, answer_ids: list[UUID]) -> list[Answer]:
        """Retrieve multiple answers by IDs.

        Args:
            answer_ids: List of answer identifiers

        Returns:
            List of answers found
        """
        pass

    @abstractmethod
    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        """Retrieve all answers for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def get_by_question_id(self, question_id: UUID) -> list[Answer]:
        """Retrieve all answers for a question.

        Args:
            question_id: Question identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(self, candidate_id: UUID) -> list[Answer]:
        """Retrieve all answers by a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def update(self, answer: Answer) -> Answer:
        """Update an existing answer.

        Args:
            answer: Answer with updated data

        Returns:
            Updated answer
        """
        pass

    @abstractmethod
    async def delete(self, answer_id: UUID) -> bool:
        """Delete an answer.

        Args:
            answer_id: Answer identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/ports/candidate_repository_port.py">
"""Candidate repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.candidate import Candidate


class CandidateRepositoryPort(ABC):
    """Interface for candidate persistence operations.

    This port abstracts database operations for candidates,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, candidate: Candidate) -> Candidate:
        """Save a candidate.

        Args:
            candidate: Candidate to save

        Returns:
            Saved candidate with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, candidate_id: UUID) -> Candidate | None:
        """Retrieve a candidate by ID.

        Args:
            candidate_id: Candidate identifier

        Returns:
            Candidate if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_email(self, email: str) -> Candidate | None:
        """Retrieve a candidate by email.

        Args:
            email: Candidate email address

        Returns:
            Candidate if found, None otherwise
        """
        pass

    @abstractmethod
    async def update(self, candidate: Candidate) -> Candidate:
        """Update an existing candidate.

        Args:
            candidate: Candidate with updated data

        Returns:
            Updated candidate
        """
        pass

    @abstractmethod
    async def delete(self, candidate_id: UUID) -> bool:
        """Delete a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Candidate]:
        """List all candidates with pagination.

        Args:
            skip: Number of candidates to skip
            limit: Maximum number of results

        Returns:
            List of candidates
        """
        pass
</file>

<file path="src/domain/ports/cv_analysis_repository_port.py">
"""CV Analysis repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.cv_analysis import CVAnalysis


class CVAnalysisRepositoryPort(ABC):
    """Interface for CV analysis persistence operations.

    This port abstracts database operations for CV analyses,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Save a CV analysis.

        Args:
            cv_analysis: CV analysis to save

        Returns:
            Saved CV analysis with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        """Retrieve a CV analysis by ID.

        Args:
            cv_analysis_id: CV analysis identifier

        Returns:
            CV analysis if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(self, candidate_id: UUID) -> list[CVAnalysis]:
        """Retrieve all CV analyses for a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of CV analyses
        """
        pass

    @abstractmethod
    async def get_latest_by_candidate_id(
        self,
        candidate_id: UUID,
    ) -> CVAnalysis | None:
        """Retrieve the most recent CV analysis for a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            Latest CV analysis if found, None otherwise
        """
        pass

    @abstractmethod
    async def update(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Update an existing CV analysis.

        Args:
            cv_analysis: CV analysis with updated data

        Returns:
            Updated CV analysis
        """
        pass

    @abstractmethod
    async def delete(self, cv_analysis_id: UUID) -> bool:
        """Delete a CV analysis.

        Args:
            cv_analysis_id: CV analysis identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/ports/cv_analyzer_port.py">
"""CV Analyzer port interface."""

from abc import ABC, abstractmethod

from ..models.cv_analysis import CVAnalysis


class CVAnalyzerPort(ABC):
    """Interface for CV analysis operations.

    This port abstracts CV parsing and analysis, allowing different
    implementations (spaCy, LangChain, etc.).
    """

    @abstractmethod
    async def analyze_cv(
        self,
        cv_file_path: str,
        candidate_id: str,
    ) -> CVAnalysis:
        """Analyze a CV file and extract structured information.

        Args:
            cv_file_path: Path to CV file (PDF, DOC, DOCX)
            candidate_id: ID of the candidate

        Returns:
            CVAnalysis with extracted skills, experience, and metadata
        """
        pass

    @abstractmethod
    async def extract_text_from_file(self, file_path: str) -> str:
        """Extract text from a document file.

        Args:
            file_path: Path to document file

        Returns:
            Extracted text content
        """
        pass
</file>

<file path="src/domain/ports/interview_repository_port.py">
"""Interview repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.interview import Interview, InterviewStatus


class InterviewRepositoryPort(ABC):
    """Interface for interview persistence operations.

    This port abstracts database operations for interviews,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, interview: Interview) -> Interview:
        """Save an interview.

        Args:
            interview: Interview to save

        Returns:
            Saved interview with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        """Retrieve an interview by ID.

        Args:
            interview_id: Interview identifier

        Returns:
            Interview if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(
        self,
        candidate_id: UUID,
        status: InterviewStatus | None = None,
    ) -> list[Interview]:
        """Retrieve interviews for a candidate.

        Args:
            candidate_id: Candidate identifier
            status: Optional status filter

        Returns:
            List of interviews
        """
        pass

    @abstractmethod
    async def get_by_status(
        self,
        status: InterviewStatus,
        limit: int = 100,
    ) -> list[Interview]:
        """Retrieve interviews by status.

        Args:
            status: Interview status
            limit: Maximum number of results

        Returns:
            List of interviews
        """
        pass

    @abstractmethod
    async def update(self, interview: Interview) -> Interview:
        """Update an existing interview.

        Args:
            interview: Interview with updated data

        Returns:
            Updated interview
        """
        pass

    @abstractmethod
    async def delete(self, interview_id: UUID) -> bool:
        """Delete an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Interview]:
        """List all interviews with pagination.

        Args:
            skip: Number of interviews to skip
            limit: Maximum number of results

        Returns:
            List of interviews
        """
        pass
</file>

<file path="src/domain/ports/question_repository_port.py">
"""Question repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.question import DifficultyLevel, Question, QuestionType


class QuestionRepositoryPort(ABC):
    """Interface for question persistence operations.

    This port abstracts database operations for questions,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, question: Question) -> Question:
        """Save a question.

        Args:
            question: Question to save

        Returns:
            Saved question with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, question_id: UUID) -> Question | None:
        """Retrieve a question by ID.

        Args:
            question_id: Question identifier

        Returns:
            Question if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_ids(self, question_ids: list[UUID]) -> list[Question]:
        """Retrieve multiple questions by IDs.

        Args:
            question_ids: List of question identifiers

        Returns:
            List of questions found
        """
        pass

    @abstractmethod
    async def find_by_skill(
        self,
        skill: str,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by skill.

        Args:
            skill: Skill to filter by
            difficulty: Optional difficulty filter
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def find_by_type(
        self,
        question_type: QuestionType,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by type.

        Args:
            question_type: Type of questions to find
            difficulty: Optional difficulty filter
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def find_by_tags(
        self,
        tags: list[str],
        match_all: bool = False,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by tags.

        Args:
            tags: Tags to search for
            match_all: If True, match all tags; if False, match any tag
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def update(self, question: Question) -> Question:
        """Update an existing question.

        Args:
            question: Question with updated data

        Returns:
            Updated question
        """
        pass

    @abstractmethod
    async def delete(self, question_id: UUID) -> bool:
        """Delete a question.

        Args:
            question_id: Question identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Question]:
        """List all questions with pagination.

        Args:
            skip: Number of questions to skip
            limit: Maximum number of results

        Returns:
            List of questions
        """
        pass
</file>

<file path="src/domain/ports/speech_to_text_port.py">
"""Speech-to-Text port interface."""

from abc import ABC, abstractmethod


class SpeechToTextPort(ABC):
    """Interface for speech-to-text operations.

    This port abstracts STT services, allowing switching between
    Azure Speech, Google Speech, etc.
    """

    @abstractmethod
    async def transcribe_audio(
        self,
        audio_file_path: str,
        language: str = "en-US",
    ) -> str:
        """Transcribe audio file to text.

        Args:
            audio_file_path: Path to audio file
            language: Language code (e.g., "en-US", "vi-VN")

        Returns:
            Transcribed text
        """
        pass

    @abstractmethod
    async def transcribe_stream(
        self,
        audio_stream: bytes,
        language: str = "en-US",
    ) -> str:
        """Transcribe streaming audio to text.

        Args:
            audio_stream: Audio data stream
            language: Language code

        Returns:
            Transcribed text
        """
        pass

    @abstractmethod
    async def detect_language(
        self,
        audio_file_path: str,
    ) -> str | None:
        """Detect language from audio file.

        Args:
            audio_file_path: Path to audio file

        Returns:
            Detected language code or None
        """
        pass
</file>

<file path="src/domain/ports/text_to_speech_port.py">
"""Text-to-Speech port interface."""

from abc import ABC, abstractmethod


class TextToSpeechPort(ABC):
    """Interface for text-to-speech operations.

    This port abstracts TTS services, allowing switching between
    Edge TTS, Google TTS, etc.
    """

    @abstractmethod
    async def synthesize_speech(
        self,
        text: str,
        language: str = "en-US",
        voice: str | None = None,
    ) -> bytes:
        """Convert text to speech audio.

        Args:
            text: Text to synthesize
            language: Language code (e.g., "en-US", "vi-VN")
            voice: Optional specific voice name

        Returns:
            Audio data as bytes
        """
        pass

    @abstractmethod
    async def save_speech_to_file(
        self,
        text: str,
        output_path: str,
        language: str = "en-US",
        voice: str | None = None,
    ) -> str:
        """Convert text to speech and save to file.

        Args:
            text: Text to synthesize
            output_path: Path where audio file should be saved
            language: Language code
            voice: Optional specific voice name

        Returns:
            Path to saved audio file
        """
        pass

    @abstractmethod
    async def list_available_voices(
        self,
        language: str | None = None,
    ) -> list[dict]:
        """List available voices.

        Args:
            language: Optional language filter

        Returns:
            List of available voices with metadata
        """
        pass
</file>

<file path="src/domain/ports/vector_search_port.py">
"""Vector search port interface."""

from abc import ABC, abstractmethod
from typing import Any
from uuid import UUID


class VectorSearchPort(ABC):
    """Interface for vector database operations.

    This port abstracts vector storage and semantic search, allowing easy
    switching between Pinecone, Weaviate, ChromaDB, etc.
    """

    @abstractmethod
    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a question's vector embedding.

        Args:
            question_id: Unique question identifier
            embedding: Vector embedding
            metadata: Additional metadata (skills, tags, difficulty, etc.)
        """
        pass

    @abstractmethod
    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a CV analysis vector embedding.

        Args:
            cv_analysis_id: Unique CV analysis identifier
            embedding: Vector embedding
            metadata: Additional metadata (skills, experience, etc.)
        """
        pass

    @abstractmethod
    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Find similar questions using semantic search.

        Args:
            query_embedding: Query vector (e.g., from CV or previous context)
            top_k: Number of results to return
            filters: Optional filters (e.g., difficulty, skills)

        Returns:
            List of similar questions with similarity scores
        """
        pass

    @abstractmethod
    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Calculate similarity between answer and reference answers.

        Args:
            answer_embedding: Candidate's answer embedding
            reference_embeddings: Reference answer embeddings

        Returns:
            Similarity score (0-1)
        """
        pass

    @abstractmethod
    async def get_embedding(
        self,
        text: str,
    ) -> list[float]:
        """Generate embedding for text.

        Args:
            text: Text to embed

        Returns:
            Vector embedding
        """
        pass

    @abstractmethod
    async def delete_embeddings(
        self,
        ids: list[UUID],
    ) -> None:
        """Delete embeddings by IDs.

        Args:
            ids: List of IDs to delete
        """
        pass
</file>

<file path="src/infrastructure/database/__init__.py">
"""Database infrastructure package."""

from .base import Base
from .session import AsyncSessionLocal, close_db, get_async_session, get_engine, init_db

__all__ = [
    "get_async_session",
    "init_db",
    "close_db",
    "AsyncSessionLocal",
    "get_engine",
    "Base",
]
</file>

<file path="src/infrastructure/database/session.py">
"""Database session management with async support."""

from collections.abc import AsyncGenerator

from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)
from sqlalchemy.pool import NullPool, QueuePool

from ..config.settings import get_settings

# Global engine instance
_async_engine: AsyncEngine | None = None
AsyncSessionLocal: async_sessionmaker[AsyncSession] | None = None


def create_engine() -> AsyncEngine:
    """Create and configure the async database engine.

    Returns:
        Configured async SQLAlchemy engine

    Notes:
        - Uses connection pooling in production for better performance
        - Disables pooling in testing to avoid connection leaks
        - Enables echo in development for SQL debugging
    """
    settings = get_settings()

    # Configure pool based on environment
    is_prod = settings.is_production()
    poolclass = QueuePool if is_prod else NullPool

    # Base engine configuration
    engine_config = {
        "url": settings.async_database_url,
        "echo": settings.debug,  # Log SQL in debug mode
        "poolclass": poolclass,
    }

    # Add pool-specific parameters only when using QueuePool
    if is_prod:
        engine_config.update({
            "pool_size": 10,
            "max_overflow": 20,
            "pool_pre_ping": True,  # Verify connections before using
            "pool_recycle": 3600,  # Recycle connections after 1 hour
        })

    engine = create_async_engine(**engine_config)

    return engine


async def init_db() -> None:
    """Initialize database engine and session factory.

    This should be called once during application startup.
    Creates the async engine and configures the session factory.
    """
    global _async_engine, AsyncSessionLocal

    if _async_engine is None:
        _async_engine = create_engine()
        AsyncSessionLocal = async_sessionmaker(
            bind=_async_engine,
            class_=AsyncSession,
            expire_on_commit=False,  # Don't expire objects after commit
            autocommit=False,
            autoflush=False,
        )


async def close_db() -> None:
    """Close database connections and cleanup resources.

    This should be called during application shutdown.
    Disposes the engine and closes all connections.
    """
    global _async_engine, AsyncSessionLocal

    if _async_engine is not None:
        await _async_engine.dispose()
        _async_engine = None
        AsyncSessionLocal = None


async def get_async_session() -> AsyncGenerator[AsyncSession, None]:
    """Get an async database session.

    This is a dependency injection function for FastAPI.
    It yields a session and ensures proper cleanup.

    Yields:
        AsyncSession: Database session

    Example:
        ```python
        @app.get("/users")
        async def get_users(db: AsyncSession = Depends(get_async_session)):
            result = await db.execute(select(User))
            return result.scalars().all()
        ```
    """
    if AsyncSessionLocal is None:
        raise RuntimeError(
            "Database not initialized. Call init_db() during application startup."
        )

    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()


def get_engine() -> AsyncEngine:
    """Get the current async database engine.

    Returns:
        AsyncEngine: The global async engine instance

    Raises:
        RuntimeError: If database not initialized
    """
    if _async_engine is None:
        raise RuntimeError(
            "Database not initialized. Call init_db() during application startup."
        )
    return _async_engine
</file>

<file path="tests/integration/api/test_planning_endpoints.py">
"""Integration tests for Phase 05: Planning API endpoints."""

from uuid import uuid4

import pytest
from fastapi.testclient import TestClient

# Note: These are integration test templates
# Actual implementation would require FastAPI app fixture


class TestPlanningEndpoints:
    """Test REST API planning endpoints."""

    @pytest.mark.integration
    def test_plan_interview_endpoint_success(self):
        """Test POST /interviews/plan success case."""
        # Template for integration test
        # Requires: FastAPI app, test database, mocked LLM

        # Expected request
        request_data = {
            "cv_analysis_id": str(uuid4()),
            "candidate_id": str(uuid4()),
        }

        # Expected response (202 Accepted)
        expected_status = 202
        expected_fields = [
            "interview_id",
            "status",
            "planned_question_count",
            "plan_metadata",
            "message",
        ]

        # Test would verify:
        # 1. POST /interviews/plan returns 202
        # 2. Response contains all expected fields
        # 3. status = "READY"
        # 4. planned_question_count matches skill diversity
        # 5. plan_metadata contains strategy, n, generated_at

        # Example assertion structure:
        # response = client.post("/interviews/plan", json=request_data)
        # assert response.status_code == expected_status
        # data = response.json()
        # for field in expected_fields:
        #     assert field in data
        # assert data["status"] == "READY"

        pass  # Template only

    @pytest.mark.integration
    def test_plan_interview_cv_not_found(self):
        """Test POST /interviews/plan with non-existent CV."""
        # Template: Verify 404 when CV analysis not found

        request_data = {
            "cv_analysis_id": str(uuid4()),  # Non-existent
            "candidate_id": str(uuid4()),
        }

        # Expected: 404 Not Found
        # Error detail: "CV analysis {id} not found"

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_ready(self):
        """Test GET /interviews/{id}/plan when READY."""
        # Template: Verify planning status check

        # Steps:
        # 1. Create planned interview
        # 2. GET /interviews/{id}/plan
        # 3. Verify status="READY", message contains question count

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_preparing(self):
        """Test GET /interviews/{id}/plan when PREPARING."""
        # Template: Verify status during planning

        # Expected message: "Interview planning in progress..."

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_interview_not_found(self):
        """Test GET /interviews/{id}/plan with non-existent interview."""
        # Template: Verify 404 error

        # Expected: 404 Not Found

        pass  # Template only


class TestAdaptiveInterviewFlow:
    """Test complete adaptive interview flow via API."""

    @pytest.mark.integration
    def test_complete_adaptive_flow(self):
        """Test end-to-end adaptive interview flow."""
        # Template for full flow test

        # Steps:
        # 1. Create CV analysis
        # 2. POST /interviews/plan
        # 3. Verify READY status
        # 4. PUT /interviews/{id}/start
        # 5. Connect WebSocket
        # 6. Receive first question
        # 7. Submit low-similarity answer
        # 8. Receive evaluation with similarity_score
        # 9. Receive follow-up question
        # 10. Submit follow-up answer
        # 11. Receive next main question
        # 12. Complete all questions
        # 13. Verify interview COMPLETED

        pass  # Template only

    @pytest.mark.integration
    def test_adaptive_vs_legacy_mode(self):
        """Test adaptive mode is triggered by plan_metadata."""
        # Template: Verify mode detection

        # Test A: Interview with plan_metadata -> adaptive mode
        # - Receives similarity_score and gaps in evaluation
        # - May receive follow-up questions

        # Test B: Interview without plan_metadata -> legacy mode
        # - No similarity_score or gaps
        # - No follow-up questions

        pass  # Template only


class TestFollowUpQuestionDelivery:
    """Test follow-up question delivery via WebSocket."""

    @pytest.mark.integration
    def test_followup_message_structure(self):
        """Test follow-up WebSocket message structure."""
        # Template: Verify message format

        expected_message_fields = {
            "type": "follow_up_question",
            "question_id": "uuid",
            "parent_question_id": "uuid",
            "text": "string",
            "generated_reason": "string",
            "order_in_sequence": "int",
            "audio_data": "base64_string",
        }

        # Test would verify all fields present and correct types

        pass  # Template only

    @pytest.mark.integration
    def test_max_3_followups_enforced(self):
        """Test max 3 follow-ups per question enforced."""
        # Template: Verify limit

        # Steps:
        # 1. Submit answer -> follow-up 1
        # 2. Submit answer -> follow-up 2
        # 3. Submit answer -> follow-up 3
        # 4. Submit answer -> next main question (no 4th follow-up)

        pass  # Template only

    @pytest.mark.integration
    def test_followup_stops_at_high_similarity(self):
        """Test no follow-up when answer meets threshold."""
        # Template: Verify threshold logic

        # Submit answer with high similarity (>= 80%)
        # -> No follow-up
        # -> Next main question

        pass  # Template only


class TestEvaluationEnhancement:
    """Test evaluation response enhancements for adaptive mode."""

    @pytest.mark.integration
    def test_evaluation_includes_similarity_score(self):
        """Test evaluation message includes similarity_score."""
        # Template: Verify adaptive evaluation fields

        expected_fields = [
            "type",  # "evaluation"
            "answer_id",
            "score",
            "feedback",
            "strengths",
            "weaknesses",
            "similarity_score",  # NEW for adaptive
            "gaps",  # NEW for adaptive
        ]

        pass  # Template only

    @pytest.mark.integration
    def test_gaps_structure(self):
        """Test gaps field structure in evaluation."""
        # Template: Verify gaps format

        expected_gaps_structure = {
            "concepts": ["list", "of", "missing"],
            "keywords": ["list", "of", "keywords"],
            "confirmed": True,  # or False
            "severity": "minor",  # or "moderate", "major"
        }

        pass  # Template only


class TestEndpointIntegration:
    """Test API endpoint integration with adaptive interviews."""

    @pytest.mark.integration
    def test_existing_endpoints_work_with_adaptive(self):
        """Test existing endpoints work with adaptive interviews."""
        # Template: Verify endpoints work with adaptive flow

        endpoints_to_test = [
            "POST /interviews/plan",  # Plan interview
            "GET /interviews/{id}",  # Get interview
            "PUT /interviews/{id}/start",  # Start interview
            "GET /interviews/{id}/questions/current",  # Get question
        ]

        # Verify all work with adaptive request/response structure

        pass  # Template only


# Test execution notes:
# These are integration test templates that demonstrate what should be tested.
# Actual implementation requires:
# 1. FastAPI test client fixture
# 2. Test database setup/teardown
# 3. Mocked external services (LLM, Vector DB, TTS)
# 4. WebSocket test client
# 5. Async test support (pytest-asyncio)
#
# Run integration tests with:
# pytest tests/integration -m integration --asyncio-mode=auto
</file>

<file path="tests/unit/use_cases/test_plan_interview.py">
"""Tests for Phase 03: PlanInterviewUseCase."""

from uuid import uuid4

import pytest

from src.application.use_cases.plan_interview import PlanInterviewUseCase
from src.domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from src.domain.models.interview import InterviewStatus


class TestPlanInterviewUseCase:
    """Test PlanInterviewUseCase."""

    @pytest.mark.asyncio
    async def test_plan_interview_with_2_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test planning with 2 skills -> n=2 questions."""
        # Create CV with 2 skills
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Python developer",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate"),
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        # Execute use case
        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=2
        assert interview.planned_question_count == 2
        assert len(interview.question_ids) == 2
        assert interview.status == InterviewStatus.READY
        assert "strategy" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_plan_interview_with_4_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test planning with 4 skills -> n=3 questions."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Full-stack developer",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="React", category="technical", proficiency="advanced"),
                ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate"),
                ExtractedSkill(skill="Docker", category="technical", proficiency="beginner"),
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=3
        assert interview.planned_question_count == 3
        assert len(interview.question_ids) == 3

    @pytest.mark.asyncio
    async def test_plan_interview_with_7_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test planning with 7 skills -> n=4 questions."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Senior engineer",
            skills=[
                ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                for i in range(7)
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=4
        assert interview.planned_question_count == 4
        assert len(interview.question_ids) == 4

    @pytest.mark.asyncio
    async def test_plan_interview_with_10_skills_max_5(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test planning with 10 skills -> n=5 (max)."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Tech lead",
            skills=[
                ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                for i in range(10)
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=5 (max cap)
        assert interview.planned_question_count == 5
        assert len(interview.question_ids) == 5

    @pytest.mark.asyncio
    async def test_plan_interview_questions_have_ideal_answer(
        self,
        mock_llm,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test all generated questions have ideal_answer and rationale."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Verify all questions have ideal_answer
        for question_id in interview.question_ids:
            question = await mock_question_repo.get_by_id(question_id)
            assert question is not None
            assert question.has_ideal_answer() is True
            assert question.is_planned is True
            assert question.ideal_answer is not None
            assert question.rationale is not None

    @pytest.mark.asyncio
    async def test_plan_interview_cv_not_found(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test error when CV analysis not found."""
        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        with pytest.raises(ValueError, match="CV analysis .* not found"):
            await use_case.execute(
                cv_analysis_id=uuid4(),  # Non-existent
                candidate_id=uuid4(),
            )

    @pytest.mark.asyncio
    async def test_plan_interview_metadata_stored(
        self,
        mock_llm,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test plan_metadata is properly stored."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Verify metadata
        assert "n" in interview.plan_metadata
        assert "generated_at" in interview.plan_metadata
        assert "strategy" in interview.plan_metadata
        assert interview.plan_metadata["strategy"] == "adaptive_planning_v1"
        assert "cv_summary" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_plan_interview_status_progression(
        self,
        mock_llm,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test interview status transitions PREPARING -> READY."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Final status should be READY
        assert interview.status == InterviewStatus.READY
        assert interview.cv_analysis_id == sample_cv_analysis.id


class TestQuestionCountCalculation:
    """Test n-calculation logic (skill diversity only, max 5)."""

    def test_calculate_n_for_various_skill_counts(self):
        """Test n-calculation for different skill counts."""
        from src.application.use_cases.plan_interview import PlanInterviewUseCase

        # Create use case instance (dependencies don't matter for this test)
        use_case = PlanInterviewUseCase(
            llm=None,  # type: ignore
            cv_analysis_repo=None,  # type: ignore
            interview_repo=None,  # type: ignore
            question_repo=None,  # type: ignore
        )

        # Test cases: (skill_count, expected_n)
        test_cases = [
            (1, 2),  # 1-2 skills -> 2
            (2, 2),
            (3, 3),  # 3-4 skills -> 3
            (4, 3),
            (5, 4),  # 5-7 skills -> 4
            (6, 4),
            (7, 4),
            (8, 5),  # 8+ skills -> 5
            (10, 5),
            (20, 5),  # Still capped at 5
        ]

        for skill_count, expected_n in test_cases:
            cv = CVAnalysis(
                candidate_id=uuid4(),
                cv_file_path="/path",
            extracted_text="Test CV",
                summary="Test",
                skills=[
                    ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                    for i in range(skill_count)
                ],
            )

            n = use_case._calculate_question_count(cv)
            assert n == expected_n, f"For {skill_count} skills, expected n={expected_n}, got {n}"

    def test_calculate_n_ignores_experience_years(self):
        """Test n-calculation ignores experience years (skill diversity only)."""
        from src.application.use_cases.plan_interview import PlanInterviewUseCase

        use_case = PlanInterviewUseCase(
            llm=None,  # type: ignore
            cv_analysis_repo=None,  # type: ignore
            interview_repo=None,  # type: ignore
            question_repo=None,  # type: ignore
        )

        # Same skills, different experience
        cv_junior = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path",
            extracted_text="Test CV",
            summary="Junior",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="beginner"),
                ExtractedSkill(skill="SQL", category="technical", proficiency="beginner"),
            ],
            work_experience_years=1,
        )

        cv_senior = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path",
            extracted_text="Test CV",
            summary="Senior",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="SQL", category="technical", proficiency="expert"),
            ],
            work_experience_years=15,
        )

        # Both should get n=2 (based on 2 skills, not experience)
        assert use_case._calculate_question_count(cv_junior) == 2
        assert use_case._calculate_question_count(cv_senior) == 2
</file>

<file path="tests/unit/use_cases/test_process_answer_adaptive.py">
"""Tests for Phase 04: ProcessAnswerAdaptiveUseCase."""

from uuid import uuid4

import pytest

from src.application.use_cases.process_answer_adaptive import (
    ProcessAnswerAdaptiveUseCase,
)
from src.domain.models.interview import InterviewStatus


class TestProcessAnswerAdaptiveUseCase:
    """Test adaptive answer processing with follow-up generation."""

    @pytest.mark.asyncio
    async def test_process_answer_high_similarity_no_followup(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test high similarity (>= 80%) -> no follow-up."""
        # Setup
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        # Good answer (will get high similarity)
        answer_text = """Recursion is when a function calls itself to solve problems.
        Key concepts: base case to stop recursion, recursive case to continue,
        and call stack management. Examples: factorial, Fibonacci, tree traversal."""

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Verify no follow-up generated
        assert answer.similarity_score is not None
        assert answer.similarity_score >= 0.8
        assert follow_up is None
        assert answer.gaps is not None

    @pytest.mark.asyncio
    async def test_process_answer_low_similarity_generates_followup(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test low similarity (< 80%) with gaps -> generate follow-up."""
        # Setup
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        # Brief answer (will get low similarity)
        answer_text = "Recursion is a function calling itself."

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Verify follow-up generated (mock always generates gaps for short answers)
        assert answer.similarity_score is not None
        # Note: Mock may return high similarity, but gaps will be detected
        # Real test: if gaps confirmed, follow-up should be generated

    @pytest.mark.asyncio
    async def test_followup_max_3_limit(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test max 3 follow-ups per question."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        # Simulate 3 follow-ups already exist
        sample_interview_adaptive.adaptive_follow_ups = [uuid4(), uuid4(), uuid4()]

        answer_text = "Brief answer"  # Would normally trigger follow-up

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Should not generate 4th follow-up
        # Note: This depends on gap detection, mock may not trigger
        # In real scenario with confirmed gaps, no follow-up due to max 3

    @pytest.mark.asyncio
    async def test_answer_evaluation_stored(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test answer evaluation is properly stored."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text="Test answer",
        )

        # Verify evaluation
        assert answer.evaluation is not None
        assert answer.evaluation.score > 0
        assert answer.evaluation.reasoning is not None
        assert len(answer.evaluation.strengths) > 0

    @pytest.mark.asyncio
    async def test_similarity_calculation_with_ideal_answer(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test similarity is calculated when ideal_answer exists."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text="Recursion calls itself with base case and examples",
        )

        # Similarity should be calculated
        assert answer.similarity_score is not None
        assert 0.0 <= answer.similarity_score <= 1.0

    @pytest.mark.asyncio
    async def test_no_similarity_without_ideal_answer(
        self,
        sample_interview_adaptive,
        sample_question_without_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test no similarity calculation when ideal_answer missing."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_without_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, follow_up, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_without_ideal_answer.id,
            answer_text="Tell me about a project",
        )

        # No similarity for behavioral questions
        assert answer.similarity_score is None
        assert follow_up is None  # No follow-ups without ideal_answer

    @pytest.mark.asyncio
    async def test_interview_not_found_error(
        self,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when interview not found."""
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Interview .* not found"):
            await use_case.execute(
                interview_id=uuid4(),  # Non-existent
                question_id=sample_question_with_ideal_answer.id,
                answer_text="Test",
            )

    @pytest.mark.asyncio
    async def test_question_not_found_error(
        self,
        sample_interview_adaptive,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when question not found."""
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Question .* not found"):
            await use_case.execute(
                interview_id=sample_interview_adaptive.id,
                question_id=uuid4(),  # Non-existent
                answer_text="Test",
            )

    @pytest.mark.asyncio
    async def test_interview_wrong_status_error(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when interview not IN_PROGRESS."""
        sample_interview_adaptive.status = InterviewStatus.COMPLETED
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Interview not in progress"):
            await use_case.execute(
                interview_id=sample_interview_adaptive.id,
                question_id=sample_question_with_ideal_answer.id,
                answer_text="Test",
            )


class TestGapDetection:
    """Test hybrid gap detection (keywords + LLM)."""

    @pytest.mark.asyncio
    async def test_keyword_gap_detection(self):
        """Test keyword-based gap detection."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        ideal_answer = """Recursion is a function calling itself.
        Key concepts: base case, recursive case, call stack, termination condition."""

        # Complete answer -> few missing keywords
        complete_answer = """Recursion calls itself with base case and recursive case.
        The call stack tracks each call and needs termination."""
        gaps = use_case._detect_keyword_gaps(complete_answer, ideal_answer)
        assert len(gaps) <= 5  # Few missing (threshold is >3, so can be 0-4)

        # Brief answer -> many missing keywords
        brief_answer = "Recursion is calling itself."
        gaps = use_case._detect_keyword_gaps(brief_answer, ideal_answer)
        assert len(gaps) > 3  # Significant gaps

    @pytest.mark.asyncio
    async def test_hybrid_gap_detection_no_keywords(self):
        """Test hybrid detection when no keyword gaps."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        # Very similar texts
        ideal = "Python is a programming language"
        answer = "Python is a programming language"

        result = await use_case._detect_gaps_hybrid(
            answer_text=answer,
            ideal_answer=ideal,
            question_text="What is Python?",
        )

        # No keyword gaps -> no LLM call -> not confirmed
        assert result["confirmed"] is False
        assert len(result["concepts"]) == 0


class TestFollowUpDecisionLogic:
    """Test _should_generate_followup decision logic."""

    def test_should_not_generate_max_followups_reached(
        self, sample_answer_low_similarity
    ):
        """Test no follow-up when max 3 reached."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        should_generate = use_case._should_generate_followup(
            sample_answer_low_similarity, follow_up_count=3
        )

        assert should_generate is False

    def test_should_not_generate_high_similarity(self, sample_answer_high_similarity):
        """Test no follow-up when similarity >= 80%."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        should_generate = use_case._should_generate_followup(
            sample_answer_high_similarity, follow_up_count=0
        )

        assert should_generate is False

    def test_should_generate_low_similarity_with_gaps(
        self, sample_answer_low_similarity
    ):
        """Test follow-up generation when similarity < 80% and gaps confirmed."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        should_generate = use_case._should_generate_followup(
            sample_answer_low_similarity, follow_up_count=0
        )

        # Low similarity + confirmed gaps -> generate
        assert should_generate is True
</file>

<file path="alembic/env.py">
"""Alembic environment configuration for async SQLAlchemy."""

import asyncio
import re
from logging.config import fileConfig

from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context
from alembic.script import ScriptDirectory

# Import Base and all models for autogenerate support
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.infrastructure.config.settings import get_settings
from src.infrastructure.database.base import Base

# Import all models to ensure they're registered with Base.metadata
from src.adapters.persistence import models  # noqa: F401

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Get database URL from settings
settings = get_settings()
config.set_main_option("sqlalchemy.url", settings.async_database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata


def get_next_sequential_revision():
    """Get the next sequential revision number by scanning existing migrations.

    Scans all migration files in the versions directory and finds the highest
    numeric revision ID, then returns the next one formatted as 4-digit zero-padded.

    Returns:
        str: Next sequential revision ID (e.g., "0001", "0002", etc.)
    """
    script_dir = ScriptDirectory.from_config(config)
    versions_dir = Path(script_dir.versions)

    max_revision = 0

    # Scan all Python files in versions directory
    for file in versions_dir.glob("*.py"):
        if file.name.startswith("__"):
            continue
        try:
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
                # Look for revision = "..." or revision: str = "..."
                # Match both quoted strings and numeric-only IDs (4+ digits for sequential)
                # Pattern matches: revision = "0001" or revision: str = '0001'
                match = re.search(
                    r'revision\s*[=:]\s*["\'](\d{4,})["\']',
                    content
                )
                if match:
                    rev_id = match.group(1)
                    # Only consider pure numeric IDs (sequential ones)
                    if rev_id.isdigit():
                        rev_num = int(rev_id)
                        max_revision = max(max_revision, rev_num)
        except Exception:
            # Skip files that can't be read or parsed
            pass

    return f"{max_revision + 1:04d}"  # Returns "0001", "0002", etc.


def process_revision_directives(context, revision, directives):
    """Customize revision ID generation to use sequential numbers.

    This hook is called by Alembic when generating new revision scripts.
    If sequential revisions are enabled in config, it replaces the auto-generated
    revision ID with a sequential numeric one.

    Args:
        context: Alembic migration context
        revision: Tuple of (revision_id, branch_labels) or None
        directives: List of revision directive objects
    """
    use_sequential = config.get_main_option(
        "use_sequential_revisions", "true"
    ).lower() == "true"

    if use_sequential and directives:
        # Check if we should override the revision ID
        # Override if:
        # 1. No explicit --rev-id was provided (revision is None or has auto-generated hash)
        # 2. The current rev_id doesn't look like a sequential number (4+ digits)
        current_rev_id = directives[0].rev_id if directives else None

        if current_rev_id:
            # If it's already a sequential number (4+ digits), don't override
            if current_rev_id.isdigit() and len(current_rev_id) >= 4:
                return
            # If it looks like a hash (alphanumeric, not all digits), override it
            if not current_rev_id.isdigit():
                new_rev_id = get_next_sequential_revision()
                directives[0].rev_id = new_rev_id
        else:
            # No rev_id set yet, generate sequential one
            new_rev_id = get_next_sequential_revision()
            directives[0].rev_id = new_rev_id


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,  # Detect column type changes
        compare_server_default=True,  # Detect default value changes
        process_revision_directives=process_revision_directives,
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    """Run migrations with the given connection.

    Args:
        connection: SQLAlchemy connection
    """
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,  # Detect column type changes
        compare_server_default=True,  # Detect default value changes
        process_revision_directives=process_revision_directives,
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """Run migrations in async mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.
    """
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode using async engine."""
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="src/adapters/mock/mock_llm_adapter.py">
"""Mock LLM adapter for development and testing."""

import random
from typing import Any
from uuid import UUID

from ...domain.models.answer import AnswerEvaluation
from ...domain.models.question import Question
from ...domain.ports.llm_port import LLMPort


class MockLLMAdapter(LLMPort):
    """Mock LLM adapter that returns realistic but fake responses.

    This adapter simulates LLM behavior for development and testing
    without requiring actual API calls to external services.
    """

    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
    ) -> str:
        """Generate mock question."""
        return f"Mock question about {skill} at {difficulty} difficulty?"

    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
    ) -> AnswerEvaluation:
        """Generate mock evaluation with realistic scores."""
        # Random score between 70-95 for realistic feel
        score = random.uniform(70.0, 95.0)

        # Simulate different evaluation patterns based on score
        if score >= 85:
            strengths = [
                "Clear and comprehensive explanation",
                "Good use of examples",
                "Strong technical understanding",
            ]
            weaknesses = ["Could provide more edge case handling"]
            improvements = ["Consider discussing performance implications"]
            sentiment = "confident"
        elif score >= 75:
            strengths = [
                "Solid understanding of concepts",
                "Relevant examples provided",
            ]
            weaknesses = [
                "Missing some technical details",
                "Could be more structured",
            ]
            improvements = [
                "Add more specific examples",
                "Elaborate on implementation details",
            ]
            sentiment = "positive"
        else:
            strengths = ["Basic understanding demonstrated"]
            weaknesses = [
                "Lacks depth",
                "Missing key concepts",
                "Limited examples",
            ]
            improvements = [
                "Study the fundamentals more thoroughly",
                "Provide concrete examples",
                "Explain reasoning more clearly",
            ]
            sentiment = "uncertain"

        return AnswerEvaluation(
            score=score,
            semantic_similarity=random.uniform(0.7, 0.95),
            completeness=random.uniform(0.7, 0.95),
            relevance=random.uniform(0.8, 1.0),
            sentiment=sentiment,
            reasoning=f"Mock evaluation: Answer demonstrates {sentiment} understanding of {question.text[:50]}...",
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_suggestions=improvements,
        )

    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate mock feedback report."""
        avg_score = sum(a.get("score", 75) for a in answers) / len(answers) if answers else 75
        return f"""
Mock Feedback Report for Interview {interview_id}

Overall Performance: {avg_score:.1f}/100

Questions Answered: {len(answers)} of {len(questions)}

Strengths:
- Good understanding of fundamental concepts
- Clear communication skills
- Relevant examples provided

Areas for Improvement:
- Dive deeper into technical details
- Practice explaining complex concepts
- Add more real-world examples

Recommendations:
- Review advanced topics in your field
- Practice mock interviews
- Study best practices and design patterns
"""

    async def summarize_cv(self, cv_text: str) -> str:
        """Generate mock CV summary."""
        return "Mock CV summary: Experienced professional with strong technical skills in software development."

    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract mock skills."""
        return [
            {"name": "Python", "category": "programming", "proficiency": "expert"},
            {"name": "FastAPI", "category": "framework", "proficiency": "advanced"},
            {"name": "PostgreSQL", "category": "database", "proficiency": "intermediate"},
        ]

    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate mock ideal answer."""
        return f"""Mock ideal answer for '{question_text[:50]}...':
This demonstrates comprehensive understanding of the concept with clear explanation,
relevant examples, and practical application. The answer covers all key aspects
including fundamental principles, real-world use cases, and potential edge cases."""

    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate mock rationale."""
        return """This answer demonstrates mastery by covering fundamental concepts,
providing practical examples, and explaining the reasoning behind technical choices.
A weaker answer would miss these comprehensive details."""

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Mock gap detection based on answer length.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with concept gap analysis
        """
        # Simple heuristic: short answers have gaps
        word_count = len(answer_text.split())

        if word_count < 30:
            # Simulate gaps for short answers
            return {
                "concepts": keyword_gaps[:2] if keyword_gaps else ["depth", "examples"],
                "keywords": keyword_gaps[:5],
                "confirmed": True,
                "severity": "moderate",
            }
        else:
            # Good answer, no gaps
            return {
                "concepts": [],
                "keywords": [],
                "confirmed": False,
                "severity": "minor",
            }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
    ) -> str:
        """Mock follow-up question generation.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question
            missing_concepts: List of concepts missing from answer
            severity: Gap severity
            order: Follow-up order in sequence

        Returns:
            Follow-up question text
        """
        concepts_str = ', '.join(missing_concepts[:2]) if missing_concepts else "that concept"
        return f"Can you elaborate more on {concepts_str}? Please provide specific examples."
</file>

<file path="src/adapters/persistence/__init__.py">
"""Persistence adapters package.

This package contains PostgreSQL implementations of repository ports
using SQLAlchemy ORM for data persistence.
"""

from .answer_repository import PostgreSQLAnswerRepository
from .candidate_repository import PostgreSQLCandidateRepository
from .cv_analysis_repository import PostgreSQLCVAnalysisRepository
from .follow_up_question_repository import PostgreSQLFollowUpQuestionRepository
from .interview_repository import PostgreSQLInterviewRepository
from .question_repository import PostgreSQLQuestionRepository

__all__ = [
    "PostgreSQLCandidateRepository",
    "PostgreSQLQuestionRepository",
    "PostgreSQLFollowUpQuestionRepository",
    "PostgreSQLInterviewRepository",
    "PostgreSQLAnswerRepository",
    "PostgreSQLCVAnalysisRepository",
]
</file>

<file path="src/domain/models/__init__.py">
"""Domain models package."""

from .answer import Answer, AnswerEvaluation
from .candidate import Candidate
from .cv_analysis import CVAnalysis, ExtractedSkill
from .follow_up_question import FollowUpQuestion
from .interview import Interview, InterviewStatus
from .question import DifficultyLevel, Question, QuestionType

__all__ = [
    "Candidate",
    "Interview",
    "InterviewStatus",
    "Question",
    "QuestionType",
    "DifficultyLevel",
    "Answer",
    "AnswerEvaluation",
    "CVAnalysis",
    "ExtractedSkill",
    "FollowUpQuestion",
]
</file>

<file path="src/domain/models/follow_up_question.py">
"""Follow-up question domain model."""

from datetime import datetime
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class FollowUpQuestion(BaseModel):
    """Represents an adaptive follow-up question.

    Follow-up questions are generated dynamically during interviews
    based on candidate answer gaps and similarity scores.
    """

    id: UUID = Field(default_factory=uuid4)
    parent_question_id: UUID  # Original question that triggered follow-up
    interview_id: UUID
    text: str  # The follow-up question text
    generated_reason: str  # Why this follow-up was needed (e.g., "Missing key concept: recursion")
    order_in_sequence: int  # 1st, 2nd, or 3rd follow-up for this parent question
    created_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False

    def is_first_followup(self) -> bool:
        """Check if this is the first follow-up for the parent question.

        Returns:
            True if order_in_sequence == 1
        """
        return self.order_in_sequence == 1

    def is_last_allowed(self) -> bool:
        """Check if this is the last allowed follow-up (max 3).

        Returns:
            True if order_in_sequence == 3
        """
        return self.order_in_sequence == 3
</file>

<file path="src/main.py">
"""Main application entry point.

This module sets up the FastAPI application with all routes and middleware.
"""

import logging
from contextlib import asynccontextmanager
from uuid import UUID

from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware

from .adapters.api.rest import health_routes
from .adapters.api.rest.interview_routes import router as interview_router
from .adapters.api.websocket.interview_handler import handle_interview_websocket
from .infrastructure.config import get_settings
from .infrastructure.database import close_db, init_db

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan context manager.

    Handles startup and shutdown events.
    """
    # Startup
    settings = get_settings()
    logger.info(f"Starting {settings.app_name} v{settings.app_version}")
    logger.info(f"Environment: {settings.environment}")
    logger.info(f"Debug mode: {settings.debug}")

    # Initialize database
    logger.info("Initializing database connection...")
    await init_db()
    logger.info("Database connection established")

    yield

    # Shutdown
    logger.info("Shutting down application...")
    logger.info("Closing database connections...")
    await close_db()
    logger.info("Database connections closed")


def create_app() -> FastAPI:
    """Create and configure the FastAPI application.

    Returns:
        Configured FastAPI application instance
    """
    settings = get_settings()

    app = FastAPI(
        title=settings.app_name,
        version=settings.app_version,
        description="AI-powered mock interview platform",
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        lifespan=lifespan,
    )

    # CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include routers
    app.include_router(health_routes.router, tags=["Health"])
    app.include_router(
        interview_router, prefix=settings.api_prefix, tags=["Interviews"]
    )

    # WebSocket endpoint for real-time interview
    @app.websocket("/ws/interviews/{interview_id}")
    async def websocket_endpoint(
        websocket: WebSocket,
        interview_id: UUID,
    ):
        """WebSocket endpoint for real-time interview communication."""
        await handle_interview_websocket(websocket, interview_id)

    # TODO: Add more routers as they are implemented
    # app.include_router(cv_routes.router, prefix=settings.api_prefix, tags=["CV"])
    # app.include_router(question_routes.router, prefix=settings.api_prefix, tags=["Questions"])

    return app


# Create application instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()

    uvicorn.run(
        "src.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
    )
</file>

<file path="tests/conftest.py">
"""Pytest configuration and fixtures."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

import pytest

from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


@pytest.fixture
def sample_cv_analysis() -> CVAnalysis:
    """Sample CV analysis for testing."""
    return CVAnalysis(
        candidate_id=uuid4(),
        cv_file_path="/path/to/cv.pdf",
        extracted_text="Sample CV text with Python, FastAPI, PostgreSQL, and Docker experience",
        summary="Experienced Python developer with 5 years of experience",
        skills=[
            ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
            ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate"),
            ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate"),
            ExtractedSkill(skill="Docker", category="technical", proficiency="beginner"),
        ],
        work_experience_years=5,
        education_level="Bachelor's Degree",
    )


@pytest.fixture
def sample_question_with_ideal_answer() -> Question:
    """Sample question with ideal answer for adaptive testing."""
    return Question(
        text="Explain the concept of recursion in programming",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Python", "Algorithms"],
        ideal_answer="""Recursion is a programming technique where a function calls itself
        to solve a problem by breaking it down into smaller subproblems. Key concepts include:
        1) Base case: A condition that stops the recursion
        2) Recursive case: The function calling itself with modified parameters
        3) Stack management: Each call is added to the call stack
        Example: Fibonacci sequence, factorial calculation, tree traversal""",
        rationale="""This answer demonstrates mastery by covering the fundamental concepts
        (base case, recursive case), explaining the mechanism (call stack), and providing
        concrete examples. A weaker answer would miss these comprehensive details.""",
    )


@pytest.fixture
def sample_question_without_ideal_answer() -> Question:
    """Sample question without ideal answer (legacy mode)."""
    return Question(
        text="Tell me about a challenging project you worked on",
        question_type=QuestionType.BEHAVIORAL,
        difficulty=DifficultyLevel.EASY,
        skills=["Communication"],
    )


@pytest.fixture
def sample_interview_adaptive(sample_cv_analysis: CVAnalysis) -> Interview:
    """Sample adaptive interview with plan_metadata."""
    interview = Interview(
        candidate_id=sample_cv_analysis.candidate_id,
        status=InterviewStatus.READY,
        cv_analysis_id=sample_cv_analysis.id,
    )
    interview.plan_metadata = {
        "n": 3,
        "generated_at": datetime.utcnow().isoformat(),
        "strategy": "adaptive_planning_v1",
        "cv_summary": sample_cv_analysis.summary,
    }
    interview.question_ids = [uuid4(), uuid4(), uuid4()]
    # Start interview to set status to IN_PROGRESS
    interview.start()
    return interview


@pytest.fixture
def sample_interview_legacy() -> Interview:
    """Sample legacy interview without plan_metadata."""
    return Interview(
        candidate_id=uuid4(),
        status=InterviewStatus.IN_PROGRESS,
    )


@pytest.fixture
def sample_answer_high_similarity(sample_question_with_ideal_answer: Question) -> Answer:
    """Sample answer with high similarity (>= 80%)."""
    answer = Answer(
        interview_id=uuid4(),
        question_id=sample_question_with_ideal_answer.id,
        candidate_id=uuid4(),
        text="""Recursion is when a function calls itself to solve problems.
        It needs a base case to stop and a recursive case to continue.
        The call stack tracks each call. Examples include factorial and Fibonacci.""",
        is_voice=False,
        similarity_score=0.85,
        gaps={"concepts": [], "keywords": [], "confirmed": False},
    )
    answer.evaluate(
        AnswerEvaluation(
            score=85.0,
            semantic_similarity=0.85,
            completeness=0.9,
            relevance=0.95,
            sentiment="confident",
            reasoning="Strong answer covering key concepts",
            strengths=["Clear explanation", "Good examples"],
            weaknesses=["Could add more detail on stack management"],
            improvement_suggestions=["Explain stack overflow scenarios"],
        )
    )
    return answer


@pytest.fixture
def sample_answer_low_similarity(sample_question_with_ideal_answer: Question) -> Answer:
    """Sample answer with low similarity (< 80%) - should trigger follow-up."""
    answer = Answer(
        interview_id=uuid4(),
        question_id=sample_question_with_ideal_answer.id,
        candidate_id=uuid4(),
        text="Recursion is a function that calls itself.",
        is_voice=False,
        similarity_score=0.45,
        gaps={
            "concepts": ["base case", "recursive case", "call stack"],
            "keywords": ["base", "stack", "parameters"],
            "confirmed": True,
            "severity": "major",
        },
    )
    answer.evaluate(
        AnswerEvaluation(
            score=55.0,
            semantic_similarity=0.45,
            completeness=0.4,
            relevance=0.8,
            sentiment="uncertain",
            reasoning="Answer is too brief and missing key concepts",
            strengths=["Correct basic definition"],
            weaknesses=["Missing base case", "No examples", "Lacks depth"],
            improvement_suggestions=[
                "Explain base case and recursive case",
                "Provide examples",
                "Discuss call stack",
            ],
        )
    )
    return answer


@pytest.fixture
def sample_follow_up_question(sample_question_with_ideal_answer: Question) -> FollowUpQuestion:
    """Sample follow-up question."""
    return FollowUpQuestion(
        parent_question_id=sample_question_with_ideal_answer.id,
        interview_id=uuid4(),
        text="Can you explain what a base case is in recursion and why it's important?",
        generated_reason="Missing concepts: base case, termination condition",
        order_in_sequence=1,
    )


# Mock repository fixtures
class MockQuestionRepository:
    """Mock question repository for testing."""

    def __init__(self) -> None:
        self.questions: dict[UUID, Question] = {}

    async def save(self, question: Question | FollowUpQuestion) -> Question | FollowUpQuestion:
        self.questions[question.id] = question  # type: ignore
        return question

    async def get_by_id(self, question_id: UUID) -> Question | None:
        return self.questions.get(question_id)  # type: ignore


class MockInterviewRepository:
    """Mock interview repository for testing."""

    def __init__(self) -> None:
        self.interviews: dict[UUID, Interview] = {}

    async def save(self, interview: Interview) -> Interview:
        self.interviews[interview.id] = interview
        return interview

    async def update(self, interview: Interview) -> Interview:
        self.interviews[interview.id] = interview
        return interview

    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        return self.interviews.get(interview_id)


class MockAnswerRepository:
    """Mock answer repository for testing."""

    def __init__(self) -> None:
        self.answers: dict[UUID, Answer] = {}

    async def save(self, answer: Answer) -> Answer:
        self.answers[answer.id] = answer
        return answer

    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        return [a for a in self.answers.values() if a.interview_id == interview_id]


class MockCVAnalysisRepository:
    """Mock CV analysis repository for testing."""

    def __init__(self) -> None:
        self.analyses: dict[UUID, CVAnalysis] = {}

    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        self.analyses[cv_analysis.id] = cv_analysis
        return cv_analysis

    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        return self.analyses.get(cv_analysis_id)


class MockVectorSearch:
    """Mock vector search for testing."""

    async def get_embedding(self, text: str) -> list[float]:
        """Return mock embedding."""
        # Simple hash-based mock embedding
        hash_val = hash(text.lower()[:50])
        return [float((hash_val >> i) & 1) for i in range(128)]

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Return mock similarity score based on text length."""
        # Simple mock: longer answers get higher similarity
        return 0.85 if len(answer_embedding) > 100 else 0.45


class MockLLM:
    """Mock LLM for testing."""

    async def evaluate_answer(
        self, question: Question, answer_text: str, context: dict[str, Any]
    ) -> AnswerEvaluation:
        """Return mock evaluation."""
        score = 85.0 if len(answer_text) > 50 else 55.0
        return AnswerEvaluation(
            score=score,
            semantic_similarity=score / 100,
            completeness=score / 100,
            relevance=0.9,
            sentiment="confident" if score > 70 else "uncertain",
            reasoning="Mock evaluation",
            strengths=["Good understanding"],
            weaknesses=["Could be more detailed"],
            improvement_suggestions=["Add examples"],
        )

    async def generate_question(
        self, context: dict[str, Any], skill: str, difficulty: str
    ) -> str:
        """Return mock question."""
        return f"Mock question about {skill} at {difficulty} level"

    async def generate_ideal_answer(
        self, question_text: str, context: dict[str, Any]
    ) -> str:
        """Return mock ideal answer."""
        return f"Mock ideal answer for: {question_text[:50]}..."

    async def generate_rationale(
        self, question_text: str, ideal_answer: str
    ) -> str:
        """Return mock rationale."""
        return "Mock rationale explaining why this is an ideal answer"

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Mock gap detection based on answer length."""
        # Simple heuristic: short answers have gaps
        word_count = len(answer_text.split())

        if word_count < 30:
            # Simulate gaps for short answers
            return {
                "concepts": keyword_gaps[:2] if keyword_gaps else ["depth", "examples"],
                "keywords": keyword_gaps[:5],
                "confirmed": True,
                "severity": "moderate",
            }
        else:
            # Good answer, no gaps
            return {
                "concepts": [],
                "keywords": [],
                "confirmed": False,
                "severity": "minor",
            }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
    ) -> str:
        """Mock follow-up question generation."""
        concepts_str = ', '.join(missing_concepts[:2]) if missing_concepts else "that concept"
        return f"Can you elaborate more on {concepts_str}? Please provide specific examples."


@pytest.fixture
def mock_question_repo() -> MockQuestionRepository:
    """Mock question repository fixture."""
    return MockQuestionRepository()


@pytest.fixture
def mock_interview_repo() -> MockInterviewRepository:
    """Mock interview repository fixture."""
    return MockInterviewRepository()


@pytest.fixture
def mock_answer_repo() -> MockAnswerRepository:
    """Mock answer repository fixture."""
    return MockAnswerRepository()


@pytest.fixture
def mock_cv_analysis_repo() -> MockCVAnalysisRepository:
    """Mock CV analysis repository fixture."""
    return MockCVAnalysisRepository()


@pytest.fixture
def mock_vector_search() -> MockVectorSearch:
    """Mock vector search fixture."""
    return MockVectorSearch()


@pytest.fixture
def mock_llm() -> MockLLM:
    """Mock LLM fixture."""
    return MockLLM()
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Role & Responsibilities

Your role is to analyze user requirements, delegate tasks to appropriate sub-agents, and ensure cohesive delivery of features that meet specifications and architectural standards.

## Workflows

- Primary workflow: `./.claude/workflows/primary-workflow.md`
- Development rules: `./.claude/workflows/development-rules.md`
- Orchestration protocols: `./.claude/workflows/orchestration-protocol.md`
- Documentation management: `./.claude/workflows/documentation-management.md`
- And other workflows: `./.claude/workflows/*`

**IMPORTANT:** You must follow strictly the development rules in `./.claude/workflows/development-rules.md` file.
**IMPORTANT:** Before you plan or proceed any implementation, always read the `./README.md` file first to get context.
**IMPORTANT:** Sacrifice grammar for the sake of concision when writing reports.
**IMPORTANT:** In reports, list any unresolved questions at the end, if any.
**IMPORTANT**: For `YYMMDD` dates, use `bash -c 'date +%y%m%d'` instead of model knowledge. Else, if using PowerShell (Windows), replace command with `Get-Date -UFormat "%y%m%d"`.

## Documentation Management

We keep all important docs in `./docs` folder and keep updating them, structure like below:

```
./docs
â”œâ”€â”€ project-overview-pdr.md
â”œâ”€â”€ code-standards.md
â”œâ”€â”€ codebase-summary.md
â”œâ”€â”€ design-guidelines.md
â”œâ”€â”€ deployment-guide.md
â”œâ”€â”€ system-architecture.md
â””â”€â”€ project-roadmap.md
```
## Project Overview

**Elios AI Interview Service** - An AI-powered mock interview platform that:
- Analyzes candidate CVs to generate personalized interview questions
- Conducts real-time interviews via text and voice chat
- Evaluates answers using semantic analysis and vector search
- Provides comprehensive feedback and performance reports

## Architecture

This project follows **Clean Architecture / Ports & Adapters (Hexagonal Architecture)** pattern.

ðŸ“š **See [System Architecture](./docs/system-architecture.md) for complete details**

### Key Principles
- **Dependency Rule**: Dependencies point inward toward domain
- **Port Interfaces**: All external dependencies accessed through abstract interfaces
- **Adapter Swappability**: Change services without touching business logic
- **Testability**: Domain logic tested in isolation with mock implementations

## Project Structure

ðŸ“š **See [Codebase Summary](./docs/codebase-summary.md) for complete structure**

Quick reference:
- `src/domain/` - Core business logic (5 models, 11 ports)
- `src/application/` - Use cases and DTOs
- `src/adapters/` - External service implementations
- `src/infrastructure/` - Config, DI, logging

## Development Commands

ðŸ“š **See [README.md](./README.md#-development) for all development commands**

Quick reference:
- **Run app**: `python -m src.main`
- **Migrations**: `alembic upgrade head`
- **Tests**: `pytest --cov=src`
- **Code quality**: `ruff check src/ && black src/ && mypy src/`

## Working with the Codebase

### Adding a New External Service

When integrating a new external service (e.g., new LLM provider, vector database):

1. **Define Port Interface** in `src/domain/ports/`:
   ```python
   # src/domain/ports/llm_port.py
   from abc import ABC, abstractmethod

   class LLMPort(ABC):
       @abstractmethod
       async def generate_question(self, context: dict) -> str:
           pass
   ```

2. **Create Adapter** in appropriate `src/adapters/` subdirectory:
   ```python
   # src/adapters/llm/openai_adapter.py
   class OpenAIAdapter(LLMPort):
       async def generate_question(self, context: dict) -> str:
           # Implementation
   ```

3. **Register in DI Container** at `src/infrastructure/dependency_injection/container.py`:
   ```python
   def configure_llm(config: Settings) -> LLMPort:
       if config.llm_provider == "openai":
           return OpenAIAdapter(config.openai_api_key)
   ```

4. **Update Configuration** in `src/infrastructure/config/settings.py` if needed.

### Creating a New Use Case

1. **Define Use Case** in `src/application/use_cases/`:
   ```python
   class StartInterviewUseCase:
       def __init__(self, interview_orchestrator: InterviewOrchestrator):
           self.orchestrator = interview_orchestrator
   ```

2. **Create DTOs** in `src/application/dto/` for input/output.

3. **Expose via API** in `src/adapters/api/rest/` or `src/adapters/api/websocket/`.

### Domain Logic Changes

- All business rules belong in `src/domain/services/`
- Domain entities in `src/domain/models/` should be rich with behavior, not anemic
- Domain layer must never import from `adapters`, `application`, or `infrastructure`

### Testing Strategy

- **Unit Tests**: Test domain services and use cases with mocked ports
- **Integration Tests**: Test adapters with real external services (use test environments)
- **E2E Tests**: Test complete interview flows through API layer

### Mock Adapters

**Available Mocks** (6 total):
- `MockLLMAdapter` - Simulates LLM responses (no OpenAI API calls)
- `MockVectorSearchAdapter` - In-memory vector search (no Pinecone)
- `MockSTTAdapter` - Simulates speech-to-text
- `MockTTSAdapter` - Simulates text-to-speech
- `MockCVAnalyzerAdapter` - Filename-based CV parsing (e.g., "python-developer.pdf" â†’ ["Python", "FastAPI"])
- `MockAnalyticsAdapter` - In-memory performance tracking

**When to Use Mocks**:
- âœ… Development without API keys
- âœ… Fast unit tests (10x faster)
- âœ… CI/CD pipelines (no external dependencies)
- âœ… Deterministic test results
- âŒ Integration tests (use real adapters)
- âŒ Production deployment

**Configuration**: Set `USE_MOCK_ADAPTERS=true` in `.env.local` (default).

**DI Container**: Automatically swaps implementations based on `settings.use_mock_adapters` flag.

**Note**: Repositories (PostgreSQL) NOT mocked - use real database for data integrity.

## Technology Stack

### Core Technologies
- **Language**: Python 3.11+
- **Framework**: FastAPI (REST API), WebSocket support
- **Async**: asyncio for asynchronous operations

### Domain Dependencies (Minimal)
- Pure Python standard library
- Pydantic for data validation in domain models

### External Services (via Adapters)
- **LLM Providers**: OpenAI GPT-4, Anthropic Claude, Meta Llama 3
- **Vector Database**: Pinecone (primary), Weaviate, ChromaDB (alternatives)
- **Speech Services**: Azure Speech-to-Text, Microsoft Edge TTS
- **Database**: PostgreSQL with SQLAlchemy ORM
- **NLP**: spaCy, LangChain
- **Document Processing**: PyPDF2, python-docx

### Development Tools
- **Testing**: pytest, pytest-asyncio, pytest-cov
- **Linting**: ruff
- **Formatting**: black
- **Type Checking**: mypy
- **Migrations**: alembic

## Configuration

Configuration is managed through environment variables and `.env` files:

- `.env.example`: Template for required environment variables
- `.env`: Local development configuration (not committed)
- `src/infrastructure/config/settings.py`: Pydantic settings management

Key configuration areas:
- LLM provider selection and API keys
- Vector database connection
- Speech service credentials
- PostgreSQL connection string
- Feature flags for adapter selection

## Key Components

### 1. AI Interviewer Engine
- **Location**: `src/domain/services/interview_orchestrator.py`
- **Responsibility**: Controls interview flow, question generation, answer analysis
- **Dependencies**: LLMPort, VectorSearchPort, QuestionRepositoryPort

### 2. CV Analyzer
- **Location**: `src/domain/services/cv_analyzer_service.py`
- **Responsibility**: Extracts skills, generates embeddings, suggests topics
- **Dependencies**: CVAnalyzerPort (adapters in `src/adapters/cv_processing/`)

### 3. Question Bank
- **Location**: `src/domain/models/question.py`, `src/adapters/persistence/postgres_repository.py`
- **Responsibility**: Question storage, retrieval, versioning
- **Technology**: PostgreSQL via adapter

### 4. Vector Database
- **Location**: `src/adapters/vector_db/`
- **Responsibility**: Semantic search for questions and answers
- **Technology**: Pinecone (swappable via adapter)

### 5. Analytics & Feedback
- **Location**: `src/domain/services/feedback_generator.py`
- **Responsibility**: Answer evaluation, performance metrics, report generation
- **Dependencies**: AnalyticsPort, LLMPort

## Interview Flow

### 1. Preparation Phase
```
Upload CV -> CV Analyzer -> Extract Skills -> Generate Embeddings -> Store in Vector DB
```

### 2. Interview Phase
```
Start Interview -> Get Question (Vector Search) -> Candidate Answers (STT) ->
Evaluate Answer -> Select Next Question -> Repeat -> End Interview
```

### 3. Feedback Phase
```
Aggregate Results -> Generate Report -> Calculate Scores -> Provide Recommendations
```

## Frontend Integration

- **Frontend**: React (pure JavaScript, no .jsx/.ts/.tsx)
- **Communication**: REST API for CRUD, WebSocket for real-time chat
- **Endpoints**: Defined in `src/adapters/api/rest/`
- **WebSocket**: Handler in `src/adapters/api/websocket/chat_handler.py`

## Common Patterns

### Dependency Injection
All dependencies are injected through constructors. The DI container (`src/infrastructure/dependency_injection/container.py`) wires everything together at application startup.

### Async/Await
Most operations are asynchronous due to I/O-bound nature (API calls, database queries). Use `async`/`await` consistently.

### Error Handling
- Domain exceptions in `src/domain/exceptions.py`
- Adapter-specific errors are caught and converted to domain exceptions
- API layer handles HTTP status codes and error responses

### Logging
- Structured logging via `src/infrastructure/logging/logger.py`
- Log at appropriate levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Include context (interview_id, candidate_id) in logs

## Performance Considerations

- **Vector Search**: Implement caching for frequently accessed embeddings
- **LLM Calls**: Rate limiting and retry logic in adapters
- **Database**: Use connection pooling, optimize queries with proper indexes
- **Real-time**: WebSocket for live interview to reduce latency

## Security Notes

- **API Keys**: Never commit to repository, use environment variables
- **Candidate Data**: PII handling and data retention policies
- **Authentication**: Implement in API layer, not domain
- **Rate Limiting**: Applied at API adapter level
</file>

<file path="src/adapters/persistence/models.py">
"""SQLAlchemy models for persistence.

These models represent the database schema and map domain entities
to database tables using SQLAlchemy ORM.
"""

from datetime import datetime
from uuid import UUID

from sqlalchemy import (
    Boolean,
    DateTime,
    Float,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
)
from sqlalchemy import (
    Enum as SQLEnum,
)
from sqlalchemy.dialects.postgresql import ARRAY, JSONB
from sqlalchemy.dialects.postgresql import UUID as PGUUID
from sqlalchemy.orm import Mapped, mapped_column, relationship

from ...domain.models.interview import InterviewStatus
from ...domain.models.question import DifficultyLevel, QuestionType
from ...infrastructure.database.base import Base


class CandidateModel(Base):
    """SQLAlchemy model for Candidate entity."""

    __tablename__ = "candidates"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    email: Mapped[str] = mapped_column(String(255), unique=True, nullable=False, index=True)
    cv_file_path: Mapped[str | None] = mapped_column(String(500), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    interviews: Mapped[list["InterviewModel"]] = relationship(
        "InterviewModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )
    cv_analyses: Mapped[list["CVAnalysisModel"]] = relationship(
        "CVAnalysisModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )

    __table_args__ = (
        Index("idx_candidates_email", "email"),
        Index("idx_candidates_created_at", "created_at"),
    )


class QuestionModel(Base):
    """SQLAlchemy model for Question entity."""

    __tablename__ = "questions"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    text: Mapped[str] = mapped_column(Text, nullable=False)
    question_type: Mapped[str] = mapped_column(
        SQLEnum(QuestionType, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    difficulty: Mapped[str] = mapped_column(
        SQLEnum(DifficultyLevel, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    skills: Mapped[list[str]] = mapped_column(ARRAY(String(100)), nullable=False, default=[])
    tags: Mapped[list[str]] = mapped_column(ARRAY(String(100)), nullable=False, default=[])
    evaluation_criteria: Mapped[str | None] = mapped_column(Text, nullable=True)
    version: Mapped[int] = mapped_column(Integer, nullable=False, default=1)
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)

    # Pre-planning fields for adaptive interviews
    ideal_answer: Mapped[str | None] = mapped_column(Text, nullable=True)
    rationale: Mapped[str | None] = mapped_column(Text, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="question",
    )

    __table_args__ = (
        Index("idx_questions_type", "question_type"),
        Index("idx_questions_difficulty", "difficulty"),
        Index("idx_questions_skills", "skills", postgresql_using="gin"),
        Index("idx_questions_tags", "tags", postgresql_using="gin"),
    )


class InterviewModel(Base):
    """SQLAlchemy model for Interview entity."""

    __tablename__ = "interviews"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    status: Mapped[str] = mapped_column(
        SQLEnum(InterviewStatus, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    cv_analysis_id: Mapped[UUID | None] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("cv_analyses.id", ondelete="SET NULL"),
        nullable=True,
    )
    question_ids: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )
    answer_ids: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )
    current_question_index: Mapped[int] = mapped_column(Integer, nullable=False, default=0)

    # NEW: Pre-planning metadata for adaptive interviews
    plan_metadata: Mapped[dict] = mapped_column(JSONB, nullable=False, default={})
    adaptive_follow_ups: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )

    started_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    completed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="interviews",
    )
    cv_analysis: Mapped["CVAnalysisModel | None"] = relationship(
        "CVAnalysisModel",
        foreign_keys=[cv_analysis_id],
    )
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="interview",
        cascade="all, delete-orphan",
    )

    __table_args__ = (
        Index("idx_interviews_candidate_id", "candidate_id"),
        Index("idx_interviews_status", "status"),
        Index("idx_interviews_created_at", "created_at"),
    )


class AnswerModel(Base):
    """SQLAlchemy model for Answer entity."""

    __tablename__ = "answers"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    interview_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("interviews.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    question_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("questions.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    text: Mapped[str] = mapped_column(Text, nullable=False)
    is_voice: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False)
    audio_file_path: Mapped[str | None] = mapped_column(String(500), nullable=True)
    duration_seconds: Mapped[float | None] = mapped_column(Float, nullable=True)
    evaluation: Mapped[dict | None] = mapped_column(JSONB, nullable=True)
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)
    answer_metadata: Mapped[dict] = mapped_column("metadata", JSONB, nullable=False, default={})

    # NEW: Adaptive evaluation fields
    similarity_score: Mapped[float | None] = mapped_column(Float, nullable=True)
    gaps: Mapped[dict | None] = mapped_column(JSONB, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    evaluated_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)

    # Relationships
    interview: Mapped["InterviewModel"] = relationship(
        "InterviewModel",
        back_populates="answers",
    )
    question: Mapped["QuestionModel"] = relationship(
        "QuestionModel",
        back_populates="answers",
    )
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="answers",
    )

    __table_args__ = (
        Index("idx_answers_interview_id", "interview_id"),
        Index("idx_answers_question_id", "question_id"),
        Index("idx_answers_candidate_id", "candidate_id"),
        Index("idx_answers_created_at", "created_at"),
    )


class CVAnalysisModel(Base):
    """SQLAlchemy model for CV Analysis entity."""

    __tablename__ = "cv_analyses"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    cv_file_path: Mapped[str] = mapped_column(String(500), nullable=False)
    extracted_text: Mapped[str] = mapped_column(Text, nullable=False)
    skills: Mapped[list[dict]] = mapped_column(JSONB, nullable=False, default=[])
    work_experience_years: Mapped[float | None] = mapped_column(Float, nullable=True)
    education_level: Mapped[str | None] = mapped_column(String(100), nullable=True)
    suggested_topics: Mapped[list[str]] = mapped_column(
        ARRAY(String(200)),
        nullable=False,
        default=[],
    )
    suggested_difficulty: Mapped[str] = mapped_column(String(50), nullable=False, default="medium")
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)
    summary: Mapped[str | None] = mapped_column(Text, nullable=True)
    cv_metadata: Mapped[dict] = mapped_column("metadata", JSONB, nullable=False, default={})
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="cv_analyses",
    )

    __table_args__ = (
        Index("idx_cv_analyses_candidate_id", "candidate_id"),
        Index("idx_cv_analyses_created_at", "created_at"),
    )


class FollowUpQuestionModel(Base):
    """SQLAlchemy model for FollowUpQuestion entity."""

    __tablename__ = "follow_up_questions"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    parent_question_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("questions.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    interview_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("interviews.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    text: Mapped[str] = mapped_column(Text, nullable=False)
    generated_reason: Mapped[str] = mapped_column(Text, nullable=False)
    order_in_sequence: Mapped[int] = mapped_column(Integer, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    __table_args__ = (
        Index("idx_follow_up_questions_parent_question_id", "parent_question_id"),
        Index("idx_follow_up_questions_interview_id", "interview_id"),
        Index("idx_follow_up_questions_created_at", "created_at"),
    )
</file>

<file path="src/application/dto/interview_dto.py">
"""Interview DTOs for REST API request/response."""

from datetime import datetime
from typing import Any
from uuid import UUID

from pydantic import BaseModel


# Response DTOs
class InterviewResponse(BaseModel):
    """Response with interview details and WebSocket URL."""
    id: UUID
    candidate_id: UUID
    status: str
    cv_analysis_id: UUID | None
    question_count: int
    current_question_index: int
    progress_percentage: float
    ws_url: str  # WebSocket URL for real-time communication
    created_at: datetime
    started_at: datetime | None

    @staticmethod
    def from_domain(interview: Any, base_url: str) -> "InterviewResponse":
        """Convert domain Interview to response DTO."""
        return InterviewResponse(
            id=interview.id,
            candidate_id=interview.candidate_id,
            status=interview.status.value,
            cv_analysis_id=interview.cv_analysis_id,
            question_count=len(interview.question_ids),
            current_question_index=interview.current_question_index,
            progress_percentage=interview.get_progress_percentage(),
            ws_url=f"{base_url}/ws/interviews/{interview.id}",
            created_at=interview.created_at,
            started_at=interview.started_at,
        )


class QuestionResponse(BaseModel):
    """Response with question details."""
    id: UUID
    text: str
    question_type: str
    difficulty: str
    index: int
    total: int
    is_follow_up: bool = False
    parent_question_id: UUID | None = None


# NEW: Planning DTOs
class PlanInterviewRequest(BaseModel):
    """Request to plan interview with adaptive questions."""
    cv_analysis_id: UUID
    candidate_id: UUID


class PlanningStatusResponse(BaseModel):
    """Response with planning status."""
    interview_id: UUID
    status: str  # PREPARING, READY, IN_PROGRESS
    planned_question_count: int | None
    plan_metadata: dict | None
    message: str


class FollowUpQuestionResponse(BaseModel):
    """Response with follow-up question details."""
    id: UUID
    parent_question_id: UUID
    text: str
    generated_reason: str
    order_in_sequence: int
</file>

<file path="src/application/use_cases/process_answer_adaptive.py">
"""Process answer with adaptive follow-up logic."""

import logging
from datetime import datetime
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.models.follow_up_question import FollowUpQuestion
from ...domain.models.interview import InterviewStatus
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from ...domain.ports.vector_search_port import VectorSearchPort

logger = logging.getLogger(__name__)


class ProcessAnswerAdaptiveUseCase:
    """Process answer with adaptive evaluation and follow-up generation.

    This use case implements the adaptive interview flow:
    1. Evaluate answer using LLM
    2. Calculate similarity vs ideal_answer
    3. Detect concept gaps (hybrid: keywords + LLM)
    4. Decide if follow-up needed (similarity <80% OR gaps exist)
    5. Generate follow-up question if needed (max 3 per main question)
    """

    def __init__(
        self,
        answer_repository: AnswerRepositoryPort,
        interview_repository: InterviewRepositoryPort,
        question_repository: QuestionRepositoryPort,
        follow_up_question_repository: FollowUpQuestionRepositoryPort,
        llm: LLMPort,
        vector_search: VectorSearchPort,
    ):
        """Initialize use case with required ports.

        Args:
            answer_repository: Answer storage
            interview_repository: Interview storage
            question_repository: Question storage
            follow_up_question_repository: Follow-up question storage
            llm: LLM service for evaluation and gap detection
            vector_search: Vector database for similarity calculation
        """
        self.answer_repo = answer_repository
        self.interview_repo = interview_repository
        self.question_repo = question_repository
        self.follow_up_question_repo = follow_up_question_repository
        self.llm = llm
        self.vector_search = vector_search

    async def execute(
        self,
        interview_id: UUID,
        question_id: UUID,
        answer_text: str,
        audio_file_path: str | None = None,
    ) -> tuple[Answer, FollowUpQuestion | None, bool]:
        """Process answer with adaptive evaluation.

        Args:
            interview_id: The interview UUID
            question_id: The question UUID
            answer_text: The answer text
            audio_file_path: Optional audio file path for voice answers

        Returns:
            Tuple of (Answer with evaluation, optional FollowUpQuestion, has_more_questions)

        Raises:
            ValueError: If interview or question not found, or invalid state
        """
        logger.info(
            "Processing adaptive answer",
            extra={"interview_id": str(interview_id), "question_id": str(question_id)},
        )

        # Step 1: Validate interview
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        if interview.status != InterviewStatus.IN_PROGRESS:
            raise ValueError(f"Interview not in progress: {interview.status}")

        # Step 2: Get question with ideal_answer
        question = await self.question_repo.get_by_id(question_id)
        if not question:
            raise ValueError(f"Question {question_id} not found")

        # Step 3: Create answer
        answer = Answer(
            interview_id=interview_id,
            question_id=question_id,
            candidate_id=interview.candidate_id,
            text=answer_text,
            is_voice=bool(audio_file_path),
            audio_file_path=audio_file_path,
            similarity_score=None,  # Will be calculated if ideal_answer exists
            gaps=None,  # Will be populated by gap detection
            created_at=datetime.utcnow(),
        )

        # Step 4: Evaluate answer using LLM
        evaluation = await self.llm.evaluate_answer(
            question=question,
            answer_text=answer_text,
            context={
                "interview_id": str(interview_id),
                "candidate_id": str(interview.candidate_id),
            },
        )
        answer.evaluate(evaluation)

        # Step 5: Calculate similarity score (if ideal_answer exists)
        if question.has_ideal_answer():
            similarity_score = await self._calculate_similarity(
                answer_text, question.ideal_answer  # type: ignore
            )
            answer.similarity_score = similarity_score
            logger.info(f"Similarity score: {similarity_score:.2f}")
        else:
            logger.warning(f"Question {question_id} has no ideal_answer, skipping similarity")

        # Step 6: Detect gaps using hybrid approach
        gaps = await self._detect_gaps_hybrid(
            answer_text=answer_text,
            ideal_answer=question.ideal_answer or "",
            question_text=question.text,
        )
        answer.gaps = gaps
        logger.info(f"Detected {len(gaps.get('concepts', []))} concept gaps")

        # Step 7: Save answer
        saved_answer = await self.answer_repo.save(answer)

        # Step 8: Update interview
        interview.add_answer(saved_answer.id)
        await self.interview_repo.update(interview)

        # Step 9: Decide if follow-up needed
        follow_up_question = None
        if question.has_ideal_answer():
            # Count existing follow-ups for this question
            follow_up_count = self._count_follow_ups_for_question(interview, question_id)

            if self._should_generate_followup(saved_answer, follow_up_count):
                follow_up_question = await self._generate_followup(
                    interview_id=interview_id,
                    parent_question=question,
                    answer=saved_answer,
                    gaps=gaps,
                    order=follow_up_count + 1,
                )
                # Save follow-up question to its own table
                await self.follow_up_question_repo.save(follow_up_question)
                # Track in interview
                interview.add_adaptive_followup(follow_up_question.id)
                await self.interview_repo.update(interview)
                logger.info(f"Generated follow-up question #{follow_up_count + 1}")

        # Step 10: Check if more questions
        has_more = interview.has_more_questions()

        return saved_answer, follow_up_question, has_more

    async def _calculate_similarity(self, answer_text: str, ideal_answer: str) -> float:
        """Calculate cosine similarity between answer and ideal_answer.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer

        Returns:
            Similarity score (0-1)
        """
        # Get embeddings
        answer_embedding = await self.vector_search.get_embedding(answer_text)
        ideal_embedding = await self.vector_search.get_embedding(ideal_answer)

        # Calculate cosine similarity
        similarity = await self.vector_search.find_similar_answers(
            answer_embedding=answer_embedding,
            reference_embeddings=[ideal_embedding],
        )

        return similarity

    async def _detect_gaps_hybrid(
        self, answer_text: str, ideal_answer: str, question_text: str
    ) -> dict[str, Any]:
        """Detect concept gaps using hybrid approach (keywords + LLM).

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question asked

        Returns:
            Gaps dict with detected concepts and keywords
        """
        # Step 1: Keyword-based gap detection (fast)
        keyword_gaps = self._detect_keyword_gaps(answer_text, ideal_answer)

        # Step 2: If keywords detected gaps, confirm with LLM
        if keyword_gaps:
            llm_gaps = await self._detect_gaps_with_llm(
                answer_text=answer_text,
                ideal_answer=ideal_answer,
                question_text=question_text,
                keyword_gaps=keyword_gaps,
            )
            return llm_gaps
        else:
            # No gaps detected by keywords
            return {"concepts": [], "keywords": [], "confirmed": False}

    def _detect_keyword_gaps(self, answer_text: str, ideal_answer: str) -> list[str]:
        """Fast keyword-based gap detection.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer

        Returns:
            List of missing keywords
        """
        # Simple keyword extraction (lowercase, remove common words)
        stop_words = {
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "do",
            "does",
            "did",
            "will",
            "would",
            "should",
            "could",
            "may",
            "might",
            "must",
            "can",
            "this",
            "that",
            "these",
            "those",
        }

        # Extract words from ideal answer (strip punctuation)
        ideal_words = {
            word.lower().strip('.,!?;:"\'-')
            for word in ideal_answer.split()
            if len(word.strip('.,!?;:"\'-')) > 3 and word.lower().strip('.,!?;:"\'-') not in stop_words
        }

        # Extract words from candidate answer (strip punctuation)
        answer_words = {
            word.lower().strip('.,!?;:"\'-')
            for word in answer_text.split()
            if len(word.strip('.,!?;:"\'-')) > 3 and word.lower().strip('.,!?;:"\'-') not in stop_words
        }

        # Find missing keywords (in ideal but not in answer)
        missing = list(ideal_words - answer_words)

        # Only consider significant if >3 keywords missing
        return missing if len(missing) > 3 else []

    async def _detect_gaps_with_llm(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Use LLM to confirm and refine gap detection.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question asked
            keyword_gaps: Keywords detected as missing

        Returns:
            Refined gaps dict with concepts and confirmation
        """
        # Use port method instead of direct client access
        return await self.llm.detect_concept_gaps(
            answer_text=answer_text,
            ideal_answer=ideal_answer,
            question_text=question_text,
            keyword_gaps=keyword_gaps,
        )

    def _should_generate_followup(self, answer: Answer, follow_up_count: int) -> bool:
        """Decide if follow-up question should be generated.

        Args:
            answer: The evaluated answer
            follow_up_count: Number of existing follow-ups

        Returns:
            True if follow-up needed
        """
        # Stop if already 3 follow-ups
        if follow_up_count >= 3:
            logger.info("Max follow-ups (3) reached")
            return False

        # Check if answer is adaptive complete
        if answer.is_adaptive_complete():
            logger.info("Answer meets adaptive completion criteria")
            return False

        # Check similarity threshold
        if answer.similarity_score and answer.similarity_score >= 0.8:
            logger.info(f"Similarity {answer.similarity_score:.2f} >= 0.8, no follow-up")
            return False

        # Check if confirmed gaps exist
        if answer.gaps and answer.gaps.get("confirmed"):
            logger.info("Confirmed gaps detected, generating follow-up")
            return True

        # Default: no follow-up
        return False

    async def _generate_followup(
        self,
        interview_id: UUID,
        parent_question: Any,
        answer: Answer,
        gaps: dict[str, Any],
        order: int,
    ) -> FollowUpQuestion:
        """Generate targeted follow-up question based on gaps.

        Args:
            interview_id: Interview ID
            parent_question: Original question
            answer: Candidate's answer
            gaps: Detected gaps
            order: Follow-up order (1, 2, or 3)

        Returns:
            Generated FollowUpQuestion
        """
        missing_concepts = gaps.get("concepts", [])
        severity = gaps.get("severity", "moderate")

        # Use port method instead of direct client access
        follow_up_text = await self.llm.generate_followup_question(
            parent_question=parent_question.text,
            answer_text=answer.text,
            missing_concepts=missing_concepts,
            severity=severity,
            order=order,
        )

        # Create FollowUpQuestion entity
        follow_up = FollowUpQuestion(
            parent_question_id=parent_question.id,
            interview_id=interview_id,
            text=follow_up_text,
            generated_reason=f"Missing concepts: {', '.join(missing_concepts[:3])}",
            order_in_sequence=order,
        )

        return follow_up

    def _count_follow_ups_for_question(self, interview: Any, question_id: UUID) -> int:
        """Count how many follow-ups already exist for a question.

        Args:
            interview: Interview entity
            question_id: Parent question ID

        Returns:
            Count of follow-ups for this question
        """
        # For MVP, count from adaptive_follow_ups list
        # TODO: In production, query FollowUpQuestion table by parent_question_id
        return len(interview.adaptive_follow_ups)
</file>

<file path="src/domain/models/answer.py">
"""Answer domain model."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class AnswerEvaluation(BaseModel):
    """Represents the evaluation of an answer.

    This is a value object containing evaluation metrics.
    """

    score: float = Field(ge=0.0, le=100.0)  # Score out of 100
    semantic_similarity: float = Field(ge=0.0, le=1.0)  # Similarity to reference
    completeness: float = Field(ge=0.0, le=1.0)  # How complete the answer is
    relevance: float = Field(ge=0.0, le=1.0)  # How relevant to the question
    sentiment: str | None = None  # e.g., "confident", "uncertain"
    reasoning: str | None = None  # AI explanation of the evaluation
    strengths: list[str] = Field(default_factory=list)
    weaknesses: list[str] = Field(default_factory=list)
    improvement_suggestions: list[str] = Field(default_factory=list)

    def is_passing(self, threshold: float = 60.0) -> bool:
        """Check if answer meets passing threshold.

        Args:
            threshold: Minimum score to pass (default: 60.0)

        Returns:
            True if score meets threshold, False otherwise
        """
        return self.score >= threshold


class Answer(BaseModel):
    """Represents a candidate's answer to a question.

    This is an entity in the interview domain.
    """

    id: UUID = Field(default_factory=uuid4)
    interview_id: UUID
    question_id: UUID
    candidate_id: UUID
    text: str  # The actual answer text
    is_voice: bool = False  # Whether answer was given via voice
    audio_file_path: str | None = None  # If voice answer
    duration_seconds: float | None = None  # Time taken to answer
    evaluation: AnswerEvaluation | None = None
    embedding: list[float] | None = None  # Vector embedding of answer
    metadata: dict[str, Any] = Field(default_factory=dict)  # Additional context

    # NEW: Adaptive evaluation fields
    similarity_score: float | None = Field(
        None, ge=0.0, le=1.0
    )  # Cosine similarity vs ideal_answer
    gaps: dict[str, Any] | None = None  # Detected concept gaps {keywords: [], entities: []}

    created_at: datetime = Field(default_factory=datetime.utcnow)
    evaluated_at: datetime | None = None

    class Config:
        """Pydantic configuration."""

        frozen = False

    def evaluate(self, evaluation: AnswerEvaluation) -> None:
        """Evaluate the answer.

        Args:
            evaluation: Evaluation results
        """
        self.evaluation = evaluation
        self.evaluated_at = datetime.utcnow()

    def is_evaluated(self) -> bool:
        """Check if answer has been evaluated.

        Returns:
            True if evaluated, False otherwise
        """
        return self.evaluation is not None

    def get_score(self) -> float | None:
        """Get the evaluation score.

        Returns:
            Score if evaluated, None otherwise
        """
        return self.evaluation.score if self.evaluation else None

    def is_complete(self) -> bool:
        """Check if answer is considered complete.

        Returns:
            True if answer has content and optional evaluation
        """
        return bool(self.text and len(self.text.strip()) > 0)

    def has_similarity_score(self) -> bool:
        """Check if similarity score is available.

        Returns:
            True if similarity_score is set
        """
        return self.similarity_score is not None

    def has_gaps(self) -> bool:
        """Check if concept gaps were detected.

        Returns:
            True if gaps dict contains concepts and confirmed is True
        """
        if self.gaps is None:
            return False
        concepts = self.gaps.get("concepts", [])
        confirmed = self.gaps.get("confirmed", False)
        return len(concepts) > 0 and confirmed

    def meets_threshold(self, similarity_threshold: float = 0.8) -> bool:
        """Check if answer meets similarity threshold.

        Args:
            similarity_threshold: Minimum similarity (default 0.8)

        Returns:
            True if similarity_score >= threshold
        """
        if not self.has_similarity_score():
            return False
        return self.similarity_score >= similarity_threshold  # type: ignore

    def is_adaptive_complete(self) -> bool:
        """Check if answer meets adaptive criteria (no follow-up needed).

        Returns:
            True if similarity >=0.8 OR no gaps detected
        """
        similarity_ok = self.similarity_score and self.similarity_score >= 0.8
        no_gaps = not self.has_gaps()

        return similarity_ok or no_gaps
</file>

<file path="src/domain/models/interview.py">
"""Interview domain model."""

from datetime import datetime
from enum import Enum
from typing import Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class InterviewStatus(str, Enum):
    """Interview status enumeration."""

    PREPARING = "preparing"  # CV analysis in progress
    READY = "ready"  # Ready to start
    IN_PROGRESS = "in_progress"  # Interview ongoing
    COMPLETED = "completed"  # Interview finished
    CANCELLED = "cancelled"  # Interview cancelled


class Interview(BaseModel):
    """Represents an interview session.

    This is the core aggregate root for the interview domain.
    It encapsulates all interview-related business logic.
    """

    id: UUID = Field(default_factory=uuid4)
    candidate_id: UUID
    status: InterviewStatus = InterviewStatus.PREPARING
    cv_analysis_id: UUID | None = None
    question_ids: list[UUID] = Field(default_factory=list)
    answer_ids: list[UUID] = Field(default_factory=list)
    current_question_index: int = 0

    # NEW: Pre-planning metadata for adaptive interviews
    plan_metadata: dict[str, Any] = Field(default_factory=dict)  # {n, generated_at, strategy}
    adaptive_follow_ups: list[UUID] = Field(default_factory=list)  # Follow-up question IDs

    started_at: datetime | None = None
    completed_at: datetime | None = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False

    def start(self) -> None:
        """Start the interview.

        Raises:
            ValueError: If interview is not ready to start
        """
        if self.status != InterviewStatus.READY:
            raise ValueError(f"Cannot start interview with status: {self.status}")

        self.status = InterviewStatus.IN_PROGRESS
        self.started_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()

    def complete(self) -> None:
        """Complete the interview.

        Raises:
            ValueError: If interview is not in progress
        """
        if self.status != InterviewStatus.IN_PROGRESS:
            raise ValueError(f"Cannot complete interview with status: {self.status}")

        self.status = InterviewStatus.COMPLETED
        self.completed_at = datetime.utcnow()
        self.updated_at = datetime.utcnow()

    def cancel(self) -> None:
        """Cancel the interview."""
        self.status = InterviewStatus.CANCELLED
        self.updated_at = datetime.utcnow()

    def mark_ready(self, cv_analysis_id: UUID) -> None:
        """Mark interview as ready after CV analysis.

        Args:
            cv_analysis_id: ID of the completed CV analysis
        """
        self.cv_analysis_id = cv_analysis_id
        self.status = InterviewStatus.READY
        self.updated_at = datetime.utcnow()

    def add_question(self, question_id: UUID) -> None:
        """Add a question to the interview.

        Args:
            question_id: ID of the question to add
        """
        self.question_ids.append(question_id)
        self.updated_at = datetime.utcnow()

    def add_answer(self, answer_id: UUID) -> None:
        """Add an answer to the interview.

        Args:
            answer_id: ID of the answer to add
        """
        self.answer_ids.append(answer_id)
        self.current_question_index += 1
        self.updated_at = datetime.utcnow()

    def has_more_questions(self) -> bool:
        """Check if there are more questions to ask.

        Returns:
            True if more questions remain, False otherwise
        """
        return self.current_question_index < len(self.question_ids)

    def get_current_question_id(self) -> UUID | None:
        """Get the current question ID.

        Returns:
            Current question ID or None if no questions remain
        """
        if self.has_more_questions():
            return self.question_ids[self.current_question_index]
        return None

    def get_progress_percentage(self) -> float:
        """Calculate interview progress percentage.

        Returns:
            Progress as a percentage (0-100)
        """
        if not self.question_ids:
            return 0.0
        return (self.current_question_index / len(self.question_ids)) * 100

    def is_active(self) -> bool:
        """Check if interview is currently active.

        Returns:
            True if interview is in progress, False otherwise
        """
        return self.status == InterviewStatus.IN_PROGRESS

    def add_adaptive_followup(self, question_id: UUID) -> None:
        """Add adaptive follow-up question to interview.

        Args:
            question_id: UUID of follow-up question

        Raises:
            ValueError: If follow-up limit exceeded (max 3 per main question)
        """
        self.adaptive_follow_ups.append(question_id)
        self.updated_at = datetime.utcnow()

    def is_planned(self) -> bool:
        """Check if interview has planning metadata.

        Returns:
            True if plan_metadata contains required keys
        """
        return "n" in self.plan_metadata and "generated_at" in self.plan_metadata

    @property
    def planned_question_count(self) -> int:
        """Get number of planned questions.

        Returns:
            Value of n from plan_metadata, or 0 if not planned
        """
        n = self.plan_metadata.get("n", 0)
        return int(n) if n is not None else 0
</file>

<file path="src/domain/ports/__init__.py">
"""Domain ports (interfaces) package."""

from .analytics_port import AnalyticsPort
from .answer_repository_port import AnswerRepositoryPort
from .candidate_repository_port import CandidateRepositoryPort
from .cv_analysis_repository_port import CVAnalysisRepositoryPort
from .cv_analyzer_port import CVAnalyzerPort
from .follow_up_question_repository_port import FollowUpQuestionRepositoryPort
from .interview_repository_port import InterviewRepositoryPort
from .llm_port import LLMPort
from .question_repository_port import QuestionRepositoryPort
from .speech_to_text_port import SpeechToTextPort
from .text_to_speech_port import TextToSpeechPort
from .vector_search_port import VectorSearchPort

__all__ = [
    "LLMPort",
    "VectorSearchPort",
    "QuestionRepositoryPort",
    "FollowUpQuestionRepositoryPort",
    "CandidateRepositoryPort",
    "InterviewRepositoryPort",
    "AnswerRepositoryPort",
    "CVAnalysisRepositoryPort",
    "CVAnalyzerPort",
    "SpeechToTextPort",
    "TextToSpeechPort",
    "AnalyticsPort",
]
</file>

<file path="src/domain/ports/llm_port.py">
"""LLM (Large Language Model) port interface."""

from abc import ABC, abstractmethod
from typing import Any
from uuid import UUID

from ..models.answer import AnswerEvaluation
from ..models.question import Question


class LLMPort(ABC):
    """Interface for Large Language Model providers.

    This port abstracts LLM interactions, allowing easy switching between
    providers like OpenAI, Claude, Llama, etc.
    """

    @abstractmethod
    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
    ) -> str:
        """Generate an interview question.

        Args:
            context: Interview context (CV analysis, previous answers, etc.)
            skill: Target skill to test
            difficulty: Question difficulty level

        Returns:
            Generated question text
        """
        pass

    @abstractmethod
    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
    ) -> AnswerEvaluation:
        """Evaluate a candidate's answer.

        Args:
            question: The question that was asked
            answer_text: Candidate's answer
            context: Additional context for evaluation

        Returns:
            Evaluation results with score and feedback
        """
        pass

    @abstractmethod
    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive feedback report.

        Args:
            interview_id: ID of the interview
            questions: All questions asked
            answers: All answers with evaluations

        Returns:
            Formatted feedback report
        """
        pass

    @abstractmethod
    async def summarize_cv(self, cv_text: str) -> str:
        """Generate a summary of a CV.

        Args:
            cv_text: Extracted CV text

        Returns:
            Summary of the CV
        """
        pass

    @abstractmethod
    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract skills from CV text using NLP.

        Args:
            text: CV text to analyze

        Returns:
            List of extracted skills with metadata
        """
        pass

    @abstractmethod
    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate ideal answer for a question.

        Args:
            question_text: The interview question
            context: CV summary, skills, etc.

        Returns:
            Ideal answer text (150-300 words)
        """
        pass

    @abstractmethod
    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate rationale explaining why answer is ideal.

        Args:
            question_text: The question
            ideal_answer: The ideal answer

        Returns:
            Rationale text (50-100 words)
        """
        pass

    @abstractmethod
    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Detect missing concepts in answer using LLM.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with keys:
                - concepts: list[str] - Missing key concepts
                - keywords: list[str] - Subset of confirmed missing keywords
                - confirmed: bool - Whether gaps are confirmed
                - severity: str - "minor" | "moderate" | "major"
        """
        pass

    @abstractmethod
    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
    ) -> str:
        """Generate targeted follow-up question.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question
            missing_concepts: List of concepts missing from answer
            severity: Gap severity ("minor" | "moderate" | "major")
            order: Follow-up order in sequence (1, 2, 3, ...)

        Returns:
            Follow-up question text
        """
        pass
</file>

<file path=".gitignore">
# Created by .ignore support plugin (hsz.mobi)
### Python template
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*,cover
.hypothesis/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# IPython Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# dotenv
.env
.env.local

# virtualenv
venv/
ENV/

# Spyder project settings
.spyderproject

# Rope project settings
.ropeproject
### VirtualEnv template
# Virtualenv
# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/
[Bb]in
[Ii]nclude
[Ll]ib
[Ll]ib64
[Ll]ocal
[Ss]cripts
pyvenv.cfg
.venv
pip-selfcheck.json

### JetBrains template
# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider
# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839

# User-specific stuff
.idea/**/workspace.xml
.idea/**/tasks.xml
.idea/**/usage.statistics.xml
.idea/**/dictionaries
.idea/**/shelf

# AWS User-specific
.idea/**/aws.xml

# Generated files
.idea/**/contentModel.xml

# Sensitive or high-churn files
.idea/**/dataSources/
.idea/**/dataSources.ids
.idea/**/dataSources.local.xml
.idea/**/sqlDataSources.xml
.idea/**/dynamic.xml
.idea/**/uiDesigner.xml
.idea/**/dbnavigator.xml

# Gradle
.idea/**/gradle.xml
.idea/**/libraries

# Gradle and Maven with auto-import
# When using Gradle or Maven with auto-import, you should exclude module files,
# since they will be recreated, and may cause churn.  Uncomment if using
# auto-import.
# .idea/artifacts
# .idea/compiler.xml
# .idea/jarRepositories.xml
# .idea/modules.xml
# .idea/*.iml
# .idea/modules
# *.iml
# *.ipr

# CMake
cmake-build-*/

# Mongo Explorer plugin
.idea/**/mongoSettings.xml

# File-based project format
*.iws

# IntelliJ
out/

# mpeltonen/sbt-idea plugin
.idea_modules/

# JIRA plugin
atlassian-ide-plugin.xml

# Cursive Clojure plugin
.idea/replstate.xml

# SonarLint plugin
.idea/sonarlint/

# Crashlytics plugin (for Android Studio and IntelliJ)
com_crashlytics_export_strings.xml
crashlytics.properties
crashlytics-build.properties
fabric.properties

# Editor-based Rest Client
.idea/httpRequests

# Android studio 3.1+ serialized cache file
.idea/caches/build_file_checksums.ser

# idea folder
.idea

# Elios-specific uploads and data
uploads/
*.pdf
*.doc
*.docx
*.wav
*.mp3

# Database files
*.db
*.sqlite3

# Pytest cache
.pytest_cache/

# MyPy cache
.mypy_cache/

.opencode/

.cursor/
</file>

<file path="README.md">
# Elios AI Interview Service

**An AI-powered mock interview platform that helps candidates prepare for technical interviews through personalized CV analysis, adaptive question generation, and real-time answer evaluation.**

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ðŸ“– Overview

Elios AI Interview Service leverages **Large Language Models (LLMs)** and **vector databases** to deliver intelligent, personalized mock interview experiences. The platform analyzes candidate CVs, generates relevant questions, evaluates answers in real-time, and provides comprehensive feedback to help candidates improve their interview performance.

### Key Features

- **ðŸŽ¯ CV Analysis**: Extract skills, experience, and education from resumes
- **ðŸ¤– Adaptive Questions**: Generate personalized interview questions based on candidate background
- **ðŸ“Š Real-Time Evaluation**: Multi-dimensional answer assessment with instant feedback
- **ðŸ’¬ Voice & Text Support**: Conduct interviews via text chat or voice (planned)
- **ðŸ“ˆ Comprehensive Reports**: Detailed performance analysis with actionable recommendations
- **ðŸ”„ Swappable AI Providers**: Easy integration of OpenAI, Claude, or Llama

### Technology Stack

- **Backend**: Python 3.11+, FastAPI, Pydantic
- **Database**: PostgreSQL (Neon), SQLAlchemy 2.0 (async)
- **AI/ML**: OpenAI GPT-4, Pinecone Vector Database
- **Architecture**: Clean Architecture (Hexagonal/Ports & Adapters)
- **Testing**: pytest, pytest-asyncio
- **Code Quality**: ruff, black, mypy

### Main flows

#### 1. Preparation Phase (Scan CV & Generate Topics)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant CVAnalyzer as ðŸ“„ CV Analyzer Component
    participant VectorDB as ðŸ§  Vector Database
    participant AIEngine as ðŸ¤– AI Interviewer Engine

    Candidate->>ChatUI: Upload CV file
    ChatUI->>CVAnalyzer: Send CV for analysis
    CVAnalyzer->>VectorDB: Generate & store CV embeddings
    VectorDB-->>CVAnalyzer: Confirm embeddings stored
    CVAnalyzer-->>ChatUI: Return extracted skills & suggested topics
    ChatUI-->>Candidate: Display preparation summary
    ChatUI->>AIEngine: Notify readiness (skills, topics)
    AIEngine-->>ChatUI: Acknowledged
```

#### 2. Interview Phase (Real-time Q&A)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant AIEngine as ðŸ¤– AI Interviewer Engine
    participant VectorDB as ðŸ§  Vector Database
    participant QBank as ðŸ“š Question Bank Service
    participant STT as ðŸŽ¤ Speech-to-Text
    participant TTS as ðŸ—£ï¸ Text-to-Speech
    participant Analytics as ðŸ“Š Analytics & Feedback Service

    %% --- Start Interview ---
    Candidate->>ChatUI: Start interview
    ChatUI->>AIEngine: Request first question
    AIEngine->>VectorDB: Query similar question embeddings (based on CV topics)
    VectorDB-->>AIEngine: Return question candidates
    AIEngine->>QBank: Fetch selected question
    QBank-->>AIEngine: Return question details
    AIEngine-->>ChatUI: Send question text
    ChatUI-->>TTS: Convert question text to speech
    TTS-->>Candidate: Play AI voice question

    %% --- Candidate answers ---
    Candidate->>STT: Speak answer
    STT-->>AIEngine: Send transcript text
    AIEngine->>VectorDB: Compare answer embeddings & evaluate quality
    VectorDB-->>AIEngine: Return similarity & semantic score
    AIEngine->>Analytics: Send answer evaluation (score, sentiment, reasoning)
    Analytics-->>AIEngine: Acknowledged

    alt More questions remain
        AIEngine->>VectorDB: Retrieve next suitable question
        VectorDB-->>AIEngine: Return next question candidate
        AIEngine-->>ChatUI: Send next question
        ChatUI-->>TTS: Convert to speech & play
        TTS-->>Candidate: Play next question
    else Interview finished
        AIEngine-->>ChatUI: Notify interview end
    end

```

#### 3. Final Stage (Evaluation & Reporting)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant AIEngine as ðŸ¤– AI Interviewer Engine
    participant Analytics as ðŸ“Š Analytics & Feedback Service

    AIEngine->>Analytics: Send final interview summary (scores, metrics, transcript)
    Analytics->>Analytics: Aggregate results & generate report
    Analytics-->>AIEngine: Acknowledged

    AIEngine-->>ChatUI: Notify interview completion
    ChatUI->>Analytics: Request final feedback report
    Analytics-->>ChatUI: Return detailed feedback & improvement suggestions
    ChatUI-->>Candidate: Display performance summary & insights

```

---

## ðŸ—ï¸ Architecture

This project follows **Clean Architecture** (Hexagonal/Ports & Adapters): Domain Layer (pure business logic) â†’ Application Layer (use cases) â†’ Adapters Layer (external services) â†’ Infrastructure Layer (config, DI).

ðŸ“š **[Full Architecture Details â†’](docs/system-architecture.md)**

---

## ðŸš€ Quick Start

### âš¡ 5-Minute Setup

**Just want to run it?** Copy and paste these commands:

```bash
# Setup environment and install dependencies
python -m venv venv && venv\Scripts\activate && pip install -e ".[dev]"

# Configure and run migrations
cp .env.example .env.local && alembic upgrade head

# Start the server
python -m src.main
```

Then visit: **http://localhost:8000/docs**

âš ï¸ **Note**: Edit `.env.local` with your API keys before full functionality works.

---

### ðŸ“‹ Detailed Setup Instructions

#### Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- PostgreSQL database (or Neon account)
- OpenAI API key
- Pinecone API key

#### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/elios/elios-ai-service.git
   cd EliosAIService
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv

   # Windows
   venv\Scripts\activate

   # Linux/macOS
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -e ".[dev]"
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env.local
   ```

   Edit `.env.local` with your credentials:
   ```env
   # Database
   DATABASE_URL=postgresql://user:password@host:5432/elios_interviews

   # LLM Provider
   LLM_PROVIDER=openai
   OPENAI_API_KEY=sk-your-api-key-here
   OPENAI_MODEL=gpt-4

   # Vector Database
   VECTOR_DB_PROVIDER=pinecone
   PINECONE_API_KEY=your-pinecone-api-key
   PINECONE_INDEX_NAME=elios-questions
   ```

5. **Run database migrations**
   ```bash
   alembic upgrade head
   ```

6. **Verify database setup**
   ```bash
   python scripts/verify_db.py
   ```

7. **Start the server**
   ```bash
   python -m src.main
   ```

   Server runs at: http://localhost:8000

   API Documentation: http://localhost:8000/docs

---

## ðŸ“– Documentation

### For Users
- **[Project Overview & PDR](docs/project-overview-pdr.md)** - Product requirements, features, and roadmap
- **[Database Setup Guide](DATABASE_SETUP.md)** - Comprehensive database configuration
- **[Environment Setup Guide](ENV_SETUP.md)** - Environment configuration best practices

### For Developers
- **[System Architecture](docs/system-architecture.md)** - Detailed architecture documentation
- **[Codebase Summary](docs/codebase-summary.md)** - Project structure and tech stack
- **[Code Standards](docs/code-standards.md)** - Coding conventions and best practices
- **[CLAUDE.md](CLAUDE.md)** - Development guidelines for AI assistants

---

## ðŸ§ª Development

### Mock Adapters for Testing

**Mock adapters** simulate external services without API costs or network latency. Enabled by default in development.

**Available Mocks** (6 total):
- `MockLLMAdapter` - Simulates OpenAI/LLM responses
- `MockVectorSearchAdapter` - In-memory vector search
- `MockSTTAdapter` - Simulates speech-to-text
- `MockTTSAdapter` - Simulates text-to-speech
- `MockCVAnalyzerAdapter` - Filename-based CV parsing
- `MockAnalyticsAdapter` - In-memory performance tracking

**Configuration**:
```env
# .env.local
USE_MOCK_ADAPTERS=true   # Use mocks (default, fast tests)
USE_MOCK_ADAPTERS=false  # Use real services (requires API keys)
```

**Benefits**:
- Tests run 10x faster (~5s vs ~30s)
- No API costs during development
- No network dependency
- Deterministic test results

**Note**: Repositories (PostgreSQL) intentionally NOT mocked - use real database for data integrity tests.

### Running Tests

```bash
# Run all tests (with mocks enabled by default)
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test types
pytest tests/unit/         # Unit tests only
pytest tests/integration/  # Integration tests only
pytest tests/e2e/          # End-to-end tests only

# Test with real adapters (requires API keys)
USE_MOCK_ADAPTERS=false pytest
```

### Code Quality

```bash
# Format code
black src/

# Lint code
ruff check src/
ruff check --fix src/  # Auto-fix issues

# Type checking
mypy src/

# Run all checks
black src/ && ruff check src/ && mypy src/
```

### Database Operations

```bash
# Create new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback one migration
alembic downgrade -1

# View migration history
alembic history

# Verify database
python scripts/verify_db.py
```

---

## ðŸŽ¯ Usage Example

### 1. Create a Candidate

```python
import httpx

async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8000/api/candidates",
        json={
            "name": "John Doe",
            "email": "john.doe@example.com"
        }
    )
    candidate = response.json()
    print(f"Created candidate: {candidate['id']}")
```

### 2. Upload and Analyze CV

```python
async with httpx.AsyncClient() as client:
    with open("resume.pdf", "rb") as cv_file:
        response = await client.post(
            "http://localhost:8000/api/cv/upload",
            files={"file": cv_file},
            data={"candidate_id": candidate['id']}
        )
    cv_analysis = response.json()
    print(f"Skills found: {cv_analysis['skills']}")
```

### 3. Start Interview

```python
async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8000/api/interviews",
        json={
            "candidate_id": candidate['id'],
            "cv_analysis_id": cv_analysis['id']
        }
    )
    interview = response.json()
    print(f"Interview ready with {len(interview['question_ids'])} questions")
```

### 4. Submit Answer

```python
async with httpx.AsyncClient() as client:
    response = await client.post(
        f"http://localhost:8000/api/interviews/{interview['id']}/answers",
        json={
            "question_id": interview['question_ids'][0],
            "answer_text": "My answer here..."
        }
    )
    evaluation = response.json()
    print(f"Score: {evaluation['score']}/100")
    print(f"Feedback: {evaluation['feedback']}")
```

---

## ðŸ“¦ Project Structure

```
EliosAIService/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ domain/              # Core business logic (5 models, 11 ports)
â”‚   â”œâ”€â”€ application/         # Use cases
â”‚   â”œâ”€â”€ adapters/            # External service implementations
â”‚   â””â”€â”€ infrastructure/      # Config, DI, database
â”œâ”€â”€ alembic/                 # Database migrations
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ tests/                   # Test suites
```

ðŸ“š **[Complete Structure â†’](docs/codebase-summary.md)**

---

## ðŸ”§ Configuration

Configuration is managed through environment variables with the following priority:

1. `.env.local` (highest priority, gitignored)
2. `.env` (can be committed, template)
3. System environment variables
4. Pydantic defaults

### Key Configuration Sections

- **Application**: Name, version, environment
- **LLM Provider**: OpenAI, Claude, or Llama configuration
- **Vector Database**: Pinecone, Weaviate, or ChromaDB settings
- **PostgreSQL**: Database connection and credentials
- **Speech Services**: Azure STT, Edge TTS (planned)
- **Interview Settings**: Question count, scoring, timeouts

See [ENV_SETUP.md](ENV_SETUP.md) for detailed configuration guide.

---

## ðŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines (coming soon).

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following our [Code Standards](docs/code-standards.md)
4. Run tests and quality checks
5. Commit using [Conventional Commits](https://www.conventionalcommits.org/)
6. Push to your branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Commit Message Format

```
<type>(<scope>): <subject>

Examples:
feat(domain): add Interview aggregate with state management
fix(persistence): handle NULL metadata in answer mapper
docs: update API documentation for CV upload endpoint
```

---

## ðŸ—ºï¸ Roadmap

### Phase 1: Foundation (Current - v0.1.0)
- âœ… Domain models and ports
- âœ… PostgreSQL persistence layer
- âœ… OpenAI LLM adapter
- âœ… Pinecone vector adapter
- âœ… Database migrations
- ðŸ”„ REST API implementation
- ðŸ”„ CV processing adapters

### Phase 2: Core Features (v0.2.0 - v0.5.0)
- â³ Voice interview support
- â³ Advanced question generation
- â³ Interview analytics
- â³ Performance benchmarks
- â³ Frontend integration

### Phase 3: Intelligence Enhancement (v0.6.0 - v0.8.0)
- â³ Multi-LLM support (Claude, Llama)
- â³ Behavioral question analysis
- â³ Personality insights
- â³ Skill gap analysis

### Phase 4: Scale & Polish (v0.9.0 - v1.0.0)
- â³ Multi-language support
- â³ Team/organization features
- â³ Mobile app support
- â³ Production deployment

See [Project Overview & PDR](docs/project-overview-pdr.md) for detailed roadmap.

---

## ðŸ“Š Current Status

**Version**: 0.1.0 (Foundation Phase)

**Implemented**:
- âœ… Clean Architecture structure
- âœ… Domain models (5 entities)
- âœ… Repository ports (5 interfaces)
- âœ… PostgreSQL persistence (5 repositories)
- âœ… OpenAI LLM adapter
- âœ… Pinecone vector adapter
- âœ… Async SQLAlchemy 2.0 with Alembic
- âœ… Configuration management
- âœ… Dependency injection container
- âœ… Use cases (AnalyzeCV, StartInterview)
- âœ… Health check API endpoint

**In Progress**:
- ðŸ”„ Complete REST API
- ðŸ”„ CV processing adapters
- ðŸ”„ WebSocket chat handler

**Planned**:
- â³ Authentication & authorization
- â³ Comprehensive testing
- â³ API documentation
- â³ Docker deployment

---

## ðŸ›¡ï¸ Security

- API keys stored in environment variables (never committed)
- SQL injection prevention via parameterized queries
- Input validation with Pydantic
- HTTPS enforcement (production)
- Data encryption at rest (Neon built-in)
- GDPR compliance considerations

Report security vulnerabilities to: security@elios.ai

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ðŸ™ Acknowledgments

- **OpenAI** for GPT-4 and Embeddings API
- **Pinecone** for vector database
- **FastAPI** for the excellent web framework
- **Neon** for serverless PostgreSQL
- **Pydantic** for data validation
- **SQLAlchemy** for ORM

---

## ðŸ“ž Contact

- **Website**: https://elios.ai
- **Email**: contact@elios.ai
- **Issues**: [GitHub Issues](https://github.com/elios/elios-ai-service/issues)
- **Discussions**: [GitHub Discussions](https://github.com/elios/elios-ai-service/discussions)

---

## â­ Support

If you find this project helpful, please consider giving it a star on GitHub! It helps others discover the project and motivates continued development.

---

**Built with â¤ï¸ using Clean Architecture principles**
</file>

<file path="src/adapters/api/rest/interview_routes.py">
"""Interview REST API endpoints."""

from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from ....application.dto.interview_dto import (
    InterviewResponse,
    PlanInterviewRequest,
    PlanningStatusResponse,
    QuestionResponse,
)
from ....application.use_cases.get_next_question import GetNextQuestionUseCase
from ....application.use_cases.plan_interview import PlanInterviewUseCase
from ....domain.models.interview import InterviewStatus
from ....infrastructure.config.settings import get_settings
from ....infrastructure.database.session import get_async_session
from ....infrastructure.dependency_injection.container import get_container

router = APIRouter(prefix="/interviews", tags=["Interviews"])


@router.get(
    "/{interview_id}",
    response_model=InterviewResponse,
    summary="Get interview details",
)
async def get_interview(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get interview by ID.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Interview details

    Raises:
        HTTPException: If interview not found
    """
    container = get_container()
    settings = get_settings()

    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    base_url = settings.ws_base_url
    return InterviewResponse.from_domain(interview, base_url)


@router.put(
    "/{interview_id}/start",
    response_model=InterviewResponse,
    summary="Start interview (move to IN_PROGRESS)",
)
async def start_interview(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Start interview session.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Updated interview details

    Raises:
        HTTPException: If interview not found or invalid state
    """
    container = get_container()
    settings = get_settings()

    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    try:
        interview.start()
        updated = await interview_repo.update(interview)

        base_url = settings.ws_base_url
        return InterviewResponse.from_domain(updated, base_url)
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


@router.get(
    "/{interview_id}/questions/current",
    response_model=QuestionResponse,
    summary="Get current question",
)
async def get_current_question(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get current unanswered question.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Current question details

    Raises:
        HTTPException: If interview not found or no more questions
    """
    container = get_container()

    use_case = GetNextQuestionUseCase(
        interview_repository=container.interview_repository_port(session),
        question_repository=container.question_repository_port(session),
    )

    try:
        question = await use_case.execute(interview_id)
        if not question:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No more questions available",
            )

        # Get interview for context
        interview = await container.interview_repository_port(
            session
        ).get_by_id(interview_id)

        if not interview:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Interview {interview_id} not found",
            )

        return QuestionResponse(
            id=question.id,
            text=question.text,
            question_type=question.question_type.value,
            difficulty=question.difficulty.value,
            index=interview.current_question_index,
            total=len(interview.question_ids),
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


# NEW: Adaptive Planning Endpoints
@router.post(
    "/plan",
    response_model=PlanningStatusResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Plan interview with adaptive questions",
)
async def plan_interview(
    request: PlanInterviewRequest,
    session: AsyncSession = Depends(get_async_session),
):
    """Plan interview by generating n questions with ideal answers.

    This endpoint triggers the pre-planning phase:
    1. Calculates n based on skill diversity (max 5)
    2. Generates n questions with ideal_answer + rationale
    3. Returns interview with status=PREPARING (async process)

    Args:
        request: Planning request with cv_analysis_id and candidate_id
        session: Database session

    Returns:
        Planning status with interview_id

    Raises:
        HTTPException: If CV analysis not found
    """
    try:
        container = get_container()

        # Validate CV analysis exists
        cv_analysis_repo = container.cv_analysis_repository_port(session)
        cv_analysis = await cv_analysis_repo.get_by_id(request.cv_analysis_id)
        if not cv_analysis:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"CV analysis {request.cv_analysis_id} not found",
            )

        # Execute planning use case
        use_case = PlanInterviewUseCase(
            llm=container.llm_port(),
            cv_analysis_repo=cv_analysis_repo,
            interview_repo=container.interview_repository_port(session),
            question_repo=container.question_repository_port(session),
        )

        interview = await use_case.execute(
            cv_analysis_id=request.cv_analysis_id,
            candidate_id=request.candidate_id,
        )

        return PlanningStatusResponse(
            interview_id=interview.id,
            status=interview.status.value,
            planned_question_count=interview.planned_question_count,
            plan_metadata=interview.plan_metadata,
            message=f"Interview planned with {interview.planned_question_count} questions",
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


@router.get(
    "/{interview_id}/plan",
    response_model=PlanningStatusResponse,
    summary="Get interview planning status",
)
async def get_planning_status(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get interview planning status.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Planning status details

    Raises:
        HTTPException: If interview not found
    """
    container = get_container()
    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    # Determine message based on status
    if interview.status == InterviewStatus.PREPARING:
        message = "Interview planning in progress..."
    elif interview.status == InterviewStatus.READY:
        message = f"Interview ready with {interview.planned_question_count} questions"
    elif interview.status == InterviewStatus.IN_PROGRESS:
        message = "Interview started"
    elif interview.status == InterviewStatus.COMPLETED:
        message = "Interview completed"
    else:
        message = f"Interview status: {interview.status.value}"

    return PlanningStatusResponse(
        interview_id=interview.id,
        status=interview.status.value,
        planned_question_count=interview.planned_question_count,
        plan_metadata=interview.plan_metadata,
        message=message,
    )
</file>

<file path="src/adapters/llm/openai_adapter.py">
"""OpenAI LLM adapter implementation."""

import json
from typing import Any
from uuid import UUID

from openai import AsyncOpenAI

from ...domain.models.answer import AnswerEvaluation
from ...domain.models.question import Question
from ...domain.ports.llm_port import LLMPort


class OpenAIAdapter(LLMPort):
    """OpenAI implementation of LLM port.

    This adapter encapsulates all OpenAI-specific logic, making it easy
    to swap for another LLM provider without touching domain logic.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4",
        temperature: float = 0.7,
    ):
        """Initialize OpenAI adapter.

        Args:
            api_key: OpenAI API key
            model: Model to use (default: gpt-4)
            temperature: Sampling temperature (default: 0.7)
        """
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature

    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
    ) -> str:
        """Generate an interview question using OpenAI.

        Args:
            context: Interview context
            skill: Target skill to test
            difficulty: Question difficulty level

        Returns:
            Generated question text
        """
        system_prompt = """You are an expert technical interviewer.
        Generate a clear, relevant interview question based on the context provided."""

        user_prompt = f"""
        Generate a {difficulty} difficulty interview question to test: {skill}

        Context:
        - Candidate's background: {context.get('cv_summary', 'Not provided')}
        - Previous topics covered: {context.get('covered_topics', [])}
        - Interview stage: {context.get('stage', 'early')}

        Return only the question text, no additional explanation.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self.temperature,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
    ) -> AnswerEvaluation:
        """Evaluate an answer using OpenAI.

        Args:
            question: The question that was asked
            answer_text: Candidate's answer
            context: Additional context

        Returns:
            Evaluation results
        """
        system_prompt = """You are an expert technical interviewer evaluating candidate answers.
        Provide objective, constructive feedback with specific scores."""

        user_prompt = f"""
        Question: {question.text}
        Question Type: {question.question_type}
        Difficulty: {question.difficulty}
        Expected Skills: {', '.join(question.skills)}

        Candidate's Answer: {answer_text}

        {"Ideal Answer: " + question.ideal_answer if question.ideal_answer else ""}

        Evaluate this answer and provide:
        1. Overall score (0-100)
        2. Completeness score (0-1)
        3. Relevance score (0-1)
        4. Sentiment (confident/uncertain/nervous)
        5. 2-3 strengths
        6. 2-3 weaknesses
        7. 2-3 improvement suggestions
        8. Brief reasoning for the score

        Return as JSON with keys: score, completeness, relevance, sentiment, strengths, weaknesses, improvements, reasoning
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Lower temperature for more consistent evaluation
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)

        return AnswerEvaluation(
            score=float(result.get("score", 0)),
            semantic_similarity=0.0,  # Will be calculated by vector search
            completeness=float(result.get("completeness", 0)),
            relevance=float(result.get("relevance", 0)),
            sentiment=result.get("sentiment"),
            reasoning=result.get("reasoning"),
            strengths=result.get("strengths", []),
            weaknesses=result.get("weaknesses", []),
            improvement_suggestions=result.get("improvements", []),
        )

    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive feedback report.

        Args:
            interview_id: ID of the interview
            questions: All questions asked
            answers: All answers with evaluations

        Returns:
            Formatted feedback report
        """
        system_prompt = """You are an expert career coach providing comprehensive interview feedback.
        Create a detailed, actionable report that helps candidates improve."""

        # Prepare interview summary
        qa_pairs = []
        for i, (q, a) in enumerate(zip(questions, answers)):
            qa_pairs.append(
                f"Q{i+1}: {q.text}\n"
                f"Answer Score: {a.get('evaluation', {}).get('score', 'N/A')}\n"
                f"Evaluation: {a.get('evaluation', {}).get('reasoning', 'N/A')}\n"
            )

        user_prompt = f"""
        Generate a comprehensive interview feedback report for interview {interview_id}.

        Interview Performance:
        {chr(10).join(qa_pairs)}

        Include:
        1. Overall Performance Summary
        2. Key Strengths (with examples)
        3. Areas for Improvement (with specific guidance)
        4. Skill-by-Skill Breakdown
        5. Actionable Next Steps

        Be encouraging but honest. Provide specific examples and actionable advice.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
        )

        content = response.choices[0].message.content
        return content or ""

    async def summarize_cv(self, cv_text: str) -> str:
        """Generate a summary of a CV.

        Args:
            cv_text: Extracted CV text

        Returns:
            Summary of the CV
        """
        system_prompt = """You are an expert recruiter analyzing candidate CVs.
        Create concise, informative summaries."""

        user_prompt = f"""
        Summarize this CV in 3-4 sentences, highlighting:
        - Key technical skills and experience
        - Years of experience and seniority level
        - Notable projects or achievements

        CV:
        {cv_text[:2000]}  # Limit to first 2000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.5,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract skills from CV text using OpenAI.

        Args:
            text: CV text to analyze

        Returns:
            List of extracted skills with metadata
        """
        system_prompt = """You are an expert at extracting structured information from CVs.
        Identify technical skills, soft skills, and tools mentioned."""

        user_prompt = f"""
        Extract all skills from this CV text. For each skill, identify:
        - name: The skill name
        - category: "technical", "soft", or "language"
        - proficiency: "beginner", "intermediate", or "expert" (infer from context)

        Return as JSON array with keys: name, category, proficiency

        CV Text:
        {text[:3000]}  # Limit to first 3000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)
        skills: list[dict[str, str]] = result.get("skills", [])
        return skills

    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate ideal answer for a question.

        Args:
            question_text: The interview question
            context: CV summary, skills, etc.

        Returns:
            Ideal answer text (150-300 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer creating reference answers.
        Generate comprehensive, technically accurate ideal answers."""

        user_prompt = f"""
        Question: {question_text}

        Candidate Background:
        - Summary: {context.get('summary', 'Not provided')}
        - Key Skills: {', '.join(context.get('skills', [])[:5])}
        - Experience: {context.get('experience', 'Not specified')} years

        Generate an ideal answer for this interview question. The answer should:
        - Be 150-300 words
        - Demonstrate expert-level understanding
        - Cover key concepts comprehensively
        - Include practical examples if relevant
        - Be technically accurate

        Output only the ideal answer text.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Low for consistency
            max_tokens=500,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate rationale explaining why answer is ideal.

        Args:
            question_text: The question
            ideal_answer: The ideal answer

        Returns:
            Rationale text (50-100 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer explaining evaluation criteria.
        Explain why an answer demonstrates mastery."""

        user_prompt = f"""
        Question: {question_text}
        Ideal Answer: {ideal_answer}

        Explain WHY this is an ideal answer in 50-100 words. Focus on:
        - What key concepts are covered
        - Why this demonstrates mastery
        - What would be missing in a weaker answer

        Output only the rationale text.
        """

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",  # Cheaper model for rationale
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=200,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Detect concept gaps using OpenAI with JSON mode.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with concept gap analysis
        """
        prompt = f"""
Question: {question_text}
Ideal Answer: {ideal_answer}
Candidate Answer: {answer_text}
Potential missing keywords: {', '.join(keyword_gaps[:10])}

Analyze and identify:
1. Key concepts in ideal answer missing from candidate answer
2. Whether missing keywords represent real conceptual gaps

Return as JSON:
- "concepts": list of missing concepts
- "confirmed": boolean
- "severity": "minor" | "moderate" | "major"
"""

        system_prompt = """You are an expert technical interviewer analyzing completeness.
Identify real conceptual gaps, not just missing synonyms."""

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)

        return {
            "concepts": result.get("concepts", []),
            "keywords": keyword_gaps[:5],
            "confirmed": result.get("confirmed", False),
            "severity": result.get("severity", "minor"),
        }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
    ) -> str:
        """Generate follow-up question using OpenAI.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question
            missing_concepts: List of concepts missing from answer
            severity: Gap severity
            order: Follow-up order in sequence

        Returns:
            Follow-up question text
        """
        prompt = f"""
Original Question: {parent_question}
Candidate's Answer: {answer_text}
Missing Concepts: {', '.join(missing_concepts)}
Gap Severity: {severity}

Generate focused follow-up question addressing missing concepts.
The question should:
- Be specific and concise
- Help candidate demonstrate understanding of: {', '.join(missing_concepts[:2])}
- Be appropriate for follow-up #{order}

Return only the question text.
"""

        system_prompt = """You are an expert technical interviewer generating follow-ups.
Ask questions that probe specific missing concepts."""

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_tokens=150,
        )

        content = response.choices[0].message.content
        return content.strip() if content else "Can you elaborate on that?"
</file>

<file path="src/application/use_cases/__init__.py">
"""Use cases package."""

from .analyze_cv import AnalyzeCVUseCase
from .plan_interview import PlanInterviewUseCase
from .process_answer_adaptive import ProcessAnswerAdaptiveUseCase

__all__ = [
    "AnalyzeCVUseCase",
    "PlanInterviewUseCase",
    "ProcessAnswerAdaptiveUseCase",
]
</file>

<file path="src/domain/models/question.py">
"""Question domain model."""

from datetime import datetime
from enum import Enum
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class QuestionType(str, Enum):
    """Question type enumeration."""

    TECHNICAL = "technical"
    BEHAVIORAL = "behavioral"
    SITUATIONAL = "situational"


class DifficultyLevel(str, Enum):
    """Question difficulty level."""

    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class Question(BaseModel):
    """Represents an interview question.

    Questions are value objects in the interview domain.
    They contain metadata for semantic search and categorization.
    """

    id: UUID = Field(default_factory=uuid4)
    text: str
    question_type: QuestionType
    difficulty: DifficultyLevel
    skills: list[str] = Field(default_factory=list)  # e.g., ["Python", "OOP"]
    tags: list[str] = Field(default_factory=list)  # e.g., ["algorithms", "data-structures"]
    evaluation_criteria: str | None = None
    version: int = 1
    embedding: list[float] | None = None  # Vector embedding for semantic search

    # Pre-planning fields for adaptive interviews
    ideal_answer: str | None = None  # Reference answer for similarity scoring and evaluation
    rationale: str | None = None  # Explanation of why this answer is ideal

    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        pass

    def has_skill(self, skill: str) -> bool:
        """Check if question tests a specific skill.

        Args:
            skill: Skill name to check

        Returns:
            True if skill is tested, False otherwise
        """
        return skill.lower() in [s.lower() for s in self.skills]

    def has_tag(self, tag: str) -> bool:
        """Check if question has a specific tag.

        Args:
            tag: Tag to check

        Returns:
            True if tag exists, False otherwise
        """
        return tag.lower() in [t.lower() for t in self.tags]

    def is_suitable_for_difficulty(self, max_difficulty: DifficultyLevel) -> bool:
        """Check if question difficulty is appropriate.

        Args:
            max_difficulty: Maximum allowed difficulty

        Returns:
            True if suitable, False otherwise
        """
        difficulty_order = {
            DifficultyLevel.EASY: 1,
            DifficultyLevel.MEDIUM: 2,
            DifficultyLevel.HARD: 3,
        }
        return difficulty_order[self.difficulty] <= difficulty_order[max_difficulty]

    def has_ideal_answer(self) -> bool:
        """Check if question has ideal answer for similarity scoring.

        Returns:
            True if ideal_answer is present and non-empty
        """
        return self.ideal_answer is not None and len(self.ideal_answer.strip()) > 10

    @property
    def is_planned(self) -> bool:
        """Check if question is part of pre-planned interview.

        Returns:
            True if has ideal_answer and rationale
        """
        return self.has_ideal_answer() and self.rationale is not None
</file>

<file path="src/infrastructure/config/settings.py">
"""Application settings using Pydantic."""

import os
import re
from functools import lru_cache
from pathlib import Path

from dotenv import load_dotenv, find_dotenv
from pydantic_settings import BaseSettings, SettingsConfigDict

# Load environment variables from .env file
# env_path = find_dotenv()
# print(f"âœ… .env file found: {env_path if env_path else 'None'}")
#
# load_dotenv(env_path)


class Settings(BaseSettings):
    """Application settings loaded from environment variables.

    This uses Pydantic for validation and type safety.
    """

    # Application
    app_name: str = "Elios AI Interview Service"
    app_version: str = "0.1.0"
    environment: str = "development"  # development, staging, production
    debug: bool = True

    # API Configuration
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_prefix: str = "/api"

    # LLM Provider Selection
    llm_provider: str = "openai"  # openai, claude, llama

    # OpenAI Configuration
    openai_api_key: str | None = None
    openai_model: str = "gpt-4"
    openai_temperature: float = 0.7

    # Anthropic Claude Configuration (alternative)
    anthropic_api_key: str | None = None
    anthropic_model: str = "claude-3-sonnet-20240229"

    # Vector Database Selection
    vector_db_provider: str = "pinecone"  # pinecone, weaviate, chroma

    # Pinecone Configuration
    pinecone_api_key: str | None = None
    pinecone_environment: str = "us-east-1"
    pinecone_index_name: str = "elios-interviews"

    # PostgreSQL Configuration
    postgres_host: str = "localhost"
    postgres_port: int = 5432
    postgres_user: str = "elios"
    postgres_password: str = ""
    postgres_db: str = "elios_interviews"
    database_url: str | None = None  # Full DATABASE_URL from environment

    @property
    def async_database_url(self) -> str:
        """Generate async PostgreSQL connection URL.

        Converts postgresql:// to postgresql+asyncpg:// for async support.
        Strips out sslmode and channel_binding parameters (not supported by asyncpg).
        If DATABASE_URL is set in environment, use that; otherwise construct from parts.

        Note: For Neon and other cloud PostgreSQL providers, asyncpg handles SSL
        automatically - no explicit SSL parameters needed.
        """
        # First check if DATABASE_URL is provided directly
        db_url = self.database_url or os.getenv("DATABASE_URL")

        if db_url:
            # Convert postgresql:// to postgresql+asyncpg://
            db_url = re.sub(r'^postgresql:', 'postgresql+asyncpg:', db_url)

            # Strip out SSL parameters that asyncpg doesn't support in URL format
            # asyncpg handles SSL automatically for cloud providers like Neon
            db_url = re.sub(r'\?sslmode=[^&]*', '', db_url)  # Remove sslmode param
            db_url = re.sub(r'&sslmode=[^&]*', '', db_url)   # Remove if not first param
            db_url = re.sub(r'\?channel_binding=[^&]*', '', db_url)  # Remove channel_binding
            db_url = re.sub(r'&channel_binding=[^&]*', '', db_url)   # Remove if not first param
            db_url = re.sub(r'\?&', '?', db_url)  # Clean up malformed query string
            db_url = re.sub(r'\?$', '', db_url)   # Remove trailing ?

            return db_url

        # Otherwise construct from individual parts
        return (
            f"postgresql+asyncpg://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
        )

    # Speech Services
    azure_speech_key: str | None = None
    azure_speech_region: str = "eastus"

    # File Storage
    upload_dir: str = "./uploads"
    cv_dir: str = "./uploads/cvs"
    audio_dir: str = "./uploads/audio"

    # Interview Configuration
    max_questions_per_interview: int = 10
    min_passing_score: float = 60.0
    question_timeout_seconds: int = 300  # 5 minutes per question

    # Logging
    log_level: str = "INFO"
    log_format: str = "json"  # json or text

    # CORS
    cors_origins: list[str] = ["http://localhost:3000", "http://localhost:5173"]

    # WebSocket Configuration
    ws_host: str = "localhost"
    ws_port: int = 8000
    ws_base_url: str = "ws://localhost:8000"

    # Mock Adapters (for development/testing)
    use_mock_adapters: bool = True  # Set to False to use real adapters

    model_config = SettingsConfigDict(
        env_file=("../.env.local", ".env"),  # Try .env.local first, fallback to .env
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )

    def print_loaded_env_file(self):
        for env_file in self.model_config["env_file"]:
            if Path(env_file).exists():
                print(f"Found {env_file} (will be used if values not already set)")
        print(f"Active environment: {self.environment}")

    def is_production(self) -> bool:
        """Check if running in production."""
        return self.environment == "production"

    def is_development(self) -> bool:
        """Check if running in development."""
        return self.environment == "development"


@lru_cache
def get_settings() -> Settings:
    """Get cached settings instance.

    Returns:
        Settings instance
    """

    settings = Settings()
    settings.print_loaded_env_file()

    return settings
</file>

<file path="src/infrastructure/dependency_injection/container.py">
"""Dependency injection container.

This module wires up all dependencies and provides them to the application.
It's the only place that knows about concrete implementations.
"""

from functools import lru_cache

from sqlalchemy.ext.asyncio import AsyncSession

# Import adapters
from ...adapters.llm.openai_adapter import OpenAIAdapter

# Import mock adapters
from ...adapters.mock import (
    MockAnalyticsAdapter,
    MockCVAnalyzerAdapter,
    MockLLMAdapter,
    MockSTTAdapter,
    MockTTSAdapter,
    MockVectorSearchAdapter,
)

# Import persistence adapters
from ...adapters.persistence import (
    PostgreSQLAnswerRepository,
    PostgreSQLCandidateRepository,
    PostgreSQLCVAnalysisRepository,
    PostgreSQLFollowUpQuestionRepository,
    PostgreSQLInterviewRepository,
    PostgreSQLQuestionRepository,
)
from ...adapters.vector_db.pinecone_adapter import PineconeAdapter
from ...domain.ports import (
    AnalyticsPort,
    AnswerRepositoryPort,
    CandidateRepositoryPort,
    CVAnalysisRepositoryPort,
    CVAnalyzerPort,
    FollowUpQuestionRepositoryPort,
    InterviewRepositoryPort,
    LLMPort,
    QuestionRepositoryPort,
    SpeechToTextPort,
    TextToSpeechPort,
    VectorSearchPort,
)
from ...infrastructure.config.settings import Settings, get_settings


class Container:
    """Dependency injection container.

    This class is responsible for creating and managing all dependencies.
    It follows the dependency inversion principle by depending on ports
    (interfaces) while providing concrete implementations.
    """

    def __init__(self, settings: Settings):
        """Initialize container with settings.

        Args:
            settings: Application settings
        """
        self.settings = settings
        self._llm_port: LLMPort | None = None
        self._vector_search_port: VectorSearchPort | None = None
        self._stt_port: SpeechToTextPort | None = None
        self._tts_port: TextToSpeechPort | None = None

    def llm_port(self) -> LLMPort:
        """Get LLM port implementation.

        Returns:
            Configured LLM port based on settings

        Raises:
            ValueError: If LLM provider is not supported or not configured
        """
        if self._llm_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_adapters:
                self._llm_port = MockLLMAdapter()
            elif self.settings.llm_provider == "openai":
                if not self.settings.openai_api_key:
                    raise ValueError("OpenAI API key not configured")

                self._llm_port = OpenAIAdapter(
                    api_key=self.settings.openai_api_key,
                    model=self.settings.openai_model,
                    temperature=self.settings.openai_temperature,
                )
            elif self.settings.llm_provider == "claude":
                if not self.settings.anthropic_api_key:
                    raise ValueError("Anthropic API key not configured")

                # Import Claude adapter when implemented
                # from ...adapters.llm.claude_adapter import ClaudeAdapter
                # self._llm_port = ClaudeAdapter(
                #     api_key=self.settings.anthropic_api_key,
                #     model=self.settings.anthropic_model,
                # )
                raise NotImplementedError("Claude adapter not yet implemented")
            else:
                raise ValueError(f"Unsupported LLM provider: {self.settings.llm_provider}")

        return self._llm_port

    def vector_search_port(self) -> VectorSearchPort:
        """Get vector search port implementation.

        Returns:
            Configured vector search port based on settings

        Raises:
            ValueError: If vector DB provider is not supported or not configured
        """
        if self._vector_search_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_adapters:
                self._vector_search_port = MockVectorSearchAdapter()
            elif self.settings.vector_db_provider == "pinecone":
                if not self.settings.pinecone_api_key:
                    raise ValueError("Pinecone API key not configured")
                if not self.settings.openai_api_key:
                    raise ValueError("OpenAI API key required for embeddings")

                self._vector_search_port = PineconeAdapter(
                    api_key=self.settings.pinecone_api_key,
                    environment=self.settings.pinecone_environment,
                    index_name=self.settings.pinecone_index_name,
                    openai_api_key=self.settings.openai_api_key,
                )
            elif self.settings.vector_db_provider == "weaviate":
                # Import Weaviate adapter when implemented
                # from ...adapters.vector_db.weaviate_adapter import WeaviateAdapter
                # self._vector_search_port = WeaviateAdapter(...)
                raise NotImplementedError("Weaviate adapter not yet implemented")
            elif self.settings.vector_db_provider == "chroma":
                # Import ChromaDB adapter when implemented
                # from ...adapters.vector_db.chroma_adapter import ChromaAdapter
                # self._vector_search_port = ChromaAdapter(...)
                raise NotImplementedError("ChromaDB adapter not yet implemented")
            else:
                raise ValueError(
                    f"Unsupported vector DB provider: {self.settings.vector_db_provider}"
                )

        return self._vector_search_port

    def question_repository_port(self, session: AsyncSession) -> QuestionRepositoryPort:
        """Get question repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured question repository
        """
        return PostgreSQLQuestionRepository(session)

    def follow_up_question_repository(
        self, session: AsyncSession
    ) -> FollowUpQuestionRepositoryPort:
        """Get follow-up question repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured follow-up question repository
        """
        return PostgreSQLFollowUpQuestionRepository(session)

    def candidate_repository_port(self, session: AsyncSession) -> CandidateRepositoryPort:
        """Get candidate repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured candidate repository
        """
        return PostgreSQLCandidateRepository(session)

    def interview_repository_port(self, session: AsyncSession) -> InterviewRepositoryPort:
        """Get interview repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured interview repository
        """
        return PostgreSQLInterviewRepository(session)

    def answer_repository_port(self, session: AsyncSession) -> AnswerRepositoryPort:
        """Get answer repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured answer repository
        """
        return PostgreSQLAnswerRepository(session)

    def cv_analysis_repository_port(
        self, session: AsyncSession
    ) -> CVAnalysisRepositoryPort:
        """Get CV analysis repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured CV analysis repository
        """
        return PostgreSQLCVAnalysisRepository(session)

    def cv_analyzer_port(self) -> CVAnalyzerPort:
        """Get CV analyzer port implementation.

        Returns:
            Configured CV analyzer

        Raises:
            NotImplementedError: Real implementation pending
        """
        if self.settings.use_mock_adapters:
            return MockCVAnalyzerAdapter()
        else:
            # TODO: Implement real CV analyzer
            # from ...adapters.cv_processing.spacy_cv_analyzer import SpacyCVAnalyzer
            # return SpacyCVAnalyzer(llm_port=self.llm_port())
            raise NotImplementedError("Real CV analyzer not yet implemented")

    def speech_to_text_port(self) -> SpeechToTextPort:
        """Get speech-to-text port implementation.

        Returns:
            Configured STT service

        Raises:
            NotImplementedError: Implementation pending
        """
        if self._stt_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_adapters:
                self._stt_port = MockSTTAdapter()
            else:
                # TODO: Implement Azure STT adapter
                # from ...adapters.speech.azure_stt_adapter import AzureSTTAdapter
                # self._stt_port = AzureSTTAdapter(
                #     api_key=self.settings.azure_speech_key,
                #     region=self.settings.azure_speech_region,
                # )
                raise NotImplementedError("Speech-to-text adapter not yet implemented")

        return self._stt_port

    def text_to_speech_port(self) -> TextToSpeechPort:
        """Get text-to-speech port implementation.

        Returns:
            Configured TTS service

        Raises:
            NotImplementedError: Implementation pending
        """
        if self._tts_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_adapters:
                self._tts_port = MockTTSAdapter()
            else:
                # TODO: Implement Edge TTS adapter
                # from ...adapters.speech.edge_tts_adapter import EdgeTTSAdapter
                # self._tts_port = EdgeTTSAdapter()
                raise NotImplementedError("Text-to-speech adapter not yet implemented")

        return self._tts_port

    def analytics_port(self) -> AnalyticsPort:
        """Get analytics port implementation.

        Returns:
            Configured analytics service

        Raises:
            NotImplementedError: Real implementation pending
        """
        if self.settings.use_mock_adapters:
            return MockAnalyticsAdapter()
        else:
            # TODO: Implement real analytics service
            # from ...adapters.analytics.analytics_adapter import AnalyticsAdapter
            # return AnalyticsAdapter(database_url=self.settings.database_url)
            raise NotImplementedError("Real analytics adapter not yet implemented")


@lru_cache
def get_container() -> Container:
    """Get cached container instance.

    Returns:
        Container instance with all dependencies configured
    """
    settings = get_settings()
    return Container(settings)
</file>

<file path="src/adapters/persistence/mappers.py">
"""Mappers to convert between domain models and SQLAlchemy models.

These mappers handle the translation between the domain layer
(Pydantic models) and the persistence layer (SQLAlchemy models).
"""


from ...domain.models.answer import Answer, AnswerEvaluation
from ...domain.models.candidate import Candidate
from ...domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from ...domain.models.follow_up_question import FollowUpQuestion
from ...domain.models.interview import Interview, InterviewStatus
from ...domain.models.question import DifficultyLevel, Question, QuestionType
from .models import (
    AnswerModel,
    CandidateModel,
    CVAnalysisModel,
    FollowUpQuestionModel,
    InterviewModel,
    QuestionModel,
)


class CandidateMapper:
    """Mapper for Candidate domain model and CandidateModel database model."""

    @staticmethod
    def to_domain(db_model: CandidateModel) -> Candidate:
        """Convert database model to domain model.

        Args:
            db_model: SQLAlchemy model instance

        Returns:
            Domain model instance
        """
        return Candidate(
            id=db_model.id,
            name=db_model.name,
            email=db_model.email,
            cv_file_path=db_model.cv_file_path,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Candidate) -> CandidateModel:
        """Convert domain model to database model.

        Args:
            domain_model: Domain model instance

        Returns:
            SQLAlchemy model instance
        """
        return CandidateModel(
            id=domain_model.id,
            name=domain_model.name,
            email=domain_model.email,
            cv_file_path=domain_model.cv_file_path,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: CandidateModel, domain_model: Candidate) -> None:
        """Update database model from domain model.

        Args:
            db_model: SQLAlchemy model to update
            domain_model: Domain model with new data
        """
        db_model.name = domain_model.name
        db_model.email = domain_model.email
        db_model.cv_file_path = domain_model.cv_file_path
        db_model.updated_at = domain_model.updated_at


class QuestionMapper:
    """Mapper for Question domain model and QuestionModel database model."""

    @staticmethod
    def to_domain(db_model: QuestionModel) -> Question:
        """Convert database model to domain model."""
        return Question(
            id=db_model.id,
            text=db_model.text,
            question_type=QuestionType(db_model.question_type),
            difficulty=DifficultyLevel(db_model.difficulty),
            skills=list(db_model.skills) if db_model.skills else [],
            tags=list(db_model.tags) if db_model.tags else [],
            evaluation_criteria=db_model.evaluation_criteria,
            version=db_model.version,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            ideal_answer=db_model.ideal_answer,
            rationale=db_model.rationale,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Question) -> QuestionModel:
        """Convert domain model to database model."""
        return QuestionModel(
            id=domain_model.id,
            text=domain_model.text,
            question_type=domain_model.question_type.value,
            difficulty=domain_model.difficulty.value,
            skills=domain_model.skills,
            tags=domain_model.tags,
            evaluation_criteria=domain_model.evaluation_criteria,
            version=domain_model.version,
            embedding=domain_model.embedding,
            ideal_answer=domain_model.ideal_answer,
            rationale=domain_model.rationale,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: QuestionModel, domain_model: Question) -> None:
        """Update database model from domain model."""
        db_model.text = domain_model.text
        db_model.question_type = domain_model.question_type.value
        db_model.difficulty = domain_model.difficulty.value
        db_model.skills = domain_model.skills
        db_model.tags = domain_model.tags
        db_model.evaluation_criteria = domain_model.evaluation_criteria
        db_model.version = domain_model.version
        db_model.embedding = domain_model.embedding
        db_model.ideal_answer = domain_model.ideal_answer
        db_model.rationale = domain_model.rationale
        db_model.updated_at = domain_model.updated_at


class InterviewMapper:
    """Mapper for Interview domain model and InterviewModel database model."""

    @staticmethod
    def to_domain(db_model: InterviewModel) -> Interview:
        """Convert database model to domain model."""
        return Interview(
            id=db_model.id,
            candidate_id=db_model.candidate_id,
            status=InterviewStatus(db_model.status),
            cv_analysis_id=db_model.cv_analysis_id,
            question_ids=list(db_model.question_ids) if db_model.question_ids else [],
            answer_ids=list(db_model.answer_ids) if db_model.answer_ids else [],
            current_question_index=db_model.current_question_index,
            plan_metadata=dict(db_model.plan_metadata) if db_model.plan_metadata else {},
            adaptive_follow_ups=list(db_model.adaptive_follow_ups) if db_model.adaptive_follow_ups else [],
            started_at=db_model.started_at,
            completed_at=db_model.completed_at,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Interview) -> InterviewModel:
        """Convert domain model to database model."""
        return InterviewModel(
            id=domain_model.id,
            candidate_id=domain_model.candidate_id,
            status=domain_model.status.value,
            cv_analysis_id=domain_model.cv_analysis_id,
            question_ids=domain_model.question_ids,
            answer_ids=domain_model.answer_ids,
            current_question_index=domain_model.current_question_index,
            plan_metadata=domain_model.plan_metadata,
            adaptive_follow_ups=domain_model.adaptive_follow_ups,
            started_at=domain_model.started_at,
            completed_at=domain_model.completed_at,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: InterviewModel, domain_model: Interview) -> None:
        """Update database model from domain model."""
        db_model.status = domain_model.status.value
        db_model.cv_analysis_id = domain_model.cv_analysis_id
        db_model.question_ids = domain_model.question_ids
        db_model.answer_ids = domain_model.answer_ids
        db_model.current_question_index = domain_model.current_question_index
        db_model.plan_metadata = domain_model.plan_metadata
        db_model.adaptive_follow_ups = domain_model.adaptive_follow_ups
        db_model.started_at = domain_model.started_at
        db_model.completed_at = domain_model.completed_at
        db_model.updated_at = domain_model.updated_at


class AnswerMapper:
    """Mapper for Answer domain model and AnswerModel database model."""

    @staticmethod
    def to_domain(db_model: AnswerModel) -> Answer:
        """Convert database model to domain model."""
        # Convert evaluation JSONB to AnswerEvaluation if present
        evaluation = None
        if db_model.evaluation:
            evaluation = AnswerEvaluation(**db_model.evaluation)

        return Answer(
            id=db_model.id,
            interview_id=db_model.interview_id,
            question_id=db_model.question_id,
            candidate_id=db_model.candidate_id,
            text=db_model.text,
            is_voice=db_model.is_voice,
            audio_file_path=db_model.audio_file_path,
            duration_seconds=db_model.duration_seconds,
            evaluation=evaluation,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            metadata=dict(db_model.answer_metadata) if db_model.answer_metadata else {},
            similarity_score=db_model.similarity_score,
            gaps=dict(db_model.gaps) if db_model.gaps else None,
            created_at=db_model.created_at,
            evaluated_at=db_model.evaluated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Answer) -> AnswerModel:
        """Convert domain model to database model."""
        # Convert AnswerEvaluation to dict for JSONB storage
        evaluation_dict = None
        if domain_model.evaluation:
            evaluation_dict = domain_model.evaluation.model_dump()

        return AnswerModel(
            id=domain_model.id,
            interview_id=domain_model.interview_id,
            question_id=domain_model.question_id,
            candidate_id=domain_model.candidate_id,
            text=domain_model.text,
            is_voice=domain_model.is_voice,
            audio_file_path=domain_model.audio_file_path,
            duration_seconds=domain_model.duration_seconds,
            evaluation=evaluation_dict,
            embedding=domain_model.embedding,
            answer_metadata=domain_model.metadata,
            similarity_score=domain_model.similarity_score,
            gaps=domain_model.gaps,
            created_at=domain_model.created_at,
            evaluated_at=domain_model.evaluated_at,
        )

    @staticmethod
    def update_db_model(db_model: AnswerModel, domain_model: Answer) -> None:
        """Update database model from domain model."""
        db_model.text = domain_model.text
        db_model.is_voice = domain_model.is_voice
        db_model.audio_file_path = domain_model.audio_file_path
        db_model.duration_seconds = domain_model.duration_seconds

        # Convert evaluation to dict if present
        if domain_model.evaluation:
            db_model.evaluation = domain_model.evaluation.model_dump()
        else:
            db_model.evaluation = None

        db_model.embedding = domain_model.embedding
        db_model.answer_metadata = domain_model.metadata
        db_model.similarity_score = domain_model.similarity_score
        db_model.gaps = domain_model.gaps
        db_model.evaluated_at = domain_model.evaluated_at


class CVAnalysisMapper:
    """Mapper for CVAnalysis domain model and CVAnalysisModel database model."""

    @staticmethod
    def to_domain(db_model: CVAnalysisModel) -> CVAnalysis:
        """Convert database model to domain model."""
        # Convert skills from JSONB to ExtractedSkill objects
        skills = []
        if db_model.skills:
            skills = [ExtractedSkill(**skill_dict) for skill_dict in db_model.skills]

        return CVAnalysis(
            id=db_model.id,
            candidate_id=db_model.candidate_id,
            cv_file_path=db_model.cv_file_path,
            extracted_text=db_model.extracted_text,
            skills=skills,
            work_experience_years=db_model.work_experience_years,
            education_level=db_model.education_level,
            suggested_topics=(
                list(db_model.suggested_topics) if db_model.suggested_topics else []
            ),
            suggested_difficulty=db_model.suggested_difficulty,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            summary=db_model.summary,
            metadata=dict(db_model.cv_metadata) if db_model.cv_metadata else {},
            created_at=db_model.created_at,
        )

    @staticmethod
    def to_db_model(domain_model: CVAnalysis) -> CVAnalysisModel:
        """Convert domain model to database model."""
        # Convert ExtractedSkill objects to dicts for JSONB storage
        skills_dicts = [skill.model_dump() for skill in domain_model.skills]

        return CVAnalysisModel(
            id=domain_model.id,
            candidate_id=domain_model.candidate_id,
            cv_file_path=domain_model.cv_file_path,
            extracted_text=domain_model.extracted_text,
            skills=skills_dicts,
            work_experience_years=domain_model.work_experience_years,
            education_level=domain_model.education_level,
            suggested_topics=domain_model.suggested_topics,
            suggested_difficulty=domain_model.suggested_difficulty,
            embedding=domain_model.embedding,
            summary=domain_model.summary,
            cv_metadata=domain_model.metadata,
            created_at=domain_model.created_at,
        )

    @staticmethod
    def update_db_model(db_model: CVAnalysisModel, domain_model: CVAnalysis) -> None:
        """Update database model from domain model."""
        db_model.cv_file_path = domain_model.cv_file_path
        db_model.extracted_text = domain_model.extracted_text
        db_model.skills = [skill.model_dump() for skill in domain_model.skills]
        db_model.work_experience_years = domain_model.work_experience_years
        db_model.education_level = domain_model.education_level
        db_model.suggested_topics = domain_model.suggested_topics
        db_model.suggested_difficulty = domain_model.suggested_difficulty
        db_model.embedding = domain_model.embedding
        db_model.summary = domain_model.summary
        db_model.cv_metadata = domain_model.metadata


class FollowUpQuestionMapper:
    """Mapper for FollowUpQuestion domain model and FollowUpQuestionModel database model."""

    @staticmethod
    def to_domain(db_model: FollowUpQuestionModel) -> FollowUpQuestion:
        """Convert database model to domain model.

        Args:
            db_model: SQLAlchemy model instance

        Returns:
            FollowUpQuestion domain model
        """
        return FollowUpQuestion(
            id=db_model.id,
            parent_question_id=db_model.parent_question_id,
            interview_id=db_model.interview_id,
            text=db_model.text,
            generated_reason=db_model.generated_reason,
            order_in_sequence=db_model.order_in_sequence,
            created_at=db_model.created_at,
        )

    @staticmethod
    def to_db_model(domain_model: FollowUpQuestion) -> FollowUpQuestionModel:
        """Convert domain model to database model.

        Args:
            domain_model: FollowUpQuestion domain model

        Returns:
            FollowUpQuestionModel SQLAlchemy model
        """
        return FollowUpQuestionModel(
            id=domain_model.id,
            parent_question_id=domain_model.parent_question_id,
            interview_id=domain_model.interview_id,
            text=domain_model.text,
            generated_reason=domain_model.generated_reason,
            order_in_sequence=domain_model.order_in_sequence,
            created_at=domain_model.created_at,
        )

    @staticmethod
    def update_db_model(
        db_model: FollowUpQuestionModel, domain_model: FollowUpQuestion
    ) -> None:
        """Update database model from domain model.

        Args:
            db_model: SQLAlchemy model to update
            domain_model: FollowUpQuestion domain model with new data
        """
        db_model.text = domain_model.text
        db_model.generated_reason = domain_model.generated_reason
        db_model.order_in_sequence = domain_model.order_in_sequence
</file>

<file path="src/adapters/api/websocket/interview_handler.py">
"""WebSocket handler for interview sessions."""

import base64
import logging
from uuid import UUID

from fastapi import WebSocket, WebSocketDisconnect

from ....application.use_cases.complete_interview import CompleteInterviewUseCase
from ....application.use_cases.get_next_question import GetNextQuestionUseCase
from ....application.use_cases.process_answer_adaptive import (
    ProcessAnswerAdaptiveUseCase,
)
from ....infrastructure.database.session import get_async_session
from ....infrastructure.dependency_injection.container import get_container
from .connection_manager import manager

logger = logging.getLogger(__name__)


async def handle_interview_websocket(
    websocket: WebSocket,
    interview_id: UUID,
):
    """WebSocket handler for interview session.

    Protocol:
        Client â†’ Server: { type: "text_answer", question_id: UUID, answer_text: str }
        Server â†’ Client: { type: "evaluation", ... }
        Server â†’ Client: { type: "question", ... }
        Server â†’ Client: { type: "interview_complete", ... }

    Args:
        websocket: WebSocket connection
        interview_id: Interview UUID
    """
    # Connect
    await manager.connect(interview_id, websocket)

    try:
        container = get_container()

        # Send first question
        async for session in get_async_session():
            interview_repo = container.interview_repository_port(session)
            question_repo = container.question_repository_port(session)
            tts = container.text_to_speech_port()

            await _send_initial_question(
                interview_id, interview_repo, question_repo, tts
            )
            break

        # Listen for messages
        while True:
            data = await websocket.receive_json()
            message_type = data.get("type")

            if message_type == "text_answer":
                await handle_text_answer(interview_id, data, container)

            elif message_type == "audio_chunk":
                await handle_audio_chunk(interview_id, data, container)

            elif message_type == "get_next_question":
                await handle_get_next_question(interview_id, container)

            else:
                await manager.send_message(
                    interview_id,
                    {
                        "type": "error",
                        "code": "UNKNOWN_MESSAGE_TYPE",
                        "message": f"Unknown message type: {message_type}",
                    },
                )

    except WebSocketDisconnect:
        manager.disconnect(interview_id)
        logger.info(f"Client disconnected from interview {interview_id}")

    except Exception as e:
        logger.error(
            f"WebSocket error for interview {interview_id}: {e}", exc_info=True
        )
        await manager.send_message(
            interview_id,
            {"type": "error", "code": "INTERNAL_ERROR", "message": str(e)},
        )
        manager.disconnect(interview_id)


async def _validate_interview_exists(interview_id: UUID, interview_repo):
    """Validate that interview exists and send error if not.

    Args:
        interview_id: Interview UUID
        interview_repo: Interview repository

    Returns:
        Interview entity if exists, None otherwise
    """
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        await manager.send_message(
            interview_id,
            {
                "type": "error",
                "code": "INTERVIEW_NOT_FOUND",
                "message": f"Interview {interview_id} not found",
            },
        )
        return None

    return interview


async def _send_initial_question(
    interview_id: UUID, interview_repo, question_repo, tts
):
    """Get and send the first question to start the interview.

    Args:
        interview_id: Interview UUID
        interview_repo: Interview repository
        question_repo: Question repository
        tts: Text-to-speech port

    Returns:
        True if question was sent, False otherwise
    """
    use_case = GetNextQuestionUseCase(
        interview_repository=interview_repo,
        question_repository=question_repo,
    )

    question = await use_case.execute(interview_id)
    if not question:
        return False

    # Get interview for context
    interview = await _validate_interview_exists(interview_id, interview_repo)
    if not interview:
        return False

    # Generate TTS audio
    audio_bytes = await tts.synthesize_speech(question.text)
    audio_data = base64.b64encode(audio_bytes).decode("utf-8")

    # Send question message
    await manager.send_message(
        interview_id,
        {
            "type": "question",
            "question_id": str(question.id),
            "text": question.text,
            "question_type": question.question_type,
            "difficulty": question.difficulty,
            "index": interview.current_question_index,
            "total": len(interview.question_ids),
            "audio_data": audio_data,
        },
    )
    return True


async def _process_answer(
    interview_id: UUID,
    question_id: UUID,
    answer_text: str,
    answer_repo,
    interview_repo,
    question_repo,
    container,
):
    """Process answer with adaptive evaluation.

    Args:
        interview_id: Interview UUID
        question_id: Question UUID
        answer_text: Candidate's answer text
        answer_repo: Answer repository
        interview_repo: Interview repository
        question_repo: Question repository
        container: DI container

    Returns:
        Tuple of (answer, follow_up_question, has_more)
    """
    use_case = ProcessAnswerAdaptiveUseCase(
        answer_repository=answer_repo,
        interview_repository=interview_repo,
        question_repository=question_repo,
        follow_up_question_repository=container.follow_up_question_repository(),
        llm=container.llm_port(),
        vector_search=container.vector_search_port(),
    )

    return await use_case.execute(
        interview_id=interview_id,
        question_id=question_id,
        answer_text=answer_text,
    )


async def _send_evaluation(interview_id: UUID, answer):
    """Send evaluation message with adaptive metrics.

    Args:
        interview_id: Interview UUID
        answer: Answer entity with evaluation
    """
    eval_message = {
        "type": "evaluation",
        "answer_id": str(answer.id),
        "score": answer.evaluation.score,
        "feedback": answer.evaluation.reasoning,
        "strengths": answer.evaluation.strengths,
        "weaknesses": answer.evaluation.weaknesses,
        "similarity_score": answer.similarity_score,
        "gaps": answer.gaps,
    }

    await manager.send_message(interview_id, eval_message)


async def _send_follow_up_question(interview_id: UUID, follow_up_question, tts):
    """Send follow-up question with audio.

    Args:
        interview_id: Interview UUID
        follow_up_question: Follow-up question entity
        tts: Text-to-speech port
    """
    audio_bytes = await tts.synthesize_speech(follow_up_question.text)
    audio_data = base64.b64encode(audio_bytes).decode("utf-8")

    await manager.send_message(
        interview_id,
        {
            "type": "follow_up_question",
            "question_id": str(follow_up_question.id),
            "parent_question_id": str(follow_up_question.parent_question_id),
            "text": follow_up_question.text,
            "generated_reason": follow_up_question.generated_reason,
            "order_in_sequence": follow_up_question.order_in_sequence,
            "audio_data": audio_data,
        },
    )


async def _send_next_question(
    interview_id: UUID, interview, interview_repo, question_repo, tts
):
    """Send next main question with audio.

    Args:
        interview_id: Interview UUID
        interview: Interview entity
        interview_repo: Interview repository
        question_repo: Question repository
        tts: Text-to-speech port
    """
    question_use_case = GetNextQuestionUseCase(
        interview_repository=interview_repo,
        question_repository=question_repo,
    )
    question = await question_use_case.execute(interview_id)

    if question:
        audio_bytes = await tts.synthesize_speech(question.text)
        audio_data = base64.b64encode(audio_bytes).decode("utf-8")

        await manager.send_message(
            interview_id,
            {
                "type": "question",
                "question_id": str(question.id),
                "text": question.text,
                "question_type": question.question_type,
                "difficulty": question.difficulty,
                "index": interview.current_question_index,
                "total": len(interview.question_ids),
                "audio_data": audio_data,
            },
        )


async def _complete_interview(interview_id: UUID, interview_repo, answer_repo):
    """Complete interview and send final results.

    Args:
        interview_id: Interview UUID
        interview_repo: Interview repository
        answer_repo: Answer repository
    """
    complete_use_case = CompleteInterviewUseCase(
        interview_repository=interview_repo,
    )
    interview = await complete_use_case.execute(interview_id)

    # Calculate overall score (average of all answer scores)
    answers = await answer_repo.get_by_interview_id(interview_id)
    overall_score = (
        sum(a.evaluation.score for a in answers if a.evaluation) / len(answers)
        if answers
        else 0.0
    )

    await manager.send_message(
        interview_id,
        {
            "type": "interview_complete",
            "interview_id": str(interview.id),
            "overall_score": overall_score,
            "total_questions": len(interview.question_ids),
            "feedback_url": f"/api/interviews/{interview_id}/feedback",
        },
    )


async def handle_text_answer(interview_id: UUID, data: dict, container):
    """Handle text answer from client with adaptive evaluation.

    Args:
        interview_id: Interview UUID
        data: Message data with question_id and answer_text
        container: DI container
    """
    async for session in get_async_session():
        # Fetch repositories once
        interview_repo = container.interview_repository_port(session)
        question_repo = container.question_repository_port(session)
        answer_repo = container.answer_repository_port(session)

        # Validate interview exists
        interview = await _validate_interview_exists(interview_id, interview_repo)
        if not interview:
            break

        # Process answer with adaptive evaluation
        answer, follow_up_question, has_more = await _process_answer(
            interview_id=interview_id,
            question_id=UUID(data["question_id"]),
            answer_text=data["answer_text"],
            answer_repo=answer_repo,
            interview_repo=interview_repo,
            question_repo=question_repo,
            container=container,
        )

        # Send evaluation
        await _send_evaluation(interview_id, answer)

        # Get TTS once (will be used for either follow-up or next question)
        tts = container.text_to_speech_port()

        # If follow-up generated, send it immediately
        if follow_up_question:
            await _send_follow_up_question(interview_id, follow_up_question, tts)
            # Don't proceed to next main question yet
            break

        # Send next question or complete
        if has_more:
            await _send_next_question(
                interview_id, interview, interview_repo, question_repo, tts
            )
        else:
            await _complete_interview(interview_id, interview_repo, answer_repo)

        break


async def handle_audio_chunk(interview_id: UUID, data: dict, container):
    """Handle audio chunk from client (for voice answers).

    Args:
        interview_id: Interview UUID
        data: Message data with audio chunk
        container: DI container
    """
    # Mock implementation for now
    await manager.send_message(
        interview_id,
        {
            "type": "transcription",
            "text": "[Mock transcription of audio]",
            "is_final": data.get("is_final", False),
        },
    )


async def handle_get_next_question(interview_id: UUID, container):
    """Handle request for next question.

    Args:
        interview_id: Interview UUID
        container: DI container
    """
    async for session in get_async_session():
        interview_repo = container.interview_repository_port(session)
        question_repo = container.question_repository_port(session)
        tts = container.text_to_speech_port()

        # Get next question
        use_case = GetNextQuestionUseCase(
            interview_repository=interview_repo,
            question_repository=question_repo,
        )
        question = await use_case.execute(interview_id)

        if not question:
            await manager.send_message(
                interview_id,
                {
                    "type": "error",
                    "code": "NO_MORE_QUESTIONS",
                    "message": "No more questions available",
                },
            )
            break

        # Validate interview exists
        interview = await _validate_interview_exists(interview_id, interview_repo)
        if not interview:
            break

        # Send question with audio
        await _send_next_question(
            interview_id, interview, interview_repo, question_repo, tts
        )
        break
</file>

</files>
