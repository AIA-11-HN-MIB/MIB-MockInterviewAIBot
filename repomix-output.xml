This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/.env.example
.env.example
.gitignore
.repomixignore
251108-reference-answer-removal-test-report.md
251108-status-value-error-investigation-report.md
251108-test-rerun-after-fixes-report.md
alembic.ini
alembic/env.py
alembic/README
alembic/script.py.mako
alembic/versions/0001_create_tables.py
alembic/versions/0002_insert_seed_data.py
alembic/versions/0003_create_evaluations_tables.py
CHANGELOG_ENV.md
CLAUDE.md
DATABASE_SETUP.md
ENV_SETUP.md
nul
pyproject.toml
pytest.ini
quickstart.bat
README.md
requirements/base.txt
requirements/dev.txt
requirements/prod.txt
src/__init__.py
src/adapters/__init__.py
src/adapters/api/__init__.py
src/adapters/api/rest/__init__.py
src/adapters/api/rest/health_routes.py
src/adapters/api/rest/interview_routes.py
src/adapters/api/websocket/__init__.py
src/adapters/api/websocket/connection_manager.py
src/adapters/api/websocket/interview_handler.py
src/adapters/api/websocket/session_orchestrator.py
src/adapters/cv_processing/__init__.py
src/adapters/cv_processing/cv_processing_adapter.py
src/adapters/llm/__init__.py
src/adapters/llm/azure_openai_adapter.py
src/adapters/llm/openai_adapter.py
src/adapters/mock/__init__.py
src/adapters/mock/mock_analytics.py
src/adapters/mock/mock_cv_analyzer.py
src/adapters/mock/mock_llm_adapter.py
src/adapters/mock/mock_stt_adapter.py
src/adapters/mock/mock_tts_adapter.py
src/adapters/mock/mock_vector_search_adapter.py
src/adapters/persistence/__init__.py
src/adapters/persistence/answer_repository.py
src/adapters/persistence/candidate_repository.py
src/adapters/persistence/cv_analysis_repository.py
src/adapters/persistence/evaluation_repository.py
src/adapters/persistence/follow_up_question_repository.py
src/adapters/persistence/interview_repository.py
src/adapters/persistence/mappers.py
src/adapters/persistence/models.py
src/adapters/persistence/question_repository.py
src/adapters/speech/__init__.py
src/adapters/speech/azure_stt_adapter.py
src/adapters/speech/azure_tts_adapter.py
src/adapters/vector_db/__init__.py
src/adapters/vector_db/chroma_adapter.py
src/adapters/vector_db/pinecone_adapter.py
src/application/__init__.py
src/application/dto/answer_dto.py
src/application/dto/audio_dto.py
src/application/dto/interview_dto.py
src/application/dto/websocket_dto.py
src/application/use_cases/__init__.py
src/application/use_cases/analyze_cv.py
src/application/use_cases/combine_evaluation.py
src/application/use_cases/complete_interview.py
src/application/use_cases/follow_up_decision.py
src/application/use_cases/generate_summary.py
src/application/use_cases/get_next_question.py
src/application/use_cases/plan_interview.py
src/application/use_cases/process_answer_adaptive.py
src/domain/__init__.py
src/domain/models/__init__.py
src/domain/models/answer.py
src/domain/models/candidate.py
src/domain/models/cv_analysis.py
src/domain/models/error_codes.py
src/domain/models/evaluation.py
src/domain/models/follow_up_question.py
src/domain/models/interview.py
src/domain/models/question.py
src/domain/ports/__init__.py
src/domain/ports/analytics_port.py
src/domain/ports/answer_repository_port.py
src/domain/ports/candidate_repository_port.py
src/domain/ports/cv_analysis_repository_port.py
src/domain/ports/cv_analyzer_port.py
src/domain/ports/evaluation_repository_port.py
src/domain/ports/follow_up_question_repository_port.py
src/domain/ports/interview_repository_port.py
src/domain/ports/llm_port.py
src/domain/ports/question_repository_port.py
src/domain/ports/speech_to_text_port.py
src/domain/ports/text_to_speech_port.py
src/domain/ports/vector_search_port.py
src/domain/services/__init__.py
src/infrastructure/__init__.py
src/infrastructure/config/__init__.py
src/infrastructure/config/settings.py
src/infrastructure/database/__init__.py
src/infrastructure/database/base.py
src/infrastructure/database/session.py
src/infrastructure/dependency_injection/__init__.py
src/infrastructure/dependency_injection/container.py
src/main.py
test_basic.py
tests/__init__.py
tests/conftest.py
tests/integration/api/test_planning_endpoints.py
tests/integration/test_interview_flow_orchestrator.py
tests/unit/adapters/api/websocket/test_session_orchestrator.py
tests/unit/adapters/test_mock_analytics.py
tests/unit/adapters/test_mock_cv_analyzer.py
tests/unit/application/use_cases/test_follow_up_decision.py
tests/unit/domain/test_adaptive_models.py
tests/unit/domain/test_interview_state_transitions.py
tests/unit/use_cases/test_complete_interview.py
tests/unit/use_cases/test_generate_summary.py
tests/unit/use_cases/test_plan_interview.py
tests/unit/use_cases/test_process_answer_adaptive.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="nul">
dir: cannot access '/a:h': No such file or directory
</file>

<file path=".claude/.env.example">
# Claude Code - Project-Level Environment Variables
# Priority: process.env > .claude/.env > .claude/hooks/.env
# Copy this file to .env and set your API keys and configuration

# ============================================
# Claude Code Notification Hooks
# ============================================
# Discord Webhook URL (for Discord notifications)
# Get from: Server Settings â†’ Integrations â†’ Webhooks â†’ New Webhook
DISCORD_WEBHOOK_URL=

# Telegram Bot Token (for Telegram notifications)
# Get from: @BotFather in Telegram
TELEGRAM_BOT_TOKEN=

# Telegram Chat ID (your chat ID or group ID)
# Get from: https://api.telegram.org/bot<BOT_TOKEN>/getUpdates
TELEGRAM_CHAT_ID=

# ============================================
# Google Gemini API Key (for Gemini skills)
# ============================================
# Get your key at: https://aistudio.google.com/apikey
GEMINI_API_KEY=your-gemini-api-key-here

# ==== Vertex AI (Optional) ====
# Uncomment to use Vertex AI instead of AI Studio
# GEMINI_USE_VERTEX=true
# VERTEX_PROJECT_ID=your-gcp-project-id
# VERTEX_LOCATION=us-central1

# Add other API keys and configuration as needed
</file>

<file path=".repomixignore">
docs/*
plans/*
assets/*
dist/*
coverage/*
build/*
ios/*
android/*

.claude/*
.serena/*
.pnpm-store/*
.github/*
.dart_tool/*
.idea/*
</file>

<file path="251108-reference-answer-removal-test-report.md">
# Test Report: reference_answer Column Removal

**Date**: 2025-11-08
**Tester**: QA Agent
**Test Scope**: Remove redundant `reference_answer` column, use `ideal_answer` instead

---

## Executive Summary

**Status**: âš ï¸ PARTIAL SUCCESS - Migration successful, but mapper and test issues found

### Key Metrics
- **Tests Run**: 87 total
- **Tests Passed**: 67 (77%)
- **Tests Failed**: 9 (10%)
- **Tests Error**: 11 (13%)
- **Coverage**: 27% overall (domain layer tested)

### Critical Issues Found
1. **AnswerMapper missing fields** - `similarity_score` and `gaps` not mapped (TYPE ERROR)
2. **ExtractedSkill alias issue** - Tests using wrong field name `name` instead of `skill`
3. **Answer.has_gaps() logic** - Returns True for empty dict, should check `confirmed` field
4. **Test assertion errors** - 2 domain model tests failing due to gap detection logic

---

## 1. Migration Status

### âœ… Database Migration: SUCCESS

**Migration Applied**: `251108_1200_drop_reference_answer_column.py`

```
INFO  [alembic.runtime.migration] Running upgrade 251106_2300 -> 251108_1200
```

**Current Migration**: `251108_1200` (head)

**Migration Chain**:
```
a4047ce5a909 (initial)
  â†’ 525593eca676 (seed)
    â†’ 251106_2300 (add planning fields)
      â†’ d0078872a49a (seed planning data)
      â†’ 251108_1200 (drop reference_answer) âœ…
```

**Schema Verification**:
```python
QuestionModel fields:
  - created_at: DATETIME
  - difficulty: VARCHAR(50)
  - embedding: ARRAY
  - evaluation_criteria: TEXT
  - id: UUID
  - ideal_answer: TEXT          âœ… Present
  - question_type: VARCHAR(50)
  - rationale: TEXT             âœ… Present
  - skills: ARRAY
  - tags: ARRAY
  - text: TEXT
  - updated_at: DATETIME
  - version: INTEGER

Has reference_answer: False     âœ… Removed
Has ideal_answer: True          âœ…
Has rationale: True             âœ…
```

---

## 2. Code Changes Verification

### âœ… Domain Model (src/domain/models/question.py)

**Status**: CORRECT

```python
# Removed reference_answer field
# Kept ideal_answer and rationale

ideal_answer: str | None = None
rationale: str | None = None
```

**Methods using ideal_answer**:
- `has_ideal_answer()` - checks if ideal_answer exists and has 10+ chars
- `is_planned` property - returns True if has ideal_answer + rationale

### âœ… Database Model (src/adapters/persistence/models.py)

**Status**: CORRECT

```python
# QuestionModel - No reference_answer column
ideal_answer: Mapped[str | None] = mapped_column(Text, nullable=True)  âœ…
rationale: Mapped[str | None] = mapped_column(Text, nullable=True)     âœ…
```

### âœ… QuestionMapper (src/adapters/persistence/mappers.py)

**Status**: CORRECT

All three mapper methods properly handle `ideal_answer` and `rationale`:
- âœ… `to_domain()` - maps ideal_answer, rationale
- âœ… `to_db_model()` - maps ideal_answer, rationale
- âœ… `update_db_model()` - maps ideal_answer, rationale

### âŒ AnswerMapper (src/adapters/persistence/mappers.py)

**Status**: INCOMPLETE - TYPE ERROR

**Issue**: Missing `similarity_score` and `gaps` fields in all mapper methods

**Mypy Error**:
```
src\adapters\persistence\mappers.py:195: error: Missing named argument "similarity_score" for "Answer"  [call-arg]
```

**Root Cause**: Answer domain model has required adaptive fields but mapper doesn't include them:

```python
# Domain Model (Answer)
similarity_score: float | None = Field(None, ge=0.0, le=1.0)
gaps: dict[str, Any] | None = None

# Database Model (AnswerModel)
similarity_score: Mapped[float | None] = mapped_column(Float, nullable=True)  âœ…
gaps: Mapped[dict | None] = mapped_column(JSONB, nullable=True)                âœ…

# Mapper - MISSING in to_domain(), to_db_model(), update_db_model()
# Lines 195-251 need to add these fields
```

**Fix Required**: Add to all three methods in AnswerMapper:
1. `to_domain()` - add `similarity_score=db_model.similarity_score, gaps=dict(db_model.gaps) if db_model.gaps else None`
2. `to_db_model()` - add `similarity_score=domain_model.similarity_score, gaps=domain_model.gaps`
3. `update_db_model()` - add `db_model.similarity_score = domain_model.similarity_score; db_model.gaps = domain_model.gaps`

### âœ… OpenAI Adapter (src/adapters/llm/openai_adapter.py)

**Status**: CORRECT

```python
# Line 107 - using ideal_answer
{"Ideal Answer: " + question.ideal_answer if question.ideal_answer else ""}

# Line 276 - generate_ideal_answer method exists
# Line 330 - ideal_answer parameter used
# Line 349 - ideal_answer in prompt
```

---

## 3. Test Results

### 3.1 Import Tests

**Status**: âœ… PASS

```bash
Import check: OK
```

All modified modules import successfully without errors.

### 3.2 Type Checking (mypy)

**Status**: âš ï¸ WARNINGS + 1 CRITICAL ERROR

**Critical Error**:
```
src\adapters\persistence\mappers.py:195: error: Missing named argument "similarity_score" for "Answer"  [call-arg]
```

**Other Type Warnings** (pre-existing, not related to this change):
- Missing type parameters for generic `dict` types
- Missing return type annotations
- These don't block functionality but should be addressed

### 3.3 Unit Tests

#### âœ… Passing Tests (67 total)

**Integration Tests** (14/14 passed):
- âœ… Planning endpoints
- âœ… Adaptive interview flow
- âœ… Follow-up question delivery
- âœ… Evaluation enhancement
- âœ… Backward compatibility

**Mock Adapter Tests** (21/21 passed):
- âœ… MockAnalytics (13 tests)
- âœ… MockCVAnalyzer (8 tests)

**Domain Model Tests** (11/13 passed):
- âœ… Question adaptive fields (3/3)
- âœ… Interview adaptive fields (3/3)
- âœ… Answer adaptive fields (3/5) - 2 failures
- âœ… FollowUpQuestion (3/3)

#### âŒ Failed Tests (9 total)

**1. Domain Model Test Failures (2 tests)**

**Test**: `test_answer_without_gaps`
**File**: `tests/unit/domain/test_adaptive_models.py:171`
**Status**: FAILED
**Error**: `AssertionError: assert True is False`

```python
# Test code
answer = Answer(
    text="Complete answer",
    gaps={"concepts": [], "confirmed": False},  # Empty concepts array
)
assert answer.has_gaps() is False  # Expected False, got True

# has_gaps() implementation (line 117)
return self.gaps is not None and len(self.gaps) > 0
# Bug: Returns True if dict exists (len=2), should check if concepts array has items OR confirmed=True
```

**Root Cause**: `has_gaps()` checks `len(self.gaps) > 0` which returns True for `{"concepts": [], "confirmed": False}` (dict length = 2 keys).

**Fix Required**: Update logic to check if concepts array has items:
```python
def has_gaps(self) -> bool:
    if self.gaps is None:
        return False
    concepts = self.gaps.get("concepts", [])
    confirmed = self.gaps.get("confirmed", False)
    return len(concepts) > 0 and confirmed
```

**Test**: `test_is_adaptive_complete_no_gaps`
**File**: `tests/unit/domain/test_adaptive_models.py:198`
**Status**: FAILED
**Error**: `AssertionError: assert False is True`

```python
answer = Answer(
    similarity_score=0.75,  # Below 0.8 threshold
    gaps={"concepts": [], "confirmed": False},  # No actual gaps
)
assert answer.is_adaptive_complete() is True  # Should be True (no gaps)

# is_adaptive_complete() logic (line 137)
similarity_ok = self.similarity_score and self.similarity_score >= 0.8  # False
no_gaps = not self.has_gaps()  # False (but should be True)
return similarity_ok or no_gaps  # False or False = False
```

**Root Cause**: Same as above - `has_gaps()` incorrectly returns True for empty gaps.

**2. ExtractedSkill ValidationError (7 tests + 4 errors)**

**Tests Affected**:
- `test_plan_interview_with_2_skills`
- `test_plan_interview_with_4_skills`
- `test_plan_interview_with_7_skills`
- `test_plan_interview_with_10_skills_max_5`
- `test_calculate_n_for_various_skill_counts`
- `test_calculate_n_ignores_experience_years`
- Plus 4 tests in `test_process_answer_adaptive.py`
- Plus errors in `test_plan_interview.py` fixtures

**Error**:
```
pydantic_core._pydantic_core.ValidationError: 1 validation error for ExtractedSkill
skill
  Field required [type=missing, input_value={'name': 'Python', 'category': 'technical', 'proficiency': 'expert'}, input_type=dict]
```

**Root Cause**: ExtractedSkill model uses alias:
```python
# Domain model (line 16)
name: str = Field(alias="skill")  # Expects "skill" key, not "name"
```

**Tests using wrong key**:
```python
# tests/conftest.py:24 and test files
ExtractedSkill(name="Python", category="technical", proficiency="expert")
# Should be:
ExtractedSkill(skill="Python", category="technical", proficiency="expert")
```

**Fix Required**: Update all test fixtures to use `skill` instead of `name`:
- `tests/conftest.py` (sample_cv_analysis fixture)
- `tests/unit/use_cases/test_plan_interview.py` (multiple instances)
- `tests/unit/use_cases/test_process_answer_adaptive.py` (fixtures)

**3. Gap Detection Test Failure (1 test)**

**Test**: `test_keyword_gap_detection`
**File**: `tests/unit/use_cases/test_process_answer_adaptive.py`
**Error**: `AssertionError: assert 8 <= 3`

This test expects max 3 keywords in gaps but got 8. Likely related to LLM output parsing or test expectations.

---

## 4. Coverage Analysis

**Overall Coverage**: 27%

**Domain Layer** (well tested):
- âœ… domain/models: High coverage
- âœ… domain/ports: 100% coverage (interfaces)

**Application Layer** (partially tested):
- âš ï¸ use_cases: Some coverage, but errors blocking full test runs

**Infrastructure Layer** (not tested):
- âŒ config: 0% coverage
- âŒ database: 0% coverage
- âŒ dependency_injection: 0% coverage
- âŒ main.py: 0% coverage

**Note**: Infrastructure typically tested via integration tests, not unit tests.

---

## 5. Grep Analysis: reference_answer Usage

**Files Still Mentioning reference_answer** (8 files found):

1. âœ… `alembic/versions/251108_1200_drop_reference_answer_column.py` - Migration file (expected)
2. âš ï¸ `tests/unit/adapters/test_mock_analytics.py` - Test may need update
3. âœ… `plans/*.md` - Documentation (historical, OK)
4. âœ… `docs/system-architecture.md` - Documentation needs update
5. âœ… `repomix-output.xml` - Generated file (ignore)
6. âœ… `alembic/versions/525593eca676_seed_sample_data.py` - Old migration (OK)
7. âœ… `alembic/versions/a4047ce5a909_initial_database_schema.py` - Old migration (OK)

**Action Required**:
- Check `test_mock_analytics.py` for any `reference_answer` usage
- Update `docs/system-architecture.md` to reflect `ideal_answer`

---

## 6. Performance Metrics

**Test Execution Time**: 1.27 seconds (87 tests)

**Fast Tests**:
- Domain model tests: < 0.5s
- Mock adapter tests: < 0.3s

**Integration Tests**: Slower due to database operations

**No performance degradation** from the migration.

---

## 7. Recommendations

### ðŸ”´ Critical (Must Fix Before Production)

1. **Fix AnswerMapper** (BLOCKS TYPE CHECKING)
   - Add `similarity_score` and `gaps` to all three mapper methods
   - Priority: HIGH
   - Impact: Type errors, potential runtime failures

2. **Fix Answer.has_gaps() Logic** (2 TEST FAILURES)
   - Update to check `concepts` array length and `confirmed` flag
   - Priority: HIGH
   - Impact: Incorrect gap detection, wrong follow-up logic

### ðŸŸ¡ Important (Should Fix Soon)

3. **Fix ExtractedSkill Test Fixtures** (11 TEST ERRORS)
   - Replace `name=` with `skill=` in all test fixtures
   - Priority: MEDIUM
   - Impact: 11 tests blocked, coverage incomplete

4. **Fix Gap Detection Test**
   - Review `test_keyword_gap_detection` expectations
   - Priority: MEDIUM
   - Impact: 1 test failure

### ðŸŸ¢ Optional (Nice to Have)

5. **Update Documentation**
   - `docs/system-architecture.md` - replace `reference_answer` with `ideal_answer`
   - Priority: LOW
   - Impact: Documentation accuracy

6. **Address Type Warnings**
   - Add type parameters for generic `dict` types
   - Add return type annotations
   - Priority: LOW
   - Impact: Code quality, type safety

---

## 8. Unresolved Questions

1. **Migration Heads**: Why are there two migration heads (`d0078872a49a` and `251108_1200`)?
   - Current status shows both as head
   - Should they be merged or is this intentional branching?

2. **Gap Detection Algorithm**: What is the expected behavior for keyword extraction?
   - Test expects <= 3 keywords but got 8
   - Is this a test issue or implementation issue?

3. **Coverage Target**: What is the project's coverage goal?
   - Current: 27%
   - Infrastructure not tested - is this acceptable?

---

## 9. Next Steps

### Immediate Actions (Before Merge)

1. âœ… Migration applied successfully
2. âŒ Fix AnswerMapper (add similarity_score, gaps)
3. âŒ Fix Answer.has_gaps() logic
4. âŒ Fix ExtractedSkill test fixtures
5. âš ï¸ Re-run full test suite
6. âš ï¸ Verify all tests pass

### Post-Merge Actions

1. Update documentation (system-architecture.md)
2. Address type warnings
3. Investigate gap detection test failure
4. Consider increasing test coverage for infrastructure layer

---

## 10. Conclusion

**Migration Status**: âœ… SUCCESS - Database schema updated correctly

**Code Status**: âš ï¸ PARTIAL - Domain model correct, mapper incomplete

**Test Status**: âŒ FAILING - 20 tests blocked by 3 issues:
1. AnswerMapper missing fields (type error)
2. has_gaps() logic incorrect (2 failures)
3. ExtractedSkill fixtures wrong (11 errors, 7 failures)

**Recommendation**: **DO NOT MERGE** until critical fixes applied.

**Estimated Fix Time**: 30-60 minutes for all three issues.

---

**Report Generated**: 2025-11-08 16:30 UTC
**Next Review**: After mapper fixes applied
</file>

<file path="251108-status-value-error-investigation-report.md">
# Investigation Report: status.value AttributeError

**Date:** 2025-11-08
**Investigator:** Debug Agent
**Issue:** AttributeError at `src/adapters/persistence/mappers.py:157`
**Context:** Post-fix for Question model `use_enum_values` removal

---

## Executive Summary

**Root Cause:** Interview model has `use_enum_values = True` (line 48), causing `status` field to serialize as string instead of `InterviewStatus` enum. Mapper code inconsistently calls `.value` on a string.

**Business Impact:**
- Interview creation/update operations fail with AttributeError
- Database persistence layer broken for Interview entity
- API endpoints returning interview data are affected

**Affected Components:**
- Interview domain model (1 instance)
- InterviewMapper (2 locations)
- Interview REST API routes (7 locations)

**Recommended Action:** Remove `use_enum_values = True` from Interview model Config (same fix as Question model)

---

## Technical Analysis

### 1. Root Cause Identification

**Interview Model Config (src/domain/models/interview.py:45-49):**
```python
class Config:
    """Pydantic configuration."""

    use_enum_values = True  # â† PROBLEM
    frozen = False
```

**Effect:** When `use_enum_values = True`, Pydantic automatically converts:
- `InterviewStatus.PREPARING` â†’ `"preparing"` (string)
- `interview.status` becomes `str`, NOT `InterviewStatus` enum

**Mapper Assumption:** Code at line 157 assumes `status` is enum:
```python
status=domain_model.status.value,  # â† Calls .value on string
```

**Error:** `str` has no `.value` attribute â†’ AttributeError

### 2. Pattern Analysis

**Comparison with Question Model Fix:**
- Question model previously had `use_enum_values = True`
- Removed in recent fix â†’ now returns enum types
- Mappers correctly call `.value` on enums
- Interview model NOT updated â†’ still has config flag

**Mapper Inconsistency:**
- Line 157 (`to_db_model`): Uses `domain_model.status.value` âœ—
- Line 171 (`update_db_model`): Uses `domain_model.status` directly âœ“
- Line 140 (`to_domain`): Correctly wraps `InterviewStatus(db_model.status)` âœ“

### 3. Evidence from Codebase

**Domain Models with use_enum_values:**
```bash
$ grep -r "use_enum_values = True" src/domain/models/
src/domain/models/interview.py:48:        use_enum_values = True
```
Only Interview model affected (Question model already fixed).

**All .status.value Usage:**
1. `src/adapters/persistence/mappers.py:157` - InterviewMapper.to_db_model() âœ—
2. `src/adapters/persistence/mappers.py:171` - InterviewMapper.update_db_model() - INCONSISTENT (no .value)
3. `src/adapters/api/rest/interview_routes.py:285` - PlanningStatusResponse âœ—
4. `src/adapters/api/rest/interview_routes.py:329-338` - Status comparisons (6 instances) âœ—
5. `src/adapters/api/rest/interview_routes.py:342` - PlanningStatusResponse âœ—

**Database Model Expectations:**
- SQLAlchemy models store enums as strings in DB
- Mappers must convert enum â†’ string via `.value`
- BUT only when domain model has enum type (NOT when `use_enum_values = True`)

### 4. Timeline of Events

1. **Original State:** Interview model had `use_enum_values = True`, mappers called `.value`
2. **Question Fix:** Question model removed `use_enum_values`, mappers updated
3. **Current State:** Interview model still has flag, mappers still call `.value` â†’ CONFLICT
4. **Error Trigger:** Any interview save/update operation calls line 157

---

## Affected File Locations

### Critical (Breaks Persistence):
1. **src/domain/models/interview.py:48**
   - `use_enum_values = True` in Config
   - Causes status to serialize as string

2. **src/adapters/persistence/mappers.py:157**
   - `status=domain_model.status.value,`
   - Assumes enum, gets string â†’ AttributeError

3. **src/adapters/persistence/mappers.py:171**
   - `db_model.status = domain_model.status`
   - No `.value` call (inconsistent with line 157)

### Secondary (API Layer Issues):
4. **src/adapters/api/rest/interview_routes.py:285**
   - `status=interview.status.value,`
   - Returns PlanningStatusResponse

5. **src/adapters/api/rest/interview_routes.py:329-338**
   - `if interview.status.value == "PREPARING":` (6 comparisons)
   - Status string matching logic

6. **src/adapters/api/rest/interview_routes.py:342**
   - `status=interview.status.value,`
   - Returns PlanningStatusResponse

---

## Solution Approach

### Recommended Fix: Remove use_enum_values (Align with Question Model)

**Primary Change:**
```python
# src/domain/models/interview.py:45-49
class Config:
    """Pydantic configuration."""

    # use_enum_values = True  # â† REMOVE THIS LINE
    frozen = False
```

**Consequences:**
- `interview.status` becomes `InterviewStatus` enum (not string)
- Mapper calls to `.value` become VALID
- API routes calling `.value` become VALID
- Aligns with Question model pattern

**Files Requiring NO Changes:**
- `src/adapters/persistence/mappers.py:157` - Already calls `.value` âœ“
- `src/adapters/api/rest/interview_routes.py` - Already calls `.value` âœ“

**Files Requiring Update:**
- `src/adapters/persistence/mappers.py:171` - Should add `.value`:
  ```python
  db_model.status = domain_model.status.value  # Add .value
  ```

### Alternative Fix: Keep use_enum_values (NOT Recommended)

If keeping `use_enum_values = True`, must REMOVE all `.value` calls:
- Lines: 157, 171, 285, 329-338, 342
- Makes code inconsistent with Question model
- Loses type safety benefits

---

## Supporting Evidence

### Mapper Code Context (Lines 152-179)

```python
@staticmethod
def to_db_model(domain_model: Interview) -> InterviewModel:
    """Convert domain model to database model."""
    return InterviewModel(
        id=domain_model.id,
        candidate_id=domain_model.candidate_id,
        status=domain_model.status.value,  # LINE 157 - ERROR HERE
        cv_analysis_id=domain_model.cv_analysis_id,
        question_ids=domain_model.question_ids,
        answer_ids=domain_model.answer_ids,
        current_question_index=domain_model.current_question_index,
        started_at=domain_model.started_at,
        completed_at=domain_model.completed_at,
        created_at=domain_model.created_at,
        updated_at=domain_model.updated_at,
    )

@staticmethod
def update_db_model(db_model: InterviewModel, domain_model: Interview) -> None:
    """Update database model from domain model."""
    db_model.status = domain_model.status  # LINE 171 - INCONSISTENT (no .value)
    db_model.cv_analysis_id = domain_model.cv_analysis_id
    # ... rest of fields
```

### Question Model Pattern (Post-Fix)

```python
# src/domain/models/question.py:51-54
class Config:
    """Pydantic configuration."""

    pass  # â† No use_enum_values

# src/adapters/persistence/mappers.py:104-105
question_type=domain_model.question_type.value,  # âœ“ Works because enum
difficulty=domain_model.difficulty.value,        # âœ“ Works because enum
```

---

## Risk Assessment

**Immediate Risk (High):**
- All interview create/update operations FAIL
- API endpoints `/interview/plan` and `/{id}/plan` broken
- Database persistence completely blocked

**Fix Risk (Low):**
- Removing `use_enum_values` is safe (proven by Question model fix)
- Existing code already expects enum + `.value` pattern
- Only line 171 needs update (add `.value`)

**Rollback Risk (None):**
- Config change is reversible
- No database migration required (still stores strings)

---

## Actionable Recommendations

### Priority 1 (Critical - Fix Immediately):
1. Remove `use_enum_values = True` from `src/domain/models/interview.py:48`
2. Update `src/adapters/persistence/mappers.py:171`:
   - Change: `db_model.status = domain_model.status`
   - To: `db_model.status = domain_model.status.value`

### Priority 2 (Verification):
3. Run unit tests for InterviewMapper
4. Test interview creation via API
5. Verify status transitions (start, complete, cancel methods)

### Priority 3 (Code Quality):
6. Add type hints to catch this earlier:
   ```python
   def to_db_model(domain_model: Interview) -> InterviewModel:
       status: str = domain_model.status.value  # Explicit type
   ```

### Priority 4 (Prevention):
7. Document enum handling pattern in `docs/code-standards.md`
8. Add linter rule to detect `use_enum_values` in domain models
9. Create test suite for all mappers with enum fields

---

## Unresolved Questions

1. **Migration Path:** Are there any database records with malformed status values?
2. **API Contracts:** Do external clients expect string status or enum object?
3. **Serialization:** How does FastAPI serialize InterviewStatus enums in responses?
4. **Test Coverage:** Why didn't tests catch this inconsistency between line 157 and 171?

---

## Appendix: Full Context

### InterviewStatus Enum Definition
```python
# src/domain/models/interview.py:11-18
class InterviewStatus(str, Enum):
    """Interview status enumeration."""

    PREPARING = "preparing"
    READY = "ready"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
```

### API Route Usage Example
```python
# src/adapters/api/rest/interview_routes.py:329-338
if interview.status.value == "PREPARING":
    message = "Interview planning in progress..."
elif interview.status.value == "READY":
    message = f"Interview ready with {interview.planned_question_count} questions"
# ... etc
```

---

**End of Report**
</file>

<file path="251108-test-rerun-after-fixes-report.md">
# Test Rerun After Critical Fixes - QA Report
**Date**: 2025-11-08
**Reporter**: QA Engineer
**Branch**: feat/EA-6-start-interview

---

## Executive Summary

**Status**: âŒ **PARTIALLY SUCCESSFUL** - 3 original issues fixed, 3 NEW issues discovered

**Test Results**: 69 PASSED / 7 FAILED / 11 ERRORS (87 total)
**Type Checking**: âŒ FAILED (53 mypy errors)
**Pass Rate**: 79.3%

---

## âœ… Original Fixes - SUCCESSFUL

All 3 critical issues from previous report have been resolved:

### 1. AnswerMapper - similarity_score & gaps âœ…
- **Fixed**: Added `similarity_score` and `gaps` to all 3 mapper methods
- **Status**: No mapper-related errors in tests
- **Files**: `src/adapters/persistence/mappers.py`

### 2. Answer.has_gaps() Logic âœ…
- **Fixed**: Changed from `len(self.concepts) >= 5` to `len(self.concepts) < 5`
- **Fixed**: Changed from `not self.confirmed` to `self.confirmed`
- **Status**: Logic now correctly identifies gaps
- **Files**: `src/domain/models/answer.py`

### 3. ExtractedSkill Fixtures âœ…
- **Fixed**: Changed all `name=` to `skill=` in test fixtures
- **Status**: No more Pydantic validation errors for ExtractedSkill
- **Files**: Multiple test files

---

## âŒ NEW Issues Discovered

### CRITICAL Issue 1: Missing similarity_score in process_answer.py

**Location**: `src/application/use_cases/process_answer.py:64`

**Error**:
```
Missing named argument "similarity_score" for "Answer" [call-arg]
```

**Problem**: Legacy `ProcessAnswerUseCase` (non-adaptive) creates Answer without required `similarity_score` field

**Code** (line 64-69):
```python
answer = Answer(
    interview_id=interview_id,
    question_id=question_id,
    candidate_id=interview.candidate_id,
    text=answer_text,
    is_voice=bool(audio_file_path),
    # MISSING: similarity_score=0.0
)
```

**Impact**:
- Type checker fails
- Legacy flow broken (non-adaptive interviews)

**Fix Required**: Add `similarity_score=0.0` parameter

---

### CRITICAL Issue 2: Missing extracted_text in CVAnalysis Fixtures

**Affected Tests**: 11 tests in `test_plan_interview.py` and `test_process_answer_adaptive.py`

**Error**:
```
pydantic_core._pydantic_core.ValidationError: 1 validation error for CVAnalysis
extracted_text
  Field required [type=missing, ...]
```

**Problem**: Test fixtures create CVAnalysis without required `extracted_text` field

**Sample Fixture** (needs fixing):
```python
cv_analysis = CVAnalysis(
    candidate_id=UUID(...),
    extracted_skills=[...],
    education_level="Bachelor's Degree"
    # MISSING: extracted_text="..."
)
```

**Impact**: 11 tests erroring out with validation failures

**Fix Required**: Add `extracted_text="Sample CV text"` to all CVAnalysis fixtures

---

### MAJOR Issue 3: Gap Detection Test Logic Mismatch

**Location**: `tests/unit/use_cases/test_process_answer_adaptive.py::TestGapDetection::test_keyword_gap_detection`

**Error**:
```python
AssertionError: assert 8 <= 3
 +  where 8 = len(['itself.', 'condition.', 'case,', 'calling', 'stack,', 'termination', 'concepts:', 'function'])
```

**Problem**: Test expects â‰¤3 gaps, but implementation returns 8

**Root Cause**: Gap detection extracts punctuation/artifacts as "gaps":
- `'itself.'` â† includes period
- `'case,'` â† includes comma
- `'concepts:'` â† includes colon
- `'stack,'` â† includes comma

**Actual Missing Concepts**: ~3-4 real keywords (termination, calling, stack, function)
**Detected "Gaps"**: 8 tokens (including punctuation artifacts)

**Impact**: Test assertion doesn't match implementation behavior

**Fix Options**:
1. **Fix Implementation**: Strip punctuation from detected gaps
2. **Fix Test**: Adjust assertion to `assert len(gaps) <= 10`
3. **Hybrid**: Improve tokenization + relax test threshold

---

## MINOR Issues

### Type Checking Warnings (53 errors)

**Categories**:
1. **Missing type annotations**: 22 errors (functions without return types)
2. **Generic dict warnings**: 12 errors (use `dict[str, Any]`)
3. **Untyped function calls**: 8 errors
4. **None attribute access**: 7 errors (Pinecone index initialization)
5. **Other**: 4 errors (unions, deprecated Pydantic config)

**Impact**: Non-blocking but reduces type safety

**Files Most Affected**:
- `src/adapters/api/websocket/connection_manager.py` (7 errors)
- `src/adapters/persistence/models.py` (7 errors)
- `src/adapters/vector_db/pinecone_adapter.py` (7 errors)
- `src/infrastructure/config/settings.py` (4 errors)

---

## Test Suite Breakdown

### âœ… Passing Test Suites (69 tests)

1. **Integration Tests** - 14/14 PASSED
   - Planning endpoints âœ…
   - Adaptive interview flow âœ…
   - Follow-up delivery âœ…
   - Backward compatibility âœ…

2. **Mock Adapter Tests** - 38/38 PASSED
   - MockAnalyticsAdapter (14 tests) âœ…
   - MockCVAnalyzerAdapter (24 tests) âœ…

3. **Use Case Tests** - 17/35 PASSED
   - Gap detection (partial) âš ï¸
   - Adaptive processing (partial) âš ï¸

### âŒ Failing Tests (18 total)

**Category A: Gap Detection Issues** (7 tests)
- `test_keyword_gap_detection` - Assertion mismatch
- `test_synonym_concept_match` - 6 != 5
- `test_case_insensitive_match` - 6 != 5
- `test_punctuation_ignored` - 7 != 5
- `test_partial_word_not_matched` - 9 != 5
- `test_empty_answer_all_gaps` - 6 != 5
- `test_concept_confirmation_threshold` - 5 != 3

**Category B: CVAnalysis Validation** (11 tests)
- All in `test_plan_interview.py` and `test_process_answer_adaptive.py`
- Missing `extracted_text` field in fixtures

---

## Performance Metrics

**Test Execution Time**: 1.69s (excellent)
**Coverage**: 15% (low - most adapters/infrastructure untested)

**Coverage by Layer**:
- Domain Models: 50-88% âœ…
- Use Cases: 17-47% âš ï¸
- Adapters: 0-5% âŒ
- Infrastructure: 0% âŒ

---

## Priority Action Items

### P0 - CRITICAL (Block Merge)

1. **Fix process_answer.py Answer Creation**
   - Add `similarity_score=0.0` to line 64
   - **ETA**: 2 minutes
   - **File**: `src/application/use_cases/process_answer.py`

2. **Fix CVAnalysis Test Fixtures**
   - Add `extracted_text="Sample CV text"` to 11 fixtures
   - **ETA**: 10 minutes
   - **Files**:
     - `tests/unit/use_cases/test_plan_interview.py`
     - `tests/unit/use_cases/test_process_answer_adaptive.py`

3. **Fix Gap Detection Test Logic**
   - Option A: Strip punctuation in implementation
   - Option B: Relax test assertions
   - **ETA**: 15 minutes
   - **File**: `tests/unit/use_cases/test_process_answer_adaptive.py`

### P1 - HIGH (Type Safety)

4. **Fix Type Checking Errors**
   - Add return type annotations (22 errors)
   - Fix generic dict types (12 errors)
   - **ETA**: 30 minutes

---

## Recommended Next Steps

1. **Immediate**: Fix P0 items (3 issues)
2. **Short-term**: Resolve mypy errors for type safety
3. **Long-term**: Increase test coverage (target 80%)

---

## Questions/Blockers

1. **Gap Detection Behavior**: Should gaps include punctuation artifacts or be cleaned?
2. **Test Strategy**: Should we prioritize integration tests over unit tests for use cases?
3. **Coverage Goals**: What's acceptable coverage threshold for this feature branch?

---

## Files Modified (3 Fixes Applied)

1. `src/adapters/persistence/mappers.py` - Added similarity_score/gaps mapping
2. `src/domain/models/answer.py` - Fixed has_gaps() logic
3. Multiple test files - Fixed ExtractedSkill fixtures

## Files Requiring Changes (3 New Issues)

1. `src/application/use_cases/process_answer.py` - Add similarity_score
2. `tests/unit/use_cases/test_plan_interview.py` - Add extracted_text to fixtures
3. `tests/unit/use_cases/test_process_answer_adaptive.py` - Fix gap detection tests

---

**Report Generated**: 2025-11-08
**Next Review**: After P0 fixes applied
</file>

<file path="alembic/README">
Generic single-database configuration.
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="alembic/versions/0003_create_evaluations_tables.py">
"""Create evaluations and evaluation_gaps tables

Revision ID: 0003
Revises: 0002
Create Date: 2025-11-14 02:23:00

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0003'
down_revision: Union[str, None] = '0002'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Create evaluations and evaluation_gaps tables, migrate data from answers."""

    # Step 1: Create evaluations table
    op.create_table(
        'evaluations',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, nullable=False),
        sa.Column('answer_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('question_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('interview_id', postgresql.UUID(as_uuid=True), nullable=False),

        # Scores
        sa.Column('raw_score', sa.Float(), nullable=False),
        sa.Column('penalty', sa.Float(), nullable=False, server_default='0'),
        sa.Column('final_score', sa.Float(), nullable=False),
        sa.Column('similarity_score', sa.Float(), nullable=True),

        # LLM evaluation details
        sa.Column('completeness', sa.Float(), nullable=False),
        sa.Column('relevance', sa.Float(), nullable=False),
        sa.Column('sentiment', sa.String(50), nullable=True),
        sa.Column('reasoning', sa.Text(), nullable=True),
        sa.Column('strengths', postgresql.ARRAY(sa.Text()), nullable=False, server_default='{}'),
        sa.Column('weaknesses', postgresql.ARRAY(sa.Text()), nullable=False, server_default='{}'),
        sa.Column('improvement_suggestions', postgresql.ARRAY(sa.Text()), nullable=False, server_default='{}'),

        # Follow-up context
        sa.Column('attempt_number', sa.Integer(), nullable=False, server_default='1'),
        sa.Column('parent_evaluation_id', postgresql.UUID(as_uuid=True), nullable=True),

        # Timestamps
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('evaluated_at', sa.DateTime(), nullable=True),

        # Foreign keys
        sa.ForeignKeyConstraint(['answer_id'], ['answers.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['interview_id'], ['interviews.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['parent_evaluation_id'], ['evaluations.id'], ondelete='SET NULL'),
    )

    # Indexes for evaluations
    op.create_index('idx_evaluations_answer_id', 'evaluations', ['answer_id'])
    op.create_index('idx_evaluations_question_id', 'evaluations', ['question_id'])
    op.create_index('idx_evaluations_interview_id', 'evaluations', ['interview_id'])
    op.create_index('idx_evaluations_parent_id', 'evaluations', ['parent_evaluation_id'])
    op.create_index('idx_evaluations_attempt_number', 'evaluations', ['attempt_number'])

    # Constraints for evaluations
    op.create_check_constraint(
        'check_raw_score_bounds',
        'evaluations',
        'raw_score >= 0 AND raw_score <= 100'
    )
    op.create_check_constraint(
        'check_penalty_bounds',
        'evaluations',
        'penalty >= -15 AND penalty <= 0'
    )
    op.create_check_constraint(
        'check_final_score_bounds',
        'evaluations',
        'final_score >= 0 AND final_score <= 100'
    )
    op.create_check_constraint(
        'check_similarity_score_bounds',
        'evaluations',
        'similarity_score IS NULL OR (similarity_score >= 0 AND similarity_score <= 1)'
    )
    op.create_check_constraint(
        'check_completeness_bounds',
        'evaluations',
        'completeness >= 0 AND completeness <= 1'
    )
    op.create_check_constraint(
        'check_relevance_bounds',
        'evaluations',
        'relevance >= 0 AND relevance <= 1'
    )
    op.create_check_constraint(
        'check_attempt_number_bounds',
        'evaluations',
        'attempt_number >= 1 AND attempt_number <= 3'
    )

    # Step 2: Create evaluation_gaps table
    op.create_table(
        'evaluation_gaps',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, nullable=False),
        sa.Column('evaluation_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('concept', sa.Text(), nullable=False),
        sa.Column('severity', sa.String(20), nullable=False, server_default="'moderate'"),
        sa.Column('resolved', sa.Boolean(), nullable=False, server_default='false'),
        sa.Column('created_at', sa.DateTime(), nullable=False),

        # Foreign key
        sa.ForeignKeyConstraint(['evaluation_id'], ['evaluations.id'], ondelete='CASCADE'),
    )

    # Indexes for evaluation_gaps
    op.create_index('idx_evaluation_gaps_evaluation_id', 'evaluation_gaps', ['evaluation_id'])
    op.create_index('idx_evaluation_gaps_resolved', 'evaluation_gaps', ['resolved'])

    # Constraint for evaluation_gaps
    op.create_check_constraint(
        'check_severity_values',
        'evaluation_gaps',
        "severity IN ('minor', 'moderate', 'major')"
    )

    # Step 3: Add evaluation_id column to answers table
    op.add_column('answers', sa.Column('evaluation_id', postgresql.UUID(as_uuid=True), nullable=True))
    op.create_foreign_key(
        'fk_answers_evaluation_id',
        'answers', 'evaluations',
        ['evaluation_id'], ['id'],
        ondelete='SET NULL'
    )
    op.create_index('idx_answers_evaluation_id', 'answers', ['evaluation_id'])

    # Step 4: Migrate existing data from answers.evaluation JSONB to evaluations table
    # This uses raw SQL because SQLAlchemy ORM doesn't handle JSONB migration well
    connection = op.get_bind()

    # Migration SQL (using PostgreSQL JSONB functions)
    migration_sql = """
    -- Insert evaluations from answers.evaluation JSONB
    INSERT INTO evaluations (
        id, answer_id, question_id, interview_id,
        raw_score, penalty, final_score, similarity_score,
        completeness, relevance, sentiment, reasoning,
        strengths, weaknesses, improvement_suggestions,
        attempt_number, parent_evaluation_id,
        created_at, evaluated_at
    )
    SELECT
        gen_random_uuid() as id,
        a.id as answer_id,
        a.question_id,
        a.interview_id,

        -- Extract score from JSONB (raw_score = final_score for old data)
        COALESCE((a.evaluation->>'score')::FLOAT, 0.0) as raw_score,
        0.0 as penalty,  -- No penalty for old data
        COALESCE((a.evaluation->>'score')::FLOAT, 0.0) as final_score,
        a.similarity_score,

        -- Extract evaluation details from JSONB
        COALESCE((a.evaluation->>'completeness')::FLOAT, 0.5) as completeness,
        COALESCE((a.evaluation->>'relevance')::FLOAT, 0.5) as relevance,
        a.evaluation->>'sentiment' as sentiment,
        a.evaluation->>'reasoning' as reasoning,

        -- Convert JSONB arrays to PostgreSQL arrays
        COALESCE(
            ARRAY(SELECT jsonb_array_elements_text(a.evaluation->'strengths')),
            ARRAY[]::TEXT[]
        ) as strengths,
        COALESCE(
            ARRAY(SELECT jsonb_array_elements_text(a.evaluation->'weaknesses')),
            ARRAY[]::TEXT[]
        ) as weaknesses,
        COALESCE(
            ARRAY(SELECT jsonb_array_elements_text(a.evaluation->'improvement_suggestions')),
            ARRAY[]::TEXT[]
        ) as improvement_suggestions,

        1 as attempt_number,  -- Assume main question for old data
        NULL as parent_evaluation_id,

        a.created_at,
        a.evaluated_at
    FROM answers a
    WHERE a.evaluation IS NOT NULL;
    """

    connection.execute(sa.text(migration_sql))

    # Step 5: Migrate gaps from answers.gaps JSONB to evaluation_gaps table
    gaps_migration_sql = """
    -- Insert gaps from answers.gaps JSONB
    INSERT INTO evaluation_gaps (id, evaluation_id, concept, severity, resolved, created_at)
    SELECT
        gen_random_uuid() as id,
        e.id as evaluation_id,
        gap_concept.value as concept,
        COALESCE(a.gaps->>'severity', 'moderate') as severity,
        FALSE as resolved,  -- Old gaps not resolved
        a.created_at
    FROM answers a
    JOIN evaluations e ON e.answer_id = a.id
    CROSS JOIN LATERAL jsonb_array_elements_text(a.gaps->'concepts') as gap_concept(value)
    WHERE a.gaps IS NOT NULL
      AND a.gaps->'concepts' IS NOT NULL
      AND jsonb_array_length(a.gaps->'concepts') > 0;
    """

    connection.execute(sa.text(gaps_migration_sql))

    # Step 6: Update answers.evaluation_id to link to new evaluations
    update_fk_sql = """
    UPDATE answers a
    SET evaluation_id = e.id
    FROM evaluations e
    WHERE e.answer_id = a.id;
    """

    connection.execute(sa.text(update_fk_sql))

    # Step 7: Drop old columns from answers table (after verification)
    # Keep them for now to allow rollback
    # op.drop_column('answers', 'evaluation')
    # op.drop_column('answers', 'similarity_score')
    # op.drop_column('answers', 'gaps')
    # op.drop_column('answers', 'evaluated_at')


def downgrade() -> None:
    """Revert migration - restore old JSONB structure."""

    connection = op.get_bind()

    # Step 1: Recreate old columns if they were dropped
    # (Skip if columns still exist)

    # Step 2: Migrate data back from evaluations to answers.evaluation JSONB
    rollback_sql = """
    -- Restore evaluation JSONB
    UPDATE answers a
    SET
        evaluation = (
            SELECT jsonb_build_object(
                'score', e.raw_score,
                'completeness', e.completeness,
                'relevance', e.relevance,
                'sentiment', e.sentiment,
                'reasoning', e.reasoning,
                'strengths', to_jsonb(e.strengths),
                'weaknesses', to_jsonb(e.weaknesses),
                'improvement_suggestions', to_jsonb(e.improvement_suggestions)
            )
            FROM evaluations e
            WHERE e.answer_id = a.id
        ),
        similarity_score = (
            SELECT e.similarity_score
            FROM evaluations e
            WHERE e.answer_id = a.id
        ),
        gaps = (
            SELECT jsonb_build_object(
                'concepts', COALESCE(jsonb_agg(g.concept), '[]'::jsonb),
                'severity', COALESCE(MIN(g.severity), 'moderate'),
                'confirmed', COALESCE(BOOL_OR(NOT g.resolved), false)
            )
            FROM evaluation_gaps g
            WHERE g.evaluation_id = (
                SELECT e.id FROM evaluations e WHERE e.answer_id = a.id
            )
        ),
        evaluated_at = (
            SELECT e.evaluated_at
            FROM evaluations e
            WHERE e.answer_id = a.id
        )
    WHERE EXISTS (
        SELECT 1 FROM evaluations e WHERE e.answer_id = a.id
    );
    """

    connection.execute(sa.text(rollback_sql))

    # Step 3: Drop new tables
    op.drop_index('idx_answers_evaluation_id', 'answers')
    op.drop_constraint('fk_answers_evaluation_id', 'answers', type_='foreignkey')
    op.drop_column('answers', 'evaluation_id')

    op.drop_table('evaluation_gaps')
    op.drop_table('evaluations')
</file>

<file path="CHANGELOG_ENV.md">
# Environment Configuration Update

## Summary

Updated all migration scripts and configuration to support `.env.local` for local development overrides. This follows security best practices by keeping sensitive credentials out of version control.

## Changes Made

### 1. Updated Scripts

All Python scripts now check for `.env.local` first, then fallback to `.env`:

#### `scripts/setup_db.py`
- Added environment file detection logic
- Prints which file is being loaded
- Tries `.env.local` â†’ `.env` â†’ warns if neither exists

#### `scripts/verify_db.py`
- Same environment loading pattern as setup_db.py
- Consistent behavior across all scripts

#### `scripts/test_env.py` (NEW)
- Test script to verify environment configuration
- Shows which file is active
- Displays loaded settings (with password masking)

### 2. Updated Shell Scripts

#### `scripts/setup_and_migrate.sh` (Linux/macOS)
- Checks for `.env.local` first
- Falls back to `.env`
- Shows which file is being used
- Exports variables from the active file

#### `scripts/setup_and_migrate.bat` (Windows)
- Same logic as bash script
- Windows-compatible syntax
- Sets `ENV_FILE` variable to track active file

### 3. Updated Documentation

#### `.env.example`
- Added header explaining the `.env` vs `.env.local` pattern
- Documents that `.env.local` takes precedence
- Notes that `.env.local` is gitignored

#### `.gitignore`
- Added explicit `.env.local` entry
- Ensures local credentials are never committed

#### `DATABASE_SETUP.md`
- Updated configuration section
- Explains file priority order
- Recommends using `.env.local` for credentials
- Links to new `ENV_SETUP.md`

#### `ENV_SETUP.md` (NEW)
- Comprehensive guide to environment configuration
- Explains priority order
- Best practices for security
- Examples of shared vs. local config
- Troubleshooting guide

## File Priority Order

The application and all scripts now follow this order:

```
1. .env.local (highest priority)
   â†“ if not found
2. .env (fallback)
   â†“ if not found
3. System environment variables
   â†“ if not found
4. Pydantic defaults (from settings.py)
```

## Migration Guide

### For Existing Developers

If you already have a `.env` file:

```bash
# Option 1: Convert .env to .env.local
mv .env .env.local

# Option 2: Keep .env and create .env.local for overrides
cp .env .env.local
# Then edit .env.local with your actual credentials

# Option 3: Just create .env.local from example
cp .env.example .env.local
# Edit with your credentials
```

### For New Developers

```bash
# Copy example to .env.local
cp .env.example .env.local

# Edit with your credentials
nano .env.local
```

## Benefits

1. **Security**
   - Sensitive credentials stay in `.env.local` (gitignored)
   - Safe default configuration in `.env` (can be committed)
   - No accidental credential commits

2. **Flexibility**
   - Each developer can have different local settings
   - Easy to switch between configurations
   - No merge conflicts on environment files

3. **Team Collaboration**
   - Shared defaults in `.env` (optional)
   - Personal overrides in `.env.local`
   - Clear separation of concerns

4. **Consistency**
   - All scripts use the same loading pattern
   - Predictable behavior across the project
   - Clear feedback on which file is active

## Verification

Test the new configuration:

```bash
# Check which environment file will be used
python scripts/test_env.py

# Run database setup (will show which file it loads)
python scripts/setup_db.py

# Run migration scripts
scripts/setup_and_migrate.bat  # Windows
# or
./scripts/setup_and_migrate.sh  # Linux/macOS
```

## Backward Compatibility

This change is **fully backward compatible**:

- Existing `.env` files continue to work
- No changes required to existing workflows
- Scripts gracefully fallback to `.env` if `.env.local` doesn't exist
- Settings.py already supported this pattern via Pydantic

## Configuration Already Supported

The `settings.py` was already configured to use this pattern:

```python
model_config = SettingsConfigDict(
    env_file=(".env.local", ".env"),  # Try .env.local first
    env_file_encoding="utf-8",
    case_sensitive=False,
)
```

This update ensures **all scripts** follow the same pattern.

## Related Files

**Created:**
- `ENV_SETUP.md` - Comprehensive environment configuration guide
- `scripts/test_env.py` - Test script for verifying configuration

**Modified:**
- `scripts/setup_db.py` - Added .env.local support
- `scripts/verify_db.py` - Added .env.local support
- `scripts/setup_and_migrate.sh` - Added .env.local support
- `scripts/setup_and_migrate.bat` - Added .env.local support
- `.env.example` - Added documentation header
- `.gitignore` - Added explicit .env.local entry
- `DATABASE_SETUP.md` - Updated configuration section

**Unchanged:**
- `src/infrastructure/config/settings.py` - Already supported this pattern
- `alembic/env.py` - Uses settings.py, automatically inherits support

## Security Notes

**DO NOT commit:**
- `.env.local` - Personal credentials and sensitive data
- `.env` (if it contains secrets) - Only commit with placeholder values

**Safe to commit:**
- `.env.example` - Template with no sensitive data
- `.env` (optional) - If it contains only non-sensitive defaults

## Questions?

- Read `ENV_SETUP.md` for detailed explanation
- Run `python scripts/test_env.py` to test your configuration
- Check `DATABASE_SETUP.md` for database-specific setup

## Date

Updated: 2025-01-31
</file>

<file path="pytest.ini">
[pytest]
# Pytest configuration for adaptive interview tests

# Test discovery
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Asyncio mode
asyncio_mode = auto

# Markers
markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (require database, external services)
    slow: Slow tests (may take > 5 seconds)
    adaptive: Tests for adaptive interview features

# Test output
addopts =
    -v
    --strict-markers
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-branch

# Coverage
[coverage:run]
source = src
omit =
    */tests/*
    */migrations/*
    */__pycache__/*
    */site-packages/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
</file>

<file path="quickstart.bat">
@echo off
REM Quick start script for Windows

echo ========================================
echo Elios AI Interview Service - Quick Start
echo ========================================
echo.

REM Check if virtual environment exists
if not exist "venv\" (
    echo [1/4] Creating virtual environment...
    python -m venv venv
    if errorlevel 1 (
        echo ERROR: Failed to create virtual environment
        pause
        exit /b 1
    )
) else (
    echo [1/4] Virtual environment already exists
)

echo [2/4] Activating virtual environment...
call venv\Scripts\activate.bat

echo [3/4] Installing dependencies...
echo This may take a few minutes on first run...
python -m pip install --upgrade pip >nul 2>&1
pip install fastapi uvicorn pydantic pydantic-settings python-dotenv

echo [4/4] Starting server...
echo.
echo ========================================
echo Server starting on http://localhost:8000
echo ========================================
echo.
echo Visit:
echo   - http://localhost:8000 (Welcome page)
echo   - http://localhost:8000/health (Health check)
echo   - http://localhost:8000/docs (API docs)
echo.
echo Press CTRL+C to stop the server
echo.

python src/main.py
</file>

<file path="requirements/base.txt">
# Core Framework
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
pydantic-settings>=2.1.0

# LLM Providers
openai>=1.3.0
anthropic>=0.7.0

# Vector Databases
pinecone-client>=3.0.0

# Database
sqlalchemy[asyncio]>=2.0.0
asyncpg>=0.29.0
alembic>=1.13.0

# NLP & Document Processing
spacy>=3.7.0
langchain>=0.1.0
PyPDF2>=3.0.0
python-docx>=1.1.0

# Utilities
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
passlib[bcrypt]>=1.7.4
httpx>=0.25.0
</file>

<file path="requirements/dev.txt">
-r base.txt

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0

# Code Quality
ruff>=0.1.6
black>=23.11.0
mypy>=1.7.0

# Development Tools
ipython>=8.18.0
python-dotenv>=1.0.0
</file>

<file path="requirements/prod.txt">
-r base.txt

# Production-specific dependencies
gunicorn>=21.2.0
</file>

<file path="src/__init__.py">
"""Elios AI Interview Service package."""

__version__ = "0.1.0"
</file>

<file path="src/adapters/__init__.py">
"""Adapters package."""
</file>

<file path="src/adapters/api/__init__.py">
"""API adapters package."""
</file>

<file path="src/adapters/api/rest/__init__.py">
"""REST API routes package."""
</file>

<file path="src/adapters/api/websocket/__init__.py">
"""WebSocket handlers for real-time interview communication."""

from .connection_manager import manager
from .interview_handler import handle_interview_websocket

__all__ = ["manager", "handle_interview_websocket"]
</file>

<file path="src/adapters/api/websocket/connection_manager.py">
"""WebSocket connection manager for interview sessions."""

import logging
from uuid import UUID

from fastapi import WebSocket

logger = logging.getLogger(__name__)


class ConnectionManager:
    """Manage WebSocket connections for interviews."""

    def __init__(self):
        # interview_id â†’ websocket
        self.active_connections: dict[UUID, WebSocket] = {}

    async def connect(self, interview_id: UUID, websocket: WebSocket):
        """Accept and register WebSocket connection.

        Args:
            interview_id: Interview UUID
            websocket: WebSocket connection
        """
        await websocket.accept()
        self.active_connections[interview_id] = websocket
        logger.info(f"WebSocket connected for interview {interview_id}")

    def disconnect(self, interview_id: UUID):
        """Remove connection.

        Args:
            interview_id: Interview UUID
        """
        if interview_id in self.active_connections:
            del self.active_connections[interview_id]
            logger.info(f"WebSocket disconnected for interview {interview_id}")

    async def send_message(self, interview_id: UUID, message: dict):
        """Send message to specific interview connection.

        Args:
            interview_id: Interview UUID
            message: Message dictionary to send
        """
        websocket = self.active_connections.get(interview_id)
        if websocket:
            await websocket.send_json(message)

    async def broadcast(self, message: dict):
        """Send message to all connections.

        Args:
            message: Message dictionary to broadcast
        """
        for websocket in self.active_connections.values():
            await websocket.send_json(message)


# Global instance
manager = ConnectionManager()
</file>

<file path="src/adapters/llm/__init__.py">
"""LLM adapters package."""

from .openai_adapter import OpenAIAdapter

__all__ = ["OpenAIAdapter"]
</file>

<file path="src/adapters/mock/mock_analytics.py">
"""Mock Analytics adapter for development and testing."""

from collections import defaultdict
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.models.question import Question
from ...domain.ports.analytics_port import AnalyticsPort


class MockAnalyticsAdapter(AnalyticsPort):
    """Mock analytics adapter that simulates performance tracking.

    This adapter provides in-memory analytics calculations without
    requiring external analytics services or databases.

    Storage is maintained in memory per instance, so each test
    should use a fresh instance for isolation.
    """

    def __init__(self) -> None:
        """Initialize mock analytics with empty storage."""
        # Store evaluations per interview
        self._evaluations: dict[UUID, list[Answer]] = defaultdict(list)

        # Store candidate history (mock historical data)
        self._candidate_history: dict[UUID, list[dict[str, Any]]] = {}

    async def record_answer_evaluation(
        self,
        interview_id: UUID,
        answer: Answer,
    ) -> None:
        """Record answer evaluation for analytics.

        Args:
            interview_id: Interview identifier
            answer: Answer with evaluation data
        """
        self._evaluations[interview_id].append(answer)

    async def get_interview_statistics(
        self,
        interview_id: UUID,
    ) -> dict[str, Any]:
        """Get statistics for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            Dictionary with interview statistics
        """
        answers = self._evaluations.get(interview_id, [])

        if not answers:
            return {
                "interview_id": str(interview_id),
                "question_count": 0,
                "answers_count": 0,
                "avg_score": 0.0,
                "completion_rate": 0.0,
                "time_spent_minutes": 0,
            }

        # Calculate average score
        scores = [
            a.evaluation.score
            for a in answers
            if a.evaluation is not None
        ]
        avg_score = sum(scores) / len(scores) if scores else 0.0

        # Mock time calculation (2-5 min per question)
        time_spent = len(answers) * 3.5  # Average 3.5 minutes per answer

        return {
            "interview_id": str(interview_id),
            "question_count": len(answers),
            "answers_count": len([a for a in answers if a.text]),
            "avg_score": round(avg_score, 2),
            "completion_rate": round(len([a for a in answers if a.text]) / len(answers) * 100, 2) if answers else 0.0,
            "time_spent_minutes": round(time_spent, 1),
            "highest_score": max(scores) if scores else 0.0,
            "lowest_score": min(scores) if scores else 0.0,
        }

    async def get_candidate_performance_history(
        self,
        candidate_id: UUID,
    ) -> list[dict[str, Any]]:
        """Get candidate's performance across all interviews.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of historical interview performance data
        """
        # Check if we have mock history for this candidate
        if candidate_id in self._candidate_history:
            return self._candidate_history[candidate_id]

        # Generate mock historical data (0-3 past interviews)
        # Simulate improvement over time
        history = []
        num_past_interviews = 2  # Mock: 2 past interviews

        for i in range(num_past_interviews):
            history.append({
                "interview_date": f"2024-{10-i:02d}-15",
                "avg_score": round(65.0 + (i * 8.0), 2),  # Improving scores
                "questions_answered": 5 + i,
                "completion_rate": round(80.0 + (i * 10.0), 2),
                "strong_skills": ["Python", "SQL"] if i == 0 else ["Python", "FastAPI", "PostgreSQL"],
                "weak_skills": ["System Design", "Architecture"] if i == 0 else ["System Design"],
            })

        self._candidate_history[candidate_id] = history
        return history

    async def generate_improvement_recommendations(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[Answer],
    ) -> list[str]:
        """Generate improvement recommendations based on performance.

        Args:
            interview_id: Interview identifier
            questions: Questions asked
            answers: Answers with evaluations

        Returns:
            List of improvement recommendations
        """
        if not answers:
            return ["Complete the interview to receive personalized recommendations"]

        # Calculate average score
        scores = [
            a.evaluation.score
            for a in answers
            if a.evaluation is not None
        ]
        avg_score = sum(scores) / len(scores) if scores else 0.0

        # Collect weaknesses from evaluations
        all_weaknesses = []
        weak_skills = []

        for answer, question in zip(answers, questions, strict=True):
            if answer.evaluation:
                all_weaknesses.extend(answer.evaluation.weaknesses)

                # Track skills with low scores (<70)
                if answer.evaluation.score < 70.0:
                    weak_skills.extend(question.skills)

        # Deduplicate and count weaknesses
        weakness_counts: dict[str, int] = defaultdict(int)
        for weakness in all_weaknesses:
            weakness_counts[weakness] += 1

        # Sort by frequency
        top_weaknesses = sorted(
            weakness_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]

        # Generate recommendations based on score ranges
        recommendations = []

        if avg_score < 60:
            recommendations.append("Focus on understanding fundamental concepts before diving into advanced topics")
            recommendations.append("Practice explaining technical concepts clearly and concisely")
            recommendations.append("Review and strengthen basic knowledge in your core skills")

            # Add specific weakness-based recommendations
            for weakness, _ in top_weaknesses[:2]:
                if "depth" in weakness.lower():
                    recommendations.append("Study topics in more depth with practical examples")
                elif "example" in weakness.lower():
                    recommendations.append("Prepare concrete examples from your experience")

        elif avg_score < 80:
            recommendations.append("Strengthen your understanding of intermediate concepts")
            recommendations.append("Practice providing more detailed and structured answers")

            # Add skill-specific recommendations
            if weak_skills:
                unique_weak_skills = list(set(weak_skills))[:2]
                for skill in unique_weak_skills:
                    recommendations.append(f"Improve your knowledge in {skill} through hands-on projects")

            # Add weakness-based recommendations
            for weakness, _ in top_weaknesses[:2]:
                recommendations.append(f"Work on: {weakness}")

        else:
            recommendations.append("Excellent performance! Continue building expertise in advanced topics")
            recommendations.append("Consider exploring cutting-edge technologies and patterns")

            # Even high performers can improve
            if top_weaknesses:
                recommendations.append(f"Minor area for growth: {top_weaknesses[0][0]}")

        # Limit to 3-5 recommendations
        return recommendations[:5]

    async def calculate_skill_scores(
        self,
        answers: list[Answer],
        questions: list[Question],
    ) -> dict[str, float]:
        """Calculate scores per skill based on answers.

        Args:
            answers: List of evaluated answers
            questions: Corresponding questions

        Returns:
            Dictionary mapping skill names to average scores
        """
        if not answers or not questions or len(answers) != len(questions):
            return {}

        # Group scores by skill
        skill_scores: dict[str, list[float]] = defaultdict(list)

        for answer, question in zip(answers, questions, strict=True):
            if answer.evaluation is None:
                continue

            score = answer.evaluation.score

            # Associate this score with all skills in the question
            for skill in question.skills:
                skill_scores[skill].append(score)

        # Calculate average score per skill
        result = {}
        for skill, scores in skill_scores.items():
            result[skill] = round(sum(scores) / len(scores), 2)

        return result
</file>

<file path="src/adapters/mock/mock_cv_analyzer.py">
"""Mock CV Analyzer adapter for development and testing."""

import random
from pathlib import Path
from uuid import UUID, uuid4

from ...domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from ...domain.ports.cv_analyzer_port import CVAnalyzerPort


class MockCVAnalyzerAdapter(CVAnalyzerPort):
    """Mock CV analyzer that returns realistic but simulated analysis.

    This adapter simulates CV parsing and skill extraction without
    requiring actual document processing or LLM API calls.

    The mock provides deterministic results based on filename patterns:
    - Files with "junior" â†’ 2-3 skills, 1-2 years exp, EASY difficulty
    - Files with "senior" â†’ 5-6 skills, 6-10 years exp, HARD difficulty
    - Default (mid-level) â†’ 4-5 skills, 3-5 years exp, MEDIUM difficulty
    """

    # Skill database organized by experience level
    JUNIOR_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="intermediate", years=1.5),
        ExtractedSkill(skill="Git", category="technical", proficiency="beginner", years=1.0),
        ExtractedSkill(skill="SQL", category="technical", proficiency="beginner", years=0.5),
        ExtractedSkill(skill="Communication", category="soft", proficiency="intermediate"),
        ExtractedSkill(skill="Team Collaboration", category="soft", proficiency="beginner"),
    ]

    MID_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="advanced", years=3.5),
        ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate", years=2.0),
        ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate", years=2.5),
        ExtractedSkill(skill="Docker", category="technical", proficiency="intermediate", years=1.5),
        ExtractedSkill(skill="REST APIs", category="technical", proficiency="advanced", years=3.0),
        ExtractedSkill(skill="Problem Solving", category="soft", proficiency="advanced"),
        ExtractedSkill(skill="Leadership", category="soft", proficiency="intermediate"),
    ]

    SENIOR_SKILLS = [
        ExtractedSkill(skill="Python", category="technical", proficiency="expert", years=7.0),
        ExtractedSkill(skill="System Design", category="technical", proficiency="expert", years=5.0),
        ExtractedSkill(skill="Microservices", category="technical", proficiency="advanced", years=4.0),
        ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="expert", years=6.0),
        ExtractedSkill(skill="AWS", category="technical", proficiency="advanced", years=4.5),
        ExtractedSkill(skill="Architecture", category="technical", proficiency="expert", years=5.5),
        ExtractedSkill(skill="Leadership", category="soft", proficiency="expert"),
        ExtractedSkill(skill="Mentoring", category="soft", proficiency="advanced"),
    ]

    SUPPORTED_EXTENSIONS = {".pdf", ".doc", ".docx"}

    async def extract_text_from_file(self, file_path: str) -> str:
        """Extract mock text from file.

        Args:
            file_path: Path to CV file

        Returns:
            Mock CV text content

        Raises:
            ValueError: If file extension not supported
        """
        path = Path(file_path)
        if path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
            raise ValueError(
                f"Unsupported file format: {path.suffix}. "
                f"Supported formats: {', '.join(self.SUPPORTED_EXTENSIONS)}"
            )

        # Return realistic mock CV text
        return """
        John Doe
        Software Engineer
        Email: john.doe@example.com
        Phone: +1-555-0100

        PROFESSIONAL SUMMARY
        Experienced software engineer with strong background in backend development,
        API design, and cloud infrastructure. Passionate about clean code and scalable
        systems.

        EXPERIENCE
        Senior Software Engineer at Tech Corp (2020-Present)
        - Designed and implemented microservices architecture serving 1M+ users
        - Led team of 5 engineers in migrating monolith to event-driven architecture
        - Reduced API latency by 40% through optimization and caching strategies
        - Mentored junior developers and conducted code reviews

        Software Engineer at StartupXYZ (2018-2020)
        - Developed REST APIs using Python and FastAPI
        - Implemented PostgreSQL database schema and query optimization
        - Built CI/CD pipelines with Docker and GitHub Actions
        - Collaborated with frontend team on API integration

        TECHNICAL SKILLS
        Languages: Python, SQL, JavaScript
        Frameworks: FastAPI, Django, Flask
        Databases: PostgreSQL, Redis, MongoDB
        Cloud: AWS (EC2, S3, Lambda), Docker, Kubernetes
        Tools: Git, GitHub Actions, Terraform

        EDUCATION
        Bachelor of Science in Computer Science
        University of Technology, 2018

        CERTIFICATIONS
        - AWS Certified Solutions Architect
        - Python Professional Certification
        """

    async def analyze_cv(
        self,
        cv_file_path: str,
        candidate_id: str,
    ) -> CVAnalysis:
        """Analyze CV and extract structured information.

        Args:
            cv_file_path: Path to CV file
            candidate_id: UUID of the candidate

        Returns:
            CVAnalysis with extracted skills and metadata
        """
        # Extract text from file
        extracted_text = await self.extract_text_from_file(cv_file_path)

        # Detect experience level from filename
        filename = Path(cv_file_path).stem.lower()
        if "junior" in filename:
            experience_level = "junior"
            skills = self.JUNIOR_SKILLS[:3]  # 2-3 skills
            years = random.uniform(1.0, 2.0)
            difficulty = "easy"
            education = "Bachelor's"
        elif "senior" in filename:
            experience_level = "senior"
            skills = self.SENIOR_SKILLS[:6]  # 5-6 skills
            years = random.uniform(6.0, 10.0)
            difficulty = "hard"
            education = "Master's"
        else:
            experience_level = "mid"
            skills = self.MID_SKILLS[:5]  # 4-5 skills
            years = random.uniform(3.0, 5.0)
            difficulty = "medium"
            education = "Bachelor's"

        # Generate suggested topics from skills
        technical_skills = [s for s in skills if s.category == "technical"]
        suggested_topics = [skill.name for skill in technical_skills]

        # Add some topic variations
        if any(s.name in ["Python", "FastAPI"] for s in skills):
            suggested_topics.append("Backend Development")
        if any(s.name in ["PostgreSQL", "SQL"] for s in skills):
            suggested_topics.append("Database Design")
        if any(s.name in ["System Design", "Architecture"] for s in skills):
            suggested_topics.append("System Architecture")

        # Create CV analysis
        return CVAnalysis(
            id=uuid4(),
            candidate_id=UUID(candidate_id),
            cv_file_path=cv_file_path,
            extracted_text=extracted_text,
            skills=skills,
            work_experience_years=round(years, 1),
            education_level=education,
            suggested_topics=suggested_topics[:5],  # Limit to 5 topics
            suggested_difficulty=difficulty,
            embedding=None,  # Mock doesn't generate embeddings
            summary=f"Mock CV analysis: {experience_level.title()}-level candidate with {years:.1f} years of experience",
            metadata={
                "experience_level": experience_level,
                "file_name": Path(cv_file_path).name,
                "mock_adapter": True,
            },
        )
</file>

<file path="src/adapters/persistence/evaluation_repository.py">
"""PostgreSQL evaluation repository implementation."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from ...domain.models.evaluation import Evaluation, ConceptGap, GapSeverity
from ...domain.ports.evaluation_repository_port import EvaluationRepositoryPort
from .models import EvaluationModel, EvaluationGapModel


class PostgreSQLEvaluationRepository(EvaluationRepositoryPort):
    """PostgreSQL implementation of evaluation repository."""

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, evaluation: Evaluation) -> Evaluation:
        """Save evaluation with gaps."""
        # Convert domain model to database model
        db_model = self._to_db_model(evaluation)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)

        # Load gaps relationship
        await self.session.refresh(db_model, ["gaps"])

        return self._to_domain(db_model)

    async def get_by_id(self, evaluation_id: UUID) -> Evaluation | None:
        """Get evaluation by ID with gaps."""
        stmt = select(EvaluationModel).where(EvaluationModel.id == evaluation_id)
        stmt = stmt.options(selectinload(EvaluationModel.gaps))
        result = await self.session.execute(stmt)
        db_model = result.scalar_one_or_none()
        return self._to_domain(db_model) if db_model else None

    async def get_by_answer_id(self, answer_id: UUID) -> Evaluation | None:
        """Get evaluation for answer with gaps."""
        stmt = select(EvaluationModel).where(EvaluationModel.answer_id == answer_id)
        stmt = stmt.options(selectinload(EvaluationModel.gaps))
        result = await self.session.execute(stmt)
        db_model = result.scalar_one_or_none()
        return self._to_domain(db_model) if db_model else None

    async def get_by_parent_evaluation_id(
        self, parent_id: UUID
    ) -> list[Evaluation]:
        """Get follow-up evaluations ordered by attempt_number."""
        stmt = (
            select(EvaluationModel)
            .where(EvaluationModel.parent_evaluation_id == parent_id)
            .options(selectinload(EvaluationModel.gaps))
            .order_by(EvaluationModel.attempt_number)
        )
        result = await self.session.execute(stmt)
        return [self._to_domain(row) for row in result.scalars().all()]

    async def update(self, evaluation: Evaluation) -> Evaluation:
        """Update evaluation."""
        stmt = select(EvaluationModel).where(EvaluationModel.id == evaluation.id)
        result = await self.session.execute(stmt)
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Evaluation {evaluation.id} not found")

        # Update fields
        db_model.raw_score = evaluation.raw_score
        db_model.penalty = evaluation.penalty
        db_model.final_score = evaluation.final_score
        db_model.similarity_score = evaluation.similarity_score
        db_model.completeness = evaluation.completeness
        db_model.relevance = evaluation.relevance
        db_model.sentiment = evaluation.sentiment
        db_model.reasoning = evaluation.reasoning
        db_model.strengths = evaluation.strengths
        db_model.weaknesses = evaluation.weaknesses
        db_model.improvement_suggestions = evaluation.improvement_suggestions
        db_model.attempt_number = evaluation.attempt_number
        db_model.parent_evaluation_id = evaluation.parent_evaluation_id
        db_model.evaluated_at = evaluation.evaluated_at

        # Update gaps (replace all)
        # Delete existing gaps
        await self.session.execute(
            select(EvaluationGapModel).where(
                EvaluationGapModel.evaluation_id == evaluation.id
            )
        )
        for gap_model in list(db_model.gaps):
            await self.session.delete(gap_model)

        # Add new gaps
        for gap in evaluation.gaps:
            gap_model = EvaluationGapModel(
                id=gap.id,
                evaluation_id=evaluation.id,
                concept=gap.concept,
                severity=gap.severity.value,
                resolved=gap.resolved,
                created_at=gap.created_at,
            )
            db_model.gaps.append(gap_model)

        await self.session.commit()
        await self.session.refresh(db_model, ["gaps"])
        return self._to_domain(db_model)

    async def delete(self, evaluation_id: UUID) -> None:
        """Delete evaluation (cascade deletes gaps)."""
        stmt = select(EvaluationModel).where(EvaluationModel.id == evaluation_id)
        result = await self.session.execute(stmt)
        db_model = result.scalar_one_or_none()

        if db_model:
            await self.session.delete(db_model)
            await self.session.commit()

    def _to_domain(self, db_model: EvaluationModel) -> Evaluation:
        """Convert database model to domain model."""
        gaps = [
            ConceptGap(
                id=gap.id,
                evaluation_id=gap.evaluation_id,
                concept=gap.concept,
                severity=GapSeverity(gap.severity),
                resolved=gap.resolved,
                created_at=gap.created_at,
            )
            for gap in db_model.gaps
        ]

        return Evaluation(
            id=db_model.id,
            answer_id=db_model.answer_id,
            question_id=db_model.question_id,
            interview_id=db_model.interview_id,
            raw_score=db_model.raw_score,
            penalty=db_model.penalty,
            final_score=db_model.final_score,
            similarity_score=db_model.similarity_score,
            completeness=db_model.completeness,
            relevance=db_model.relevance,
            sentiment=db_model.sentiment,
            reasoning=db_model.reasoning,
            strengths=list(db_model.strengths),
            weaknesses=list(db_model.weaknesses),
            improvement_suggestions=list(db_model.improvement_suggestions),
            attempt_number=db_model.attempt_number,
            parent_evaluation_id=db_model.parent_evaluation_id,
            gaps=gaps,
            created_at=db_model.created_at,
            evaluated_at=db_model.evaluated_at,
        )

    def _to_db_model(self, domain_model: Evaluation) -> EvaluationModel:
        """Convert domain model to database model."""
        gap_models = [
            EvaluationGapModel(
                id=gap.id,
                evaluation_id=domain_model.id,
                concept=gap.concept,
                severity=gap.severity.value,
                resolved=gap.resolved,
                created_at=gap.created_at,
            )
            for gap in domain_model.gaps
        ]

        return EvaluationModel(
            id=domain_model.id,
            answer_id=domain_model.answer_id,
            question_id=domain_model.question_id,
            interview_id=domain_model.interview_id,
            raw_score=domain_model.raw_score,
            penalty=domain_model.penalty,
            final_score=domain_model.final_score,
            similarity_score=domain_model.similarity_score,
            completeness=domain_model.completeness,
            relevance=domain_model.relevance,
            sentiment=domain_model.sentiment,
            reasoning=domain_model.reasoning,
            strengths=domain_model.strengths,
            weaknesses=domain_model.weaknesses,
            improvement_suggestions=domain_model.improvement_suggestions,
            attempt_number=domain_model.attempt_number,
            parent_evaluation_id=domain_model.parent_evaluation_id,
            gaps=gap_models,
            created_at=domain_model.created_at,
            evaluated_at=domain_model.evaluated_at,
        )
</file>

<file path="src/adapters/persistence/follow_up_question_repository.py">
"""PostgreSQL implementation of FollowUpQuestionRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.follow_up_question import FollowUpQuestion
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from .mappers import FollowUpQuestionMapper
from .models import FollowUpQuestionModel


class PostgreSQLFollowUpQuestionRepository(FollowUpQuestionRepositoryPort):
    """PostgreSQL implementation of follow-up question repository.

    This adapter implements the FollowUpQuestionRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, follow_up_question: FollowUpQuestion) -> FollowUpQuestion:
        """Save a new follow-up question to the database.

        Args:
            follow_up_question: FollowUpQuestion to save

        Returns:
            Saved follow-up question
        """
        db_model = FollowUpQuestionMapper.to_db_model(follow_up_question)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return FollowUpQuestionMapper.to_domain(db_model)

    async def get_by_id(self, question_id: UUID) -> FollowUpQuestion | None:
        """Retrieve a follow-up question by ID.

        Args:
            question_id: Follow-up question identifier

        Returns:
            FollowUpQuestion if found, None otherwise
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(FollowUpQuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()
        return FollowUpQuestionMapper.to_domain(db_model) if db_model else None

    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            List of follow-up questions ordered by order_in_sequence
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel)
            .where(FollowUpQuestionModel.parent_question_id == parent_question_id)
            .order_by(FollowUpQuestionModel.order_in_sequence)
        )
        db_models = result.scalars().all()
        return [FollowUpQuestionMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_interview_id(self, interview_id: UUID) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of follow-up questions ordered by created_at
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel)
            .where(FollowUpQuestionModel.interview_id == interview_id)
            .order_by(FollowUpQuestionModel.created_at)
        )
        db_models = result.scalars().all()
        return [FollowUpQuestionMapper.to_domain(db_model) for db_model in db_models]

    async def count_by_parent_question_id(self, parent_question_id: UUID) -> int:
        """Count follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            Count of follow-up questions
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(
                FollowUpQuestionModel.parent_question_id == parent_question_id
            )
        )
        db_models = result.scalars().all()
        return len(db_models)

    async def delete(self, question_id: UUID) -> bool:
        """Delete a follow-up question.

        Args:
            question_id: Follow-up question identifier

        Returns:
            True if deleted, False if not found
        """
        result = await self.session.execute(
            select(FollowUpQuestionModel).where(FollowUpQuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()

        if db_model:
            await self.session.delete(db_model)
            await self.session.commit()
            return True
        return False
</file>

<file path="src/adapters/speech/azure_stt_adapter.py">
"""Azure Speech-to-Text adapter implementation."""

import asyncio
import json
import logging
from typing import Any

import azure.cognitiveservices.speech as speechsdk

from ...domain.ports.speech_to_text_port import SpeechToTextPort

logger = logging.getLogger(__name__)


class AzureSpeechToTextAdapter(SpeechToTextPort):
    """Azure Speech SDK implementation for speech-to-text with voice metrics.

    This adapter uses Azure Cognitive Services Speech SDK to transcribe
    audio and extract voice quality metrics (intonation, fluency, confidence).
    """

    def __init__(
        self,
        api_key: str,
        region: str,
        language: str = "en-US",
    ):
        """Initialize Azure STT adapter.

        Args:
            api_key: Azure Speech Services API key
            region: Azure region (e.g., "eastus", "westus")
            language: Default language code (e.g., "en-US")
        """
        self.api_key = api_key
        self.region = region
        self.default_language = language

        # Create speech config
        self.speech_config = speechsdk.SpeechConfig(
            subscription=api_key,
            region=region,
        )
        self.speech_config.speech_recognition_language = language

        # Enable detailed result
        self.speech_config.output_format = speechsdk.OutputFormat.Detailed

        logger.info(f"Initialized Azure STT adapter (region={region}, language={language})")

    async def transcribe_audio(
        self,
        audio_bytes: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Transcribe audio bytes to text with voice metrics.

        Args:
            audio_bytes: Audio data as bytes (WAV/PCM format, 16kHz mono)
            language: Language code

        Returns:
            Dict with text, voice_metrics, and metadata

        Raises:
            ValueError: If speech recognition fails
        """
        # Run sync Azure SDK in thread pool
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            self._transcribe_sync,
            audio_bytes,
            language,
        )

        return result

    async def transcribe_stream(
        self,
        audio_stream: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Transcribe streaming audio to text with voice metrics.

        Args:
            audio_stream: Audio data stream
            language: Language code

        Returns:
            Same dict structure as transcribe_audio()
        """
        # For now, treat stream as complete audio
        return await self.transcribe_audio(audio_stream, language)

    async def detect_language(
        self,
        audio_bytes: bytes,
    ) -> str | None:
        """Detect language from audio bytes.

        Args:
            audio_bytes: Audio data as bytes

        Returns:
            Detected language code or None
        """
        # Azure SDK has language detection, but for simplicity return default
        # In production, implement proper language detection
        return self.default_language

    def _transcribe_sync(
        self,
        audio_bytes: bytes,
        language: str,
    ) -> dict[str, Any]:
        """Synchronous transcription using Azure SDK.

        Args:
            audio_bytes: Audio data as bytes
            language: Language code

        Returns:
            Dict with text, voice_metrics, and metadata
        """
        try:
            # Create push stream
            stream = speechsdk.audio.PushAudioInputStream()

            # Write audio data to stream
            stream.write(audio_bytes)
            stream.close()

            # Create audio config from stream
            audio_config = speechsdk.audio.AudioConfig(stream=stream)

            # Update language if different from default
            if language != self.default_language:
                self.speech_config.speech_recognition_language = language

            # Create recognizer
            recognizer = speechsdk.SpeechRecognizer(
                speech_config=self.speech_config,
                audio_config=audio_config,
            )

            # Perform recognition
            result = recognizer.recognize_once()

            # Check result
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                # Extract text and metrics
                text = result.text
                voice_metrics = self._calculate_voice_metrics(result, audio_bytes)
                duration_seconds = result.duration.total_seconds()

                logger.info(f"Transcribed {len(text)} chars, duration={duration_seconds:.2f}s")

                return {
                    "text": text,
                    "voice_metrics": voice_metrics,
                    "metadata": {
                        "duration_seconds": duration_seconds,
                        "audio_format": "wav",
                    }
                }

            elif result.reason == speechsdk.ResultReason.NoMatch:
                logger.warning("No speech detected in audio")
                raise ValueError("No speech detected in audio")

            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation = result.cancellation_details
                logger.error(f"Speech recognition canceled: {cancellation.reason}")
                if cancellation.reason == speechsdk.CancellationReason.Error:
                    logger.error(f"Error details: {cancellation.error_details}")
                raise ValueError(f"Speech recognition failed: {cancellation.error_details}")

            else:
                raise ValueError(f"Unexpected result reason: {result.reason}")

        except Exception as e:
            logger.error(f"Error during speech recognition: {str(e)}")
            raise

    def _calculate_voice_metrics(
        self,
        result: speechsdk.SpeechRecognitionResult,
        audio_bytes: bytes,
    ) -> dict[str, float]:
        """Calculate voice quality metrics from Azure recognition result.

        Args:
            result: Azure speech recognition result
            audio_bytes: Original audio bytes

        Returns:
            Dict with intonation, fluency, confidence scores and speaking rate
        """
        try:
            # Get detailed JSON result with prosody data
            json_result = result.properties.get(
                speechsdk.PropertyId.SpeechServiceResponse_JsonResult
            )

            if json_result:
                data = json.loads(json_result)
                # Extract confidence from detailed result
                confidence = data.get("NBest", [{}])[0].get("Confidence", 0.8)
            else:
                # Fallback to basic confidence
                confidence = 0.8  # Default if not available

            # Calculate metrics
            text = result.text
            duration_seconds = result.duration.total_seconds()

            # Intonation score (pitch variance) - estimated from confidence and length
            # Higher confidence + longer speech = better intonation
            intonation_score = min(confidence + (duration_seconds / 30.0) * 0.1, 1.0)

            # Fluency score based on speaking rate
            word_count = len(text.split())
            speaking_rate_wpm = int((word_count / duration_seconds) * 60) if duration_seconds > 0 else 0

            # Optimal speaking rate: 120-180 WPM
            # Calculate fluency based on how close to optimal range
            if 120 <= speaking_rate_wpm <= 180:
                fluency_score = 0.9 + (confidence * 0.1)  # High fluency in optimal range
            elif 90 <= speaking_rate_wpm < 120:
                fluency_score = 0.7 + ((speaking_rate_wpm - 90) / 30.0) * 0.2
            elif 180 < speaking_rate_wpm <= 220:
                fluency_score = 0.7 + ((220 - speaking_rate_wpm) / 40.0) * 0.2
            else:
                fluency_score = 0.5  # Too slow or too fast

            # Clamp scores to [0, 1]
            intonation_score = max(0.0, min(intonation_score, 1.0))
            fluency_score = max(0.0, min(fluency_score, 1.0))
            confidence_score = max(0.0, min(confidence, 1.0))

            return {
                "intonation_score": round(intonation_score, 3),
                "fluency_score": round(fluency_score, 3),
                "confidence_score": round(confidence_score, 3),
                "speaking_rate_wpm": speaking_rate_wpm,
            }

        except Exception as e:
            logger.warning(f"Error calculating voice metrics, using defaults: {e}")
            # Return default metrics if calculation fails
            return {
                "intonation_score": 0.7,
                "fluency_score": 0.7,
                "confidence_score": 0.7,
                "speaking_rate_wpm": 150,
            }
</file>

<file path="src/adapters/speech/azure_tts_adapter.py">
"""Azure Text-to-Speech adapter implementation."""

import asyncio
import logging

import azure.cognitiveservices.speech as speechsdk

from ...domain.ports.text_to_speech_port import TextToSpeechPort

logger = logging.getLogger(__name__)


class AzureTextToSpeechAdapter(TextToSpeechPort):
    """Azure Speech SDK implementation for text-to-speech.

    This adapter uses Azure Cognitive Services Speech SDK to synthesize
    speech from text with configurable voices and speaking rates.

    Includes LRU caching for frequently synthesized texts to reduce API calls.
    """

    def __init__(
        self,
        api_key: str,
        region: str,
        default_voice: str = "en-US-AriaNeural",
        cache_size: int = 128,
    ):
        """Initialize Azure TTS adapter.

        Args:
            api_key: Azure Speech Services API key
            region: Azure region (e.g., "eastus", "westus")
            default_voice: Default voice name (e.g., "en-US-AriaNeural")
            cache_size: Max number of cached audio outputs (default 128)
        """
        self.api_key = api_key
        self.region = region
        self.default_voice = default_voice
        self.cache_size = cache_size

        # Create speech config
        self.speech_config = speechsdk.SpeechConfig(
            subscription=api_key,
            region=region,
        )
        self.speech_config.speech_synthesis_voice_name = default_voice

        # Set output format to WAV 16kHz mono
        self.speech_config.set_speech_synthesis_output_format(
            speechsdk.SpeechSynthesisOutputFormat.Riff16Khz16BitMonoPcm
        )

        logger.info(f"Initialized Azure TTS adapter (region={region}, voice={default_voice})")

    async def synthesize_speech(
        self,
        text: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> bytes:
        """Convert text to speech audio.

        Args:
            text: Text to synthesize
            voice: Voice name (e.g., "en-US-AriaNeural")
            speed: Speaking rate multiplier (0.5-2.0)

        Returns:
            WAV audio bytes (16kHz mono)

        Raises:
            ValueError: If speech synthesis fails
        """
        # Check cache first
        cache_key: tuple[str, str, float] = (text, voice, speed)
        cached_audio = self._get_from_cache(cache_key)
        if cached_audio:
            logger.debug(f"Cache hit for text (length={len(text)})")
            return cached_audio

        # Run sync Azure SDK in thread pool
        loop = asyncio.get_event_loop()
        audio_bytes = await loop.run_in_executor(
            None,
            self._synthesize_sync,
            text,
            voice,
            speed,
        )

        # Cache the result
        self._add_to_cache(cache_key, audio_bytes)

        return audio_bytes

    async def save_speech_to_file(
        self,
        text: str,
        output_path: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> str:
        """Convert text to speech and save to file.

        Args:
            text: Text to synthesize
            output_path: Path where audio file should be saved
            voice: Voice name
            speed: Speaking rate multiplier

        Returns:
            Path to saved audio file
        """
        audio_bytes = await self.synthesize_speech(text, voice, speed)

        with open(output_path, "wb") as f:
            f.write(audio_bytes)

        logger.info(f"Saved audio to {output_path} ({len(audio_bytes)} bytes)")
        return output_path

    async def get_available_voices(self) -> list[str]:
        """Get list of available voice names.

        Returns:
            List of voice name strings

        Note:
            In production, this would query Azure's voice list API.
            For now, returns commonly used neural voices.
        """
        # Commonly available Azure Neural voices
        return [
            "en-US-AriaNeural",
            "en-US-JennyNeural",
            "en-US-GuyNeural",
            "en-US-DavisNeural",
            "en-GB-SoniaNeural",
            "en-GB-RyanNeural",
            "en-AU-NatashaNeural",
            "en-AU-WilliamNeural",
        ]

    def _synthesize_sync(
        self,
        text: str,
        voice: str,
        speed: float,
    ) -> bytes:
        """Synchronous speech synthesis using Azure SDK.

        Args:
            text: Text to synthesize
            voice: Voice name
            speed: Speaking rate multiplier

        Returns:
            WAV audio bytes

        Raises:
            ValueError: If synthesis fails
        """
        try:
            # Update voice if different from default
            if voice != self.default_voice:
                self.speech_config.speech_synthesis_voice_name = voice

            # Create synthesizer for in-memory synthesis
            synthesizer = speechsdk.SpeechSynthesizer(
                speech_config=self.speech_config,
                audio_config=None,  # Use in-memory synthesis
            )

            # Build SSML with speed adjustment
            ssml = self._build_ssml(text, voice, speed)

            # Synthesize speech
            result = synthesizer.speak_ssml_async(ssml).get()

            # Check result
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                audio_bytes = result.audio_data
                logger.info(f"Synthesized {len(text)} chars â†’ {len(audio_bytes)} bytes")
                return audio_bytes

            elif result.reason == speechsdk.ResultReason.Canceled:
                cancellation = result.cancellation_details
                logger.error(f"Speech synthesis canceled: {cancellation.reason}")
                if cancellation.reason == speechsdk.CancellationReason.Error:
                    logger.error(f"Error details: {cancellation.error_details}")
                raise ValueError(f"Speech synthesis failed: {cancellation.error_details}")

            else:
                raise ValueError(f"Unexpected result reason: {result.reason}")

        except Exception as e:
            logger.error(f"Error during speech synthesis: {str(e)}")
            raise

    def _build_ssml(self, text: str, voice: str, speed: float) -> str:
        """Build SSML (Speech Synthesis Markup Language) with speed control.

        Args:
            text: Text to synthesize
            voice: Voice name
            speed: Speaking rate multiplier (0.5-2.0)

        Returns:
            SSML string
        """
        # Clamp speed to valid range
        speed = max(0.5, min(speed, 2.0))

        # Convert speed to percentage (1.0 = +0%, 1.5 = +50%, 0.5 = -50%)
        speed_percent = int((speed - 1.0) * 100)
        speed_str = f"{speed_percent:+d}%" if speed_percent != 0 else "0%"

        ssml = f"""
        <speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xml:lang="en-US">
            <voice name="{voice}">
                <prosody rate="{speed_str}">
                    {text}
                </prosody>
            </voice>
        </speak>
        """

        return ssml.strip()

    def _get_from_cache(self, cache_key: tuple[str, str, float]) -> bytes | None:
        """Get audio from LRU cache.

        Args:
            cache_key: Tuple of (text, voice, speed)

        Returns:
            Cached audio bytes or None

        Note:
            In production, implement proper caching (Redis, memcached, etc.)
            For now, returns None (no caching implemented)
        """
        return None

    def _add_to_cache(self, cache_key: tuple[str, str, float], audio_bytes: bytes) -> None:
        """Add audio to LRU cache.

        Args:
            cache_key: Tuple of (text, voice, speed)
            audio_bytes: Audio bytes to cache

        Note:
            In production, implement proper caching (Redis, memcached, etc.)
            For now, this is a no-op.
        """
        pass
</file>

<file path="src/adapters/vector_db/__init__.py">
"""Vector database adapters package."""

from .pinecone_adapter import PineconeAdapter

__all__ = ["PineconeAdapter"]
</file>

<file path="src/application/__init__.py">
"""Application layer package."""
</file>

<file path="src/application/dto/answer_dto.py">
"""Answer DTOs for REST API request/response."""

from uuid import UUID

from pydantic import BaseModel


class SubmitAnswerRequest(BaseModel):
    """Request to submit an answer to a question."""
    question_id: UUID
    answer_text: str
    audio_file_path: str | None = None


class AnswerEvaluationResponse(BaseModel):
    """Response with answer evaluation results."""
    answer_id: UUID
    question_id: UUID
    score: float
    feedback: str
    strengths: list[str]
    weaknesses: list[str]
    improvement_suggestions: list[str]
    next_question_available: bool
</file>

<file path="src/application/dto/audio_dto.py">
"""Audio-related Data Transfer Objects."""

from pydantic import BaseModel, Field


class AudioChunkDTO(BaseModel):
    """Audio chunk for WebSocket transmission."""

    audio_data: str = Field(..., description="Base64-encoded audio bytes")
    chunk_index: int = Field(..., description="Sequential chunk number")
    is_final: bool = Field(..., description="Whether this is the last chunk")
    format: str = Field(default="webm", description="Audio format (webm, wav, mp3)")


class VoiceMetricsDTO(BaseModel):
    """Voice analysis results from speech-to-text."""

    intonation_score: float = Field(
        ..., ge=0.0, le=1.0, description="Pitch variance score (0-1)"
    )
    fluency_score: float = Field(
        ..., ge=0.0, le=1.0, description="Speaking fluency score (0-1)"
    )
    confidence_score: float = Field(
        ..., ge=0.0, le=1.0, description="Speech recognition confidence (0-1)"
    )
    speaking_rate_wpm: int = Field(..., ge=0, description="Words per minute")
    duration_seconds: float = Field(..., ge=0.0, description="Audio duration in seconds")


class TranscriptionResultDTO(BaseModel):
    """Complete transcription result with text and metrics."""

    text: str = Field(..., description="Transcribed text")
    voice_metrics: VoiceMetricsDTO = Field(..., description="Voice analysis metrics")
    metadata: dict = Field(default_factory=dict, description="Additional metadata")
</file>

<file path="src/application/use_cases/combine_evaluation.py">
"""Combine theoretical and speaking evaluations into overall score."""

import logging
from typing import Any

from ...domain.models.answer import AnswerEvaluation

logger = logging.getLogger(__name__)


class CombineEvaluationUseCase:
    """Combine theoretical (semantic) and speaking (voice) evaluations.

    This use case implements weighted scoring to produce an overall
    interview performance score combining both content quality and
    speaking delivery.

    Weights:
    - Theoretical: 70% (semantic similarity, completeness, relevance)
    - Speaking: 30% (intonation, fluency, confidence)
    """

    def __init__(
        self,
        theoretical_weight: float = 0.7,
        speaking_weight: float = 0.3,
    ):
        """Initialize combine evaluation use case.

        Args:
            theoretical_weight: Weight for theoretical evaluation (default 0.7)
            speaking_weight: Weight for speaking evaluation (default 0.3)
        """
        if abs(theoretical_weight + speaking_weight - 1.0) > 0.01:
            raise ValueError("Weights must sum to 1.0")

        self.theoretical_weight = theoretical_weight
        self.speaking_weight = speaking_weight

    def execute(
        self,
        theoretical_eval: AnswerEvaluation,
        voice_metrics: dict[str, float] | None = None,
    ) -> dict[str, Any]:
        """Combine theoretical and speaking evaluations.

        Args:
            theoretical_eval: Semantic evaluation from LLM
            voice_metrics: Voice quality metrics from STT (optional)

        Returns:
            Dict with combined evaluation:
            {
                "overall_score": float (0-100),
                "theoretical_score": float (0-100),
                "speaking_score": float | None (0-100),
                "breakdown": {...},
                "combined_feedback": str,
            }
        """
        theoretical_score = theoretical_eval.score

        # Calculate speaking score if voice metrics available
        if voice_metrics:
            speaking_score = self._calculate_speaking_score(voice_metrics)
            overall_score = (
                theoretical_score * self.theoretical_weight
                + speaking_score * self.speaking_weight
            )
        else:
            # Text-only answer: use 100% theoretical weight
            speaking_score = None
            overall_score = theoretical_score

        logger.info(
            f"Combined evaluation: theoretical={theoretical_score:.1f}, "
            f"speaking={speaking_score:.1f if speaking_score else 'N/A'}, "
            f"overall={overall_score:.1f}"
        )

        return {
            "overall_score": round(overall_score, 2),
            "theoretical_score": round(theoretical_score, 2),
            "speaking_score": round(speaking_score, 2) if speaking_score else None,
            "breakdown": self._build_breakdown(
                theoretical_eval, voice_metrics, theoretical_score, speaking_score
            ),
            "combined_feedback": self._generate_combined_feedback(
                theoretical_eval, voice_metrics
            ),
        }

    def _calculate_speaking_score(self, voice_metrics: dict[str, float]) -> float:
        """Calculate speaking score from voice metrics.

        Args:
            voice_metrics: Dict with intonation_score, fluency_score, confidence_score

        Returns:
            Speaking score (0-100)
        """
        # Get metrics with defaults
        intonation = voice_metrics.get("intonation_score", 0.5)
        fluency = voice_metrics.get("fluency_score", 0.5)
        confidence = voice_metrics.get("confidence_score", 0.5)

        # Average and convert to 0-100 scale
        avg_score = (intonation + fluency + confidence) / 3.0
        return avg_score * 100.0

    def _build_breakdown(
        self,
        theoretical_eval: AnswerEvaluation,
        voice_metrics: dict[str, float] | None,
        theoretical_score: float,
        speaking_score: float | None,
    ) -> dict[str, Any]:
        """Build detailed breakdown of evaluation components.

        Args:
            theoretical_eval: Semantic evaluation
            voice_metrics: Voice metrics (optional)
            theoretical_score: Calculated theoretical score
            speaking_score: Calculated speaking score (optional)

        Returns:
            Detailed breakdown dict
        """
        breakdown: dict[str, Any] = {
            "theoretical": {
                "score": round(theoretical_score, 2),
                "weight": self.theoretical_weight,
                "metrics": {
                    "semantic_similarity": round(theoretical_eval.semantic_similarity, 3),
                    "completeness": round(theoretical_eval.completeness, 3),
                    "relevance": round(theoretical_eval.relevance, 3),
                },
            }
        }

        if voice_metrics and speaking_score is not None:
            breakdown["speaking"] = {
                "score": round(speaking_score, 2),
                "weight": self.speaking_weight,
                "metrics": {
                    "intonation_score": round(voice_metrics.get("intonation_score", 0.5), 3),
                    "fluency_score": round(voice_metrics.get("fluency_score", 0.5), 3),
                    "confidence_score": round(voice_metrics.get("confidence_score", 0.5), 3),
                    "speaking_rate_wpm": voice_metrics.get("speaking_rate_wpm", 0),
                },
            }

        return breakdown

    def _generate_combined_feedback(
        self,
        theoretical_eval: AnswerEvaluation,
        voice_metrics: dict[str, float] | None,
    ) -> str:
        """Generate combined feedback message.

        Args:
            theoretical_eval: Semantic evaluation
            voice_metrics: Voice metrics (optional)

        Returns:
            Combined feedback string
        """
        feedback_parts = []

        # Theoretical feedback
        if theoretical_eval.reasoning:
            feedback_parts.append(f"Content: {theoretical_eval.reasoning}")

        # Speaking feedback
        if voice_metrics:
            speaking_feedback = self._generate_speaking_feedback(voice_metrics)
            feedback_parts.append(f"Delivery: {speaking_feedback}")

        return " | ".join(feedback_parts) if feedback_parts else "Evaluation complete."

    def _generate_speaking_feedback(self, voice_metrics: dict[str, float]) -> str:
        """Generate feedback for speaking delivery.

        Args:
            voice_metrics: Voice quality metrics

        Returns:
            Speaking feedback string
        """
        intonation = voice_metrics.get("intonation_score", 0.5)
        fluency = voice_metrics.get("fluency_score", 0.5)
        confidence = voice_metrics.get("confidence_score", 0.5)

        feedback_parts = []

        if intonation >= 0.8:
            feedback_parts.append("excellent voice expression")
        elif intonation >= 0.6:
            feedback_parts.append("good voice modulation")
        else:
            feedback_parts.append("work on voice variety")

        if fluency >= 0.8:
            feedback_parts.append("smooth delivery")
        elif fluency >= 0.6:
            feedback_parts.append("adequate fluency")
        else:
            feedback_parts.append("practice speaking more fluently")

        if confidence >= 0.8:
            feedback_parts.append("confident tone")
        elif confidence >= 0.6:
            feedback_parts.append("moderate confidence")
        else:
            feedback_parts.append("build speaking confidence")

        return ", ".join(feedback_parts)
</file>

<file path="src/application/use_cases/generate_summary.py">
"""Generate comprehensive interview summary use case."""

from datetime import datetime, timezone
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.models.interview import Interview
from ...domain.models.question import Question
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort


class GenerateSummaryUseCase:
    """Generate comprehensive interview summary.

    Aggregates all evaluations (main questions + follow-ups), analyzes gap
    progression, and generates personalized recommendations using LLM.
    """

    def __init__(
        self,
        interview_repository: InterviewRepositoryPort,
        answer_repository: AnswerRepositoryPort,
        question_repository: QuestionRepositoryPort,
        follow_up_question_repository: FollowUpQuestionRepositoryPort,
        llm: LLMPort,
    ):
        self.interview_repo = interview_repository
        self.answer_repo = answer_repository
        self.question_repo = question_repository
        self.follow_up_repo = follow_up_question_repository
        self.llm = llm

    async def execute(self, interview_id: UUID) -> dict[str, Any]:
        """Generate comprehensive summary.

        Args:
            interview_id: Interview UUID

        Returns:
            dict containing:
                - interview_id: UUID
                - overall_score: float (weighted avg of all evaluations)
                - theoretical_score_avg: float
                - speaking_score_avg: float
                - total_questions: int (main questions only)
                - total_follow_ups: int
                - question_summaries: list[dict] (per-question analysis)
                - gap_progression: dict (how gaps changed after follow-ups)
                - strengths: list[str]
                - weaknesses: list[str]
                - study_recommendations: list[str]
                - technique_tips: list[str]
                - completion_time: datetime

        Raises:
            ValueError: If interview not found
        """
        # Fetch interview
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        # Fetch all answers (main + follow-ups)
        all_answers = await self.answer_repo.get_by_interview_id(interview_id)

        # Group answers by main question
        question_groups = await self._group_answers_by_main_question(interview, all_answers)

        # Calculate aggregate metrics
        metrics = self._calculate_aggregate_metrics(all_answers)

        # Analyze gap progression
        gap_progression = await self._analyze_gap_progression(question_groups)

        # Generate LLM-powered recommendations
        recommendations = await self._generate_recommendations(
            interview, all_answers, gap_progression
        )

        return {
            "interview_id": str(interview_id),
            "overall_score": metrics["overall_score"],
            "theoretical_score_avg": metrics["theoretical_avg"],
            "speaking_score_avg": metrics["speaking_avg"],
            "total_questions": len(interview.question_ids),
            "total_follow_ups": len(interview.adaptive_follow_ups),
            "question_summaries": await self._create_question_summaries(question_groups),
            "gap_progression": gap_progression,
            "strengths": recommendations["strengths"],
            "weaknesses": recommendations["weaknesses"],
            "study_recommendations": recommendations["study_topics"],
            "technique_tips": recommendations["technique_tips"],
            "completion_time": datetime.now(timezone.utc).isoformat(),
        }

    async def _group_answers_by_main_question(
        self,
        interview: Interview,
        all_answers: list[Answer],
    ) -> dict[UUID, dict[str, Any]]:
        """Group answers by main question with follow-ups.

        Args:
            interview: Interview entity
            all_answers: All answers (main + follow-ups)

        Returns:
            Dict mapping main_question_id to:
                - question: Main question entity
                - main_answer: Answer to main question
                - follow_ups: List of follow-up question entities
                - follow_up_answers: List of answers to follow-ups
        """
        groups = {}

        for main_question_id in interview.question_ids:
            main_question = await self.question_repo.get_by_id(main_question_id)

            # Find main answer
            main_answer = next(
                (a for a in all_answers if a.question_id == main_question_id),
                None,
            )

            # Find follow-up answers
            follow_ups = await self.follow_up_repo.get_by_parent_question_id(main_question_id)
            follow_up_answers = [
                next((a for a in all_answers if a.question_id == fu.id), None) for fu in follow_ups
            ]

            groups[main_question_id] = {
                "question": main_question,
                "main_answer": main_answer,
                "follow_ups": follow_ups,
                "follow_up_answers": [a for a in follow_up_answers if a is not None],
            }

        return groups

    def _calculate_aggregate_metrics(self, all_answers: list[Answer]) -> dict[str, float]:
        """Calculate aggregate scores.

        Args:
            all_answers: All answers (main + follow-ups)

        Returns:
            Dict with keys:
                - overall_score: Weighted average (70% theoretical + 30% speaking)
                - theoretical_avg: Average of theoretical scores
                - speaking_avg: Average of speaking scores
        """
        evaluated_answers = [a for a in all_answers if a.is_evaluated()]

        if not evaluated_answers:
            return {
                "overall_score": 0.0,
                "theoretical_avg": 0.0,
                "speaking_avg": 0.0,
            }

        # Theoretical score (from evaluation)
        theoretical_scores = [
            a.evaluation.score for a in evaluated_answers if a.evaluation is not None
        ]

        # Speaking score (from voice metrics)
        speaking_scores = [
            a.voice_metrics.get("overall_score", 50.0) for a in evaluated_answers if a.voice_metrics
        ]

        # If no voice metrics, default to 50.0
        speaking_avg = sum(speaking_scores) / len(speaking_scores) if speaking_scores else 50.0

        theoretical_avg = sum(theoretical_scores) / len(theoretical_scores)

        # Overall = 70% theoretical + 30% speaking
        overall_score = (theoretical_avg * 0.7) + (speaking_avg * 0.3)

        return {
            "overall_score": round(overall_score, 2),
            "theoretical_avg": round(theoretical_avg, 2),
            "speaking_avg": round(speaking_avg, 2),
        }

    async def _analyze_gap_progression(
        self, question_groups: dict[UUID, dict[str, Any]]
    ) -> dict[str, Any]:
        """Analyze how gaps changed after follow-ups.

        Args:
            question_groups: Grouped answers by main question

        Returns:
            Dict with keys:
                - questions_with_followups: int
                - gaps_filled: int (concepts mastered after follow-ups)
                - gaps_remaining: int (concepts still missing)
                - avg_followups_per_question: float
        """
        progression = {
            "questions_with_followups": 0,
            "gaps_filled": 0,
            "gaps_remaining": 0,
            "avg_followups_per_question": 0.0,
        }

        questions_with_followups = 0
        total_followups = 0
        gaps_filled = 0
        gaps_remaining = 0

        for group in question_groups.values():
            main_answer = group["main_answer"]
            follow_up_answers = group["follow_up_answers"]

            if not follow_up_answers or not main_answer:
                continue

            questions_with_followups += 1
            total_followups += len(follow_up_answers)

            # Compare initial gaps vs final gaps
            initial_gaps = set(main_answer.gaps.get("concepts", []) if main_answer.gaps else [])
            final_answer = follow_up_answers[-1]
            final_gaps = set(final_answer.gaps.get("concepts", []) if final_answer.gaps else [])

            gaps_filled += len(initial_gaps - final_gaps)
            gaps_remaining += len(final_gaps)

        progression["questions_with_followups"] = questions_with_followups
        progression["gaps_filled"] = gaps_filled
        progression["gaps_remaining"] = gaps_remaining
        progression["avg_followups_per_question"] = (
            round(total_followups / questions_with_followups, 2)
            if questions_with_followups > 0
            else 0.0
        )

        return progression

    async def _generate_recommendations(
        self,
        interview: Interview,
        all_answers: list[Answer],
        gap_progression: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Use LLM to generate personalized recommendations.

        Args:
            interview: Interview entity
            all_answers: All answers
            gap_progression: Gap progression analysis

        Returns:
            Dict with keys:
                - strengths: list[str] (top 3-5 strengths)
                - weaknesses: list[str] (top 3-5 weaknesses)
                - study_topics: list[str] (topic-specific recommendations)
                - technique_tips: list[str] (voice, pacing, structure tips)
        """
        context = {
            "interview_id": str(interview.id),
            "total_answers": len(all_answers),
            "gap_progression": gap_progression,
            "evaluations": [
                {
                    "question_id": str(a.question_id),
                    "score": a.evaluation.score,
                    "strengths": a.evaluation.strengths,
                    "weaknesses": a.evaluation.weaknesses,
                }
                for a in all_answers
                if a.is_evaluated() and a.evaluation is not None
            ],
        }

        return await self.llm.generate_interview_recommendations(context)

    async def _create_question_summaries(
        self, question_groups: dict[UUID, dict[str, Any]]
    ) -> list[dict[str, Any]]:
        """Create per-question analysis.

        Args:
            question_groups: Grouped answers by main question

        Returns:
            List of dicts with keys:
                - question_id: UUID
                - question_text: str
                - main_answer_score: float
                - follow_up_count: int
                - initial_gaps: list[str]
                - final_gaps: list[str]
                - improvement: bool (whether gaps were filled)
        """
        summaries = []

        for main_question_id, group in question_groups.items():
            question = group["question"]
            main_answer = group["main_answer"]
            follow_up_answers = group["follow_up_answers"]

            initial_gaps = (
                main_answer.gaps.get("concepts", []) if main_answer and main_answer.gaps else []
            )
            final_gaps = initial_gaps

            if follow_up_answers:
                final_answer = follow_up_answers[-1]
                final_gaps = (
                    final_answer.gaps.get("concepts", [])
                    if final_answer and final_answer.gaps
                    else []
                )

            summaries.append(
                {
                    "question_id": str(main_question_id),
                    "question_text": question.text if question else "Unknown",
                    "main_answer_score": (
                        main_answer.evaluation.score
                        if main_answer and main_answer.is_evaluated()
                        else 0.0
                    ),
                    "follow_up_count": len(follow_up_answers),
                    "initial_gaps": initial_gaps,
                    "final_gaps": final_gaps,
                    "improvement": len(final_gaps) < len(initial_gaps),
                }
            )

        return summaries
</file>

<file path="src/application/use_cases/get_next_question.py">
"""Get next question use case."""

from uuid import UUID

from ...domain.models.question import Question
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort


class GetNextQuestionUseCase:
    """Get next question in interview sequence."""

    def __init__(
        self,
        interview_repository: InterviewRepositoryPort,
        question_repository: QuestionRepositoryPort,
    ):
        self.interview_repo = interview_repository
        self.question_repo = question_repository

    async def execute(self, interview_id: UUID) -> Question | None:
        """Get next unanswered question.

        Args:
            interview_id: The interview UUID

        Returns:
            Next question or None if interview complete

        Raises:
            ValueError: If interview not found
        """
        # Get interview
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        # Check if more questions available
        if not interview.has_more_questions():
            return None

        # Get current question
        question_id = interview.get_current_question_id()
        if not question_id:
            return None

        question = await self.question_repo.get_by_id(question_id)
        return question
</file>

<file path="src/domain/__init__.py">
"""Domain layer package."""
</file>

<file path="src/domain/models/error_codes.py">
"""WebSocket error codes for interview sessions."""

from enum import Enum


class WebSocketErrorCode(str, Enum):
    """Error codes for WebSocket interview communication.

    Each error code indicates a specific failure scenario and includes
    metadata about recoverability and available fallback options.
    """

    # Interview state errors
    INTERVIEW_NOT_FOUND = "INTERVIEW_NOT_FOUND"
    INTERVIEW_NOT_READY = "INTERVIEW_NOT_READY"
    INTERVIEW_ALREADY_COMPLETED = "INTERVIEW_ALREADY_COMPLETED"
    INVALID_STATE = "INVALID_STATE"
    NO_MORE_QUESTIONS = "NO_MORE_QUESTIONS"

    # Question errors
    QUESTION_NOT_FOUND = "QUESTION_NOT_FOUND"
    QUESTION_ALREADY_ANSWERED = "QUESTION_ALREADY_ANSWERED"

    # Audio processing errors (Phase 1 integration)
    AUDIO_FORMAT_UNSUPPORTED = "AUDIO_FORMAT_UNSUPPORTED"
    AUDIO_TOO_SHORT = "AUDIO_TOO_SHORT"
    AUDIO_TOO_LONG = "AUDIO_TOO_LONG"
    AUDIO_DECODING_FAILED = "AUDIO_DECODING_FAILED"
    STT_FAILED = "STT_FAILED"
    TTS_FAILED = "TTS_FAILED"
    VOICE_METRICS_UNAVAILABLE = "VOICE_METRICS_UNAVAILABLE"

    # Message errors
    UNKNOWN_MESSAGE_TYPE = "UNKNOWN_MESSAGE_TYPE"
    INVALID_MESSAGE_FORMAT = "INVALID_MESSAGE_FORMAT"
    MISSING_REQUIRED_FIELD = "MISSING_REQUIRED_FIELD"

    # General errors
    INTERNAL_ERROR = "INTERNAL_ERROR"
    TIMEOUT = "TIMEOUT"
    RATE_LIMIT_EXCEEDED = "RATE_LIMIT_EXCEEDED"
    DATABASE_ERROR = "DATABASE_ERROR"

    def is_recoverable(self) -> bool:
        """Check if error is recoverable with retry.

        Returns:
            True if client can retry the operation
        """
        recoverable_errors = {
            self.TIMEOUT,
            self.STT_FAILED,
            self.TTS_FAILED,
            self.DATABASE_ERROR,
            self.INTERNAL_ERROR,
        }
        return self in recoverable_errors

    def get_fallback_option(self) -> str | None:
        """Get fallback option for this error.

        Returns:
            Suggested fallback option (e.g., "text_mode") or None
        """
        fallback_map = {
            self.STT_FAILED: "text_mode",
            self.TTS_FAILED: "text_only",
            self.AUDIO_FORMAT_UNSUPPORTED: "text_mode",
            self.VOICE_METRICS_UNAVAILABLE: "continue_without_metrics",
        }
        return fallback_map.get(self, None)
</file>

<file path="src/domain/models/evaluation.py">
"""Evaluation domain model."""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class GapSeverity(str, Enum):
    """Concept gap severity levels."""

    MINOR = "minor"
    MODERATE = "moderate"
    MAJOR = "major"


class ConceptGap(BaseModel):
    """Represents a missing concept in an answer.

    Value object - identified by evaluation_id + concept.
    """

    id: UUID = Field(default_factory=uuid4)
    evaluation_id: UUID
    concept: str  # Missing concept (e.g., "event loop", "closure")
    severity: GapSeverity
    resolved: bool = False  # True if filled in follow-up
    created_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False  # Allow mutation for resolved field


class Evaluation(BaseModel):
    """Represents evaluation of an answer.

    Entity - tracks scoring, gaps, and penalties across follow-up attempts.
    """

    id: UUID = Field(default_factory=uuid4)
    answer_id: UUID
    question_id: UUID  # Main question OR follow-up question
    interview_id: UUID

    # Scores
    raw_score: float = Field(ge=0.0, le=100.0)  # LLM score before penalty
    penalty: float = Field(ge=-15.0, le=0.0, default=0.0)  # Attempt penalty
    final_score: float = Field(ge=0.0, le=100.0)  # raw_score + penalty
    similarity_score: float | None = Field(None, ge=0.0, le=1.0)  # Cosine similarity

    # LLM evaluation details
    completeness: float = Field(ge=0.0, le=1.0)
    relevance: float = Field(ge=0.0, le=1.0)
    sentiment: str | None = None  # confident/uncertain/nervous
    reasoning: str | None = None
    strengths: list[str] = Field(default_factory=list)
    weaknesses: list[str] = Field(default_factory=list)
    improvement_suggestions: list[str] = Field(default_factory=list)

    # Follow-up context
    attempt_number: int = Field(ge=1, le=3, default=1)  # 1 (main), 2-3 (follow-ups)
    parent_evaluation_id: UUID | None = None  # NULL if main question

    # Gaps (relationship)
    gaps: list[ConceptGap] = Field(default_factory=list)

    created_at: datetime = Field(default_factory=datetime.utcnow)
    evaluated_at: datetime | None = None

    class Config:
        """Pydantic configuration."""

        frozen = False

    def apply_penalty(self, attempt_number: int) -> None:
        """Apply penalty based on attempt number.

        Penalty progression: 0 (1st) / -5 (2nd) / -15 (3rd)

        Args:
            attempt_number: 1, 2, or 3

        Raises:
            ValueError: If attempt_number not in [1, 2, 3]
        """
        if attempt_number == 1:
            self.penalty = 0.0
        elif attempt_number == 2:
            self.penalty = -5.0
        elif attempt_number == 3:
            self.penalty = -15.0
        else:
            raise ValueError(f"Invalid attempt_number: {attempt_number} (must be 1-3)")

        self.attempt_number = attempt_number
        self.final_score = max(0.0, min(100.0, self.raw_score + self.penalty))

    def has_gaps(self) -> bool:
        """Check if unresolved gaps exist.

        Returns:
            True if any gap is not resolved, False otherwise
        """
        return any(not gap.resolved for gap in self.gaps)

    def resolve_gaps(self) -> None:
        """Mark all gaps as resolved."""
        for gap in self.gaps:
            gap.resolved = True

    def is_passing(self, threshold: float = 60.0) -> bool:
        """Check if final score meets threshold.

        Args:
            threshold: Minimum score to pass (default: 60.0)

        Returns:
            True if final_score >= threshold, False otherwise
        """
        return self.final_score >= threshold

    def is_gap_resolved_by_criteria(self) -> bool:
        """Check if gaps should be considered resolved based on criteria.

        Gap resolution criteria:
        - completeness >= 0.8 OR
        - final_score >= 80 OR
        - attempt_number == 3 (max attempts reached)

        Returns:
            True if criteria met, False otherwise
        """
        return (
            self.completeness >= 0.8
            or self.final_score >= 80
            or self.attempt_number == 3
        )


@dataclass(frozen=True)
class FollowUpEvaluationContext:
    """Context for evaluating follow-up answers.

    Immutable value object passed to LLM adapters to provide context
    about previous attempts, gaps, and scores.
    """

    parent_question_id: UUID
    follow_up_question_id: UUID
    attempt_number: int  # 2 or 3
    previous_evaluations: list[Evaluation]  # All previous attempts (main + follow-ups)
    cumulative_gaps: list[ConceptGap]  # All unresolved gaps from previous attempts
    previous_scores: list[float]  # Scores from previous attempts
    parent_ideal_answer: str  # For reference

    @property
    def has_persistent_gaps(self) -> bool:
        """Check if any gaps persist across attempts.

        Returns:
            True if unresolved gaps exist, False otherwise
        """
        return len(self.cumulative_gaps) > 0

    @property
    def average_previous_score(self) -> float:
        """Calculate average score from previous attempts.

        Returns:
            Average score, or 0.0 if no previous attempts
        """
        if not self.previous_scores:
            return 0.0
        return sum(self.previous_scores) / len(self.previous_scores)

    def get_persistent_gap_concepts(self) -> list[str]:
        """Get list of persistent gap concepts.

        Returns:
            List of concept names that remain unresolved
        """
        return [gap.concept for gap in self.cumulative_gaps if not gap.resolved]
</file>

<file path="src/domain/ports/evaluation_repository_port.py">
"""Evaluation repository port."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.evaluation import Evaluation


class EvaluationRepositoryPort(ABC):
    """Interface for evaluation persistence."""

    @abstractmethod
    async def save(self, evaluation: Evaluation) -> Evaluation:
        """Save evaluation with gaps.

        Args:
            evaluation: Evaluation to save

        Returns:
            Saved evaluation with generated ID
        """
        pass

    @abstractmethod
    async def get_by_id(self, evaluation_id: UUID) -> Evaluation | None:
        """Get evaluation by ID.

        Args:
            evaluation_id: Evaluation UUID

        Returns:
            Evaluation if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_answer_id(self, answer_id: UUID) -> Evaluation | None:
        """Get evaluation for answer (1:1 relationship).

        Args:
            answer_id: Answer UUID

        Returns:
            Evaluation if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_parent_evaluation_id(
        self, parent_id: UUID
    ) -> list[Evaluation]:
        """Get follow-up evaluations for parent.

        Args:
            parent_id: Parent evaluation UUID

        Returns:
            List of follow-up evaluations ordered by attempt_number
        """
        pass

    @abstractmethod
    async def update(self, evaluation: Evaluation) -> Evaluation:
        """Update evaluation.

        Args:
            evaluation: Evaluation to update

        Returns:
            Updated evaluation

        Raises:
            ValueError: If evaluation not found
        """
        pass

    @abstractmethod
    async def delete(self, evaluation_id: UUID) -> None:
        """Delete evaluation.

        Args:
            evaluation_id: Evaluation UUID to delete
        """
        pass
</file>

<file path="src/domain/ports/follow_up_question_repository_port.py">
"""Follow-up question repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.follow_up_question import FollowUpQuestion


class FollowUpQuestionRepositoryPort(ABC):
    """Interface for follow-up question persistence operations.

    This port abstracts database operations for follow-up questions,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, follow_up_question: FollowUpQuestion) -> FollowUpQuestion:
        """Save a follow-up question.

        Args:
            follow_up_question: FollowUpQuestion to save

        Returns:
            Saved follow-up question with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, question_id: UUID) -> FollowUpQuestion | None:
        """Retrieve a follow-up question by ID.

        Args:
            question_id: Follow-up question identifier

        Returns:
            FollowUpQuestion if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            List of follow-up questions ordered by order_in_sequence
        """
        pass

    @abstractmethod
    async def get_by_interview_id(self, interview_id: UUID) -> list[FollowUpQuestion]:
        """Retrieve all follow-up questions for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of follow-up questions ordered by created_at
        """
        pass

    @abstractmethod
    async def count_by_parent_question_id(self, parent_question_id: UUID) -> int:
        """Count follow-up questions for a parent question.

        Args:
            parent_question_id: Parent question identifier

        Returns:
            Count of follow-up questions
        """
        pass

    @abstractmethod
    async def delete(self, question_id: UUID) -> bool:
        """Delete a follow-up question.

        Args:
            question_id: Follow-up question identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/services/__init__.py">
"""Domain services package."""
</file>

<file path="src/infrastructure/__init__.py">
"""Infrastructure layer package."""
</file>

<file path="src/infrastructure/config/__init__.py">
"""Configuration package."""

from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"]
</file>

<file path="src/infrastructure/database/base.py">
"""SQLAlchemy declarative base and common utilities."""

from sqlalchemy.orm import DeclarativeBase


class Base(DeclarativeBase):
    """Base class for all SQLAlchemy models.

    All database models should inherit from this class.
    This provides the declarative base for SQLAlchemy 2.0+.
    """

    pass
</file>

<file path="src/infrastructure/dependency_injection/__init__.py">
"""Dependency injection package."""

from .container import Container, get_container

__all__ = ["Container", "get_container"]
</file>

<file path="test_basic.py">
"""Quick test script to verify setup."""

import asyncio
from src.domain.models import Candidate, Interview, Question, DifficultyLevel, QuestionType
from src.infrastructure.config import get_settings


async def main():
    print("=== Testing Elios AI Service Setup ===\n")

    # Test 1: Configuration
    print("1. Testing Configuration...")
    settings = get_settings()
    print(f"   âœ“ App Name: {settings.app_name}")
    print(f"   âœ“ Environment: {settings.environment}")
    print(f"   âœ“ API Port: {settings.api_port}\n")

    # Test 2: Domain Models
    print("2. Testing Domain Models...")
    candidate = Candidate(name="Test User", email="test@example.com")
    print(f"   âœ“ Created Candidate: {candidate.name} ({candidate.id})")

    interview = Interview(candidate_id=candidate.id)
    print(f"   âœ“ Created Interview: {interview.id}")
    print(f"   âœ“ Interview Status: {interview.status}\n")

    question = Question(
        text="What is dependency injection?",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Software Architecture"]
    )
    print(f"   âœ“ Created Question: {question.text[:40]}...")
    print(f"   âœ“ Question Difficulty: {question.difficulty}\n")

    # Test 3: Interview Flow
    print("3. Testing Interview Flow...")
    interview.mark_ready(candidate.id)
    print(f"   âœ“ Interview marked ready")

    interview.start()
    print(f"   âœ“ Interview started: {interview.status}")

    interview.add_question(question.id)
    print(f"   âœ“ Added question to interview")
    print(f"   âœ“ Progress: {interview.get_progress_percentage()}%\n")

    print("=== All Tests Passed! ===")
    print("\nYou can now start the server:")
    print("  python src/main.py")
    print("\nThen visit:")
    print("  http://localhost:8000")
    print("  http://localhost:8000/docs")


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="tests/__init__.py">
"""Tests package."""
</file>

<file path="tests/unit/adapters/test_mock_analytics.py">
"""Unit tests for MockAnalyticsAdapter."""

import pytest
from uuid import uuid4

from src.adapters.mock.mock_analytics import MockAnalyticsAdapter
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.question import Question, QuestionType, DifficultyLevel


@pytest.fixture
def analytics():
    """Create analytics adapter instance."""
    return MockAnalyticsAdapter()


@pytest.fixture
def sample_question():
    """Create sample question."""
    return Question(
        id=uuid4(),
        text="What is Python?",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Python", "Programming"],
        reference_answer="Python is a high-level programming language...",
        evaluation_criteria="Understanding Python fundamentals",
    )


def _create_answer(question_id):
    """Helper to create sample answer."""
    evaluation = AnswerEvaluation(
        score=85.0,
        semantic_similarity=0.9,
        completeness=0.85,
        relevance=0.95,
        sentiment="confident",
        reasoning="Good answer with examples",
        strengths=["Clear explanation", "Good examples"],
        weaknesses=["Could add more depth"],
        improvement_suggestions=["Add advanced use cases"],
    )

    return Answer(
        id=uuid4(),
        question_id=question_id,
        candidate_id=uuid4(),
        interview_id=uuid4(),
        text="Python is a versatile programming language known for readability",
        evaluation=evaluation,
    )



@pytest.fixture
def sample_answer(sample_question):
    """Create sample answer with evaluation."""
    return _create_answer(sample_question.id)


class TestRecordAnswerEvaluation:
    """Test record_answer_evaluation method."""

    @pytest.mark.asyncio
    async def test_record_single_answer(self, analytics, sample_answer):
        """Test recording single answer."""
        interview_id = uuid4()

        await analytics.record_answer_evaluation(interview_id, sample_answer)

        # Verify it was stored
        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["answers_count"] == 1

    @pytest.mark.asyncio
    async def test_record_multiple_answers(self, analytics, sample_answer):
        """Test recording multiple answers."""
        interview_id = uuid4()

        # Record 3 answers
        for _ in range(3):
            await analytics.record_answer_evaluation(interview_id, sample_answer)

        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["answers_count"] == 3


class TestGetInterviewStatistics:
    """Test get_interview_statistics method."""

    @pytest.mark.asyncio
    async def test_empty_interview(self, analytics):
        """Test statistics for interview with no answers."""
        interview_id = uuid4()

        stats = await analytics.get_interview_statistics(interview_id)

        assert stats["interview_id"] == str(interview_id)
        assert stats["question_count"] == 0
        assert stats["answers_count"] == 0
        assert stats["avg_score"] == 0.0
        assert stats["completion_rate"] == 0.0

    @pytest.mark.asyncio
    async def test_statistics_calculation(self, analytics, sample_question):
        """Test statistics are calculated correctly."""
        interview_id = uuid4()

        # Create answers with different scores
        scores = [80.0, 90.0, 70.0]
        for score in scores:
            evaluation = AnswerEvaluation(
                score=score,
                semantic_similarity=0.8,
                completeness=0.8,
                relevance=0.8,
                sentiment="positive",
                reasoning="Test",
                strengths=["Test"],
                weaknesses=[],
                improvement_suggestions=[],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                candidate_id=uuid4(),
                interview_id=interview_id,
                text="Test answer",
                evaluation=evaluation,
            )
            await analytics.record_answer_evaluation(interview_id, answer)

        stats = await analytics.get_interview_statistics(interview_id)

        assert stats["question_count"] == 3
        assert stats["answers_count"] == 3
        assert stats["avg_score"] == 80.0  # (80+90+70)/3
        assert stats["completion_rate"] == 100.0
        assert stats["highest_score"] == 90.0
        assert stats["lowest_score"] == 70.0
        assert stats["time_spent_minutes"] > 0

    @pytest.mark.asyncio
    async def test_completion_rate(self, analytics, sample_question):
        """Test completion rate calculation."""
        interview_id = uuid4()

        # Create 2 answers with text and 1 without
        for i in range(3):
            evaluation = AnswerEvaluation(
                score=80.0,
                semantic_similarity=0.8,
                completeness=0.8,
                relevance=0.8,
                sentiment="positive",
                reasoning="Test",
                strengths=[],
                weaknesses=[],
                improvement_suggestions=[],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                interview_id=interview_id,
                candidate_id=uuid4(),
                text="Answer" if i < 2 else "",  # 2 with text, 1 empty
                evaluation=evaluation,
            )
            await analytics.record_answer_evaluation(interview_id, answer)

        stats = await analytics.get_interview_statistics(interview_id)
        assert stats["completion_rate"] == pytest.approx(66.67, rel=0.1)


class TestGetCandidatePerformanceHistory:
    """Test get_candidate_performance_history method."""

    @pytest.mark.asyncio
    async def test_first_time_candidate(self, analytics):
        """Test history for first-time candidate."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        assert isinstance(history, list)
        assert len(history) >= 0  # Mock generates 0-3 past interviews

    @pytest.mark.asyncio
    async def test_history_structure(self, analytics):
        """Test history entries have correct structure."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        if history:
            entry = history[0]
            assert "interview_date" in entry
            assert "avg_score" in entry
            assert "questions_answered" in entry
            assert "completion_rate" in entry
            assert "strong_skills" in entry
            assert "weak_skills" in entry

    @pytest.mark.asyncio
    async def test_history_shows_improvement(self, analytics):
        """Test that mock history shows score improvement."""
        candidate_id = uuid4()

        history = await analytics.get_candidate_performance_history(candidate_id)

        if len(history) >= 2:
            # Scores should improve over time
            assert history[1]["avg_score"] >= history[0]["avg_score"]


class TestGenerateImprovementRecommendations:
    """Test generate_improvement_recommendations method."""

    @pytest.mark.asyncio
    async def test_no_answers(self, analytics):
        """Test recommendations with no answers."""
        interview_id = uuid4()
        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, [], []
        )

        assert isinstance(recommendations, list)
        assert len(recommendations) > 0
        assert any("Complete the interview" in r for r in recommendations)

    @pytest.mark.asyncio
    async def test_low_score_recommendations(self, analytics, sample_question):
        """Test recommendations for low scores (<60)."""
        interview_id = uuid4()

        # Create low-scoring answers
        questions = [sample_question, sample_question, sample_question]  # 3 questions for 3 answers
        answers = []
        for _ in range(3):
            evaluation = AnswerEvaluation(
                score=50.0,
                semantic_similarity=0.5,
                completeness=0.5,
                relevance=0.6,
                sentiment="uncertain",
                reasoning="Lacks depth",
                strengths=[],
                weaknesses=["Lacks depth", "Missing examples"],
                improvement_suggestions=["Study fundamentals"],
            )
            answer = Answer(
                id=uuid4(),
                question_id=sample_question.id,
                interview_id=interview_id,
                candidate_id=uuid4(),
                text="Brief answer",
                evaluation=evaluation,
            )
            answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) >= 3
        assert len(recommendations) <= 5
        assert any("fundamental" in r.lower() for r in recommendations)

    @pytest.mark.asyncio
    async def test_mid_score_recommendations(self, analytics, sample_question):
        """Test recommendations for mid-range scores (60-80)."""
        interview_id = uuid4()

        questions = [sample_question]
        answers = []
        evaluation = AnswerEvaluation(
            score=70.0,
            semantic_similarity=0.7,
            completeness=0.7,
            relevance=0.75,
            sentiment="positive",
            reasoning="Good but could improve",
            strengths=["Clear"],
            weaknesses=["Could add more detail"],
            improvement_suggestions=["Add examples"],
        )
        answer = Answer(
            id=uuid4(),
            question_id=sample_question.id,
            interview_id=interview_id,
            candidate_id=uuid4(),
            text="Decent answer",
            evaluation=evaluation,
        )
        answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) >= 2
        assert len(recommendations) <= 5

    @pytest.mark.asyncio
    async def test_high_score_recommendations(self, analytics, sample_question):
        """Test recommendations for high scores (>80)."""
        interview_id = uuid4()

        questions = [sample_question]
        answers = []
        evaluation = AnswerEvaluation(
            score=90.0,
            semantic_similarity=0.95,
            completeness=0.9,
            relevance=0.95,
            sentiment="confident",
            reasoning="Excellent answer",
            strengths=["Comprehensive", "Clear", "Good examples"],
            weaknesses=["Minor formatting"],
            improvement_suggestions=["Consider edge cases"],
        )
        answer = Answer(
            id=uuid4(),
            question_id=sample_question.id,
            interview_id=interview_id,
            candidate_id=uuid4(),
            text="Excellent detailed answer",
            evaluation=evaluation,
        )
        answers.append(answer)

        recommendations = await analytics.generate_improvement_recommendations(
            interview_id, questions, answers
        )

        assert len(recommendations) > 0
        assert len(recommendations) <= 5
        assert any("excellent" in r.lower() or "continue" in r.lower() for r in recommendations)


class TestCalculateSkillScores:
    """Test calculate_skill_scores method."""

    @pytest.mark.asyncio
    async def test_empty_data(self, analytics):
        """Test with no answers."""
        scores = await analytics.calculate_skill_scores([], [])
        assert scores == {}

    @pytest.mark.asyncio
    async def test_single_skill(self, analytics):
        """Test scoring for single skill."""
        question = Question(
            id=uuid4(),
            text="What is Python?",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python"],
        )

        evaluation = AnswerEvaluation(
            score=85.0,
            semantic_similarity=0.85,
            completeness=0.85,
            relevance=0.9,
            sentiment="confident",
            reasoning="Good",
            strengths=[],
            weaknesses=[],
            improvement_suggestions=[],
        )
        answer = Answer(
            id=uuid4(),
            question_id=question.id,
            interview_id=uuid4(),
            candidate_id=uuid4(),
            text="Answer",
            evaluation=evaluation,
        )

        scores = await analytics.calculate_skill_scores([answer], [question])

        assert "Python" in scores
        assert scores["Python"] == 85.0

    @pytest.mark.asyncio
    async def test_multiple_skills(self, analytics):
        """Test scoring across multiple skills."""
        questions = [
            Question(
                id=uuid4(),
                text="Q1",
                question_type=QuestionType.TECHNICAL,
                difficulty=DifficultyLevel.MEDIUM,
                skills=["Python", "FastAPI"],
            ),
            Question(
                id=uuid4(),
                text="Q2",
                question_type=QuestionType.TECHNICAL,
                difficulty=DifficultyLevel.MEDIUM,
                skills=["Python"],
            ),
        ]

        answers = [
            Answer(
                id=uuid4(),
                question_id=questions[0].id,
                interview_id=uuid4(),
                candidate_id=uuid4(),
                text="A1",
                evaluation=AnswerEvaluation(
                    score=80.0,
                    semantic_similarity=0.8,
                    completeness=0.8,
                    relevance=0.8,
                    sentiment="positive",
                    reasoning="",
                    strengths=[],
                    weaknesses=[],
                    improvement_suggestions=[],
                ),
            ),
            Answer(
                id=uuid4(),
                question_id=questions[1].id,
                interview_id=uuid4(),
                candidate_id=uuid4(),
                text="A2",
                evaluation=AnswerEvaluation(
                    score=90.0,
                    semantic_similarity=0.9,
                    completeness=0.9,
                    relevance=0.9,
                    sentiment="confident",
                    reasoning="",
                    strengths=[],
                    weaknesses=[],
                    improvement_suggestions=[],
                ),
            ),
        ]

        scores = await analytics.calculate_skill_scores(answers, questions)

        assert "Python" in scores
        assert "FastAPI" in scores
        assert scores["Python"] == 85.0  # Average of 80 and 90
        assert scores["FastAPI"] == 80.0

    @pytest.mark.asyncio
    async def test_mismatched_lengths(self, analytics, sample_question):
        """Test with mismatched answer/question counts."""
        answer = _create_answer(sample_question.id)
        scores = await analytics.calculate_skill_scores([answer], [])
        assert scores == {}
</file>

<file path="tests/unit/adapters/test_mock_cv_analyzer.py">
"""Unit tests for MockCVAnalyzerAdapter."""

import pytest
from uuid import UUID, uuid4

from src.adapters.mock.mock_cv_analyzer import MockCVAnalyzerAdapter
from src.domain.models.cv_analysis import CVAnalysis


@pytest.fixture
def cv_analyzer():
    """Create CV analyzer instance."""
    return MockCVAnalyzerAdapter()


class TestExtractTextFromFile:
    """Test extract_text_from_file method."""

    @pytest.mark.asyncio
    async def test_extract_text_pdf(self, cv_analyzer):
        """Test extracting text from PDF file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.pdf")

        assert isinstance(text, str)
        assert len(text) > 200
        assert "John Doe" in text
        assert "Software Engineer" in text

    @pytest.mark.asyncio
    async def test_extract_text_doc(self, cv_analyzer):
        """Test extracting text from DOC file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.doc")

        assert isinstance(text, str)
        assert len(text) > 0

    @pytest.mark.asyncio
    async def test_extract_text_docx(self, cv_analyzer):
        """Test extracting text from DOCX file."""
        text = await cv_analyzer.extract_text_from_file("test_cv.docx")

        assert isinstance(text, str)
        assert len(text) > 0

    @pytest.mark.asyncio
    async def test_unsupported_format(self, cv_analyzer):
        """Test error for unsupported file format."""
        with pytest.raises(ValueError, match="Unsupported file format"):
            await cv_analyzer.extract_text_from_file("test_cv.txt")

    @pytest.mark.asyncio
    async def test_unsupported_format_xlsx(self, cv_analyzer):
        """Test error for XLSX file."""
        with pytest.raises(ValueError, match="Unsupported file format"):
            await cv_analyzer.extract_text_from_file("test_cv.xlsx")


class TestAnalyzeCV:
    """Test analyze_cv method."""

    @pytest.mark.asyncio
    async def test_analyze_junior_cv(self, cv_analyzer):
        """Test analyzing junior-level CV."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="junior_developer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert str(cv_analysis.candidate_id) == candidate_id
        assert len(cv_analysis.skills) >= 2
        assert len(cv_analysis.skills) <= 3
        assert cv_analysis.work_experience_years is not None
        assert 1.0 <= cv_analysis.work_experience_years <= 2.0
        assert cv_analysis.suggested_difficulty == "easy"
        assert cv_analysis.education_level == "Bachelor's"

    @pytest.mark.asyncio
    async def test_analyze_senior_cv(self, cv_analyzer):
        """Test analyzing senior-level CV."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="senior_engineer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert len(cv_analysis.skills) >= 5
        assert len(cv_analysis.skills) <= 6
        assert cv_analysis.work_experience_years is not None
        assert 6.0 <= cv_analysis.work_experience_years <= 10.0
        assert cv_analysis.suggested_difficulty == "hard"
        assert cv_analysis.education_level == "Master's"

    @pytest.mark.asyncio
    async def test_analyze_mid_level_cv(self, cv_analyzer):
        """Test analyzing mid-level CV (default)."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="developer.pdf",
            candidate_id=candidate_id
        )

        assert isinstance(cv_analysis, CVAnalysis)
        assert len(cv_analysis.skills) >= 4
        assert len(cv_analysis.skills) <= 5
        assert cv_analysis.work_experience_years is not None
        assert 3.0 <= cv_analysis.work_experience_years <= 5.0
        assert cv_analysis.suggested_difficulty == "medium"
        assert cv_analysis.education_level == "Bachelor's"

    @pytest.mark.asyncio
    async def test_cv_analysis_structure(self, cv_analyzer):
        """Test CV analysis has all required fields."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="test.pdf",
            candidate_id=candidate_id
        )

        assert cv_analysis.id is not None
        assert cv_analysis.cv_file_path == "test.pdf"
        assert cv_analysis.extracted_text is not None
        assert len(cv_analysis.extracted_text) > 0
        assert isinstance(cv_analysis.skills, list)
        assert len(cv_analysis.skills) > 0
        assert isinstance(cv_analysis.suggested_topics, list)
        assert len(cv_analysis.suggested_topics) > 0
        assert cv_analysis.summary is not None
        assert "Mock CV analysis" in cv_analysis.summary

    @pytest.mark.asyncio
    async def test_skills_are_technical(self, cv_analyzer):
        """Test that extracted skills include technical skills."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="senior_engineer.pdf",
            candidate_id=candidate_id
        )

        technical_skills = cv_analysis.get_technical_skills()
        assert len(technical_skills) > 0

        # Check skill structure
        for skill in cv_analysis.skills:
            assert skill.name is not None
            assert skill.category in ["technical", "soft"]

    @pytest.mark.asyncio
    async def test_suggested_topics_from_skills(self, cv_analyzer):
        """Test that suggested topics are derived from skills."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="python_developer.pdf",
            candidate_id=candidate_id
        )

        # Topics should be related to skills
        assert len(cv_analysis.suggested_topics) > 0
        assert len(cv_analysis.suggested_topics) <= 5

    @pytest.mark.asyncio
    async def test_metadata_included(self, cv_analyzer):
        """Test that metadata is included in analysis."""
        candidate_id = str(uuid4())
        cv_analysis = await cv_analyzer.analyze_cv(
            cv_file_path="junior_dev.pdf",
            candidate_id=candidate_id
        )

        assert "experience_level" in cv_analysis.metadata
        assert "file_name" in cv_analysis.metadata
        assert "mock_adapter" in cv_analysis.metadata
        assert cv_analysis.metadata["mock_adapter"] is True
        assert cv_analysis.metadata["experience_level"] == "junior"

    @pytest.mark.asyncio
    async def test_consistent_results(self, cv_analyzer):
        """Test that same filename produces consistent experience level."""
        candidate_id = str(uuid4())

        # Call twice with same filename
        result1 = await cv_analyzer.analyze_cv("junior_dev.pdf", candidate_id)
        result2 = await cv_analyzer.analyze_cv("junior_dev.pdf", candidate_id)

        assert result1.suggested_difficulty == result2.suggested_difficulty
        assert len(result1.skills) == len(result2.skills)
        assert result1.education_level == result2.education_level
</file>

<file path="tests/unit/application/use_cases/test_follow_up_decision.py">
"""Tests for FollowUpDecisionUseCase."""

import pytest
from datetime import datetime
from uuid import UUID, uuid4

from src.application.use_cases.follow_up_decision import FollowUpDecisionUseCase
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.follow_up_question import FollowUpQuestion


class MockAnswerRepository:
    """Mock answer repository for testing."""

    def __init__(self):
        self.answers = {}

    async def get_by_question_id(self, question_id: UUID) -> Answer | None:
        return self.answers.get(question_id)

    def add_answer(self, question_id: UUID, answer: Answer):
        self.answers[question_id] = answer


class MockFollowUpQuestionRepository:
    """Mock follow-up question repository for testing."""

    def __init__(self):
        self.follow_ups = {}

    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        return self.follow_ups.get(parent_question_id, [])

    def add_follow_ups(self, parent_question_id: UUID, follow_ups: list[FollowUpQuestion]):
        self.follow_ups[parent_question_id] = follow_ups


@pytest.fixture
def answer_repo():
    return MockAnswerRepository()


@pytest.fixture
def follow_up_repo():
    return MockFollowUpQuestionRepository()


@pytest.fixture
def use_case(answer_repo, follow_up_repo):
    return FollowUpDecisionUseCase(
        answer_repository=answer_repo,
        follow_up_question_repository=follow_up_repo,
    )


@pytest.fixture
def interview_id():
    return uuid4()


@pytest.fixture
def parent_question_id():
    return uuid4()


@pytest.fixture
def candidate_id():
    return uuid4()


@pytest.mark.asyncio
async def test_no_followup_when_max_reached(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo, answer_repo
):
    """Test that no follow-up is generated when max (3) already reached."""
    # Create 3 existing follow-ups
    follow_ups = [
        FollowUpQuestion(
            id=uuid4(),
            parent_question_id=parent_question_id,
            interview_id=interview_id,
            text=f"Follow-up {i+1}",
            generated_reason="Missing concepts",
            order_in_sequence=i + 1,
        )
        for i in range(3)
    ]
    follow_up_repo.add_follow_ups(parent_question_id, follow_ups)

    # Create latest answer with gaps
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Incomplete answer",
        similarity_score=0.5,
        gaps={"concepts": ["concept1", "concept2"], "confirmed": True},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is False
    assert "Max follow-ups (3)" in decision["reason"]
    assert decision["follow_up_count"] == 3


@pytest.mark.asyncio
async def test_no_followup_when_similarity_high(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo
):
    """Test that no follow-up is generated when similarity >= 0.8."""
    # No existing follow-ups
    follow_up_repo.add_follow_ups(parent_question_id, [])

    # Create latest answer with high similarity
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Complete answer",
        similarity_score=0.85,
        gaps={"concepts": [], "confirmed": False},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is False
    assert "0.85" in decision["reason"]
    assert decision["follow_up_count"] == 0


@pytest.mark.asyncio
async def test_no_followup_when_no_gaps(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo
):
    """Test that no follow-up is generated when no gaps detected."""
    # No existing follow-ups
    follow_up_repo.add_follow_ups(parent_question_id, [])

    # Create latest answer with no gaps
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Complete answer",
        similarity_score=0.7,
        gaps={"concepts": [], "confirmed": False},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is False
    assert "No concept gaps" in decision["reason"] or "No cumulative gaps" in decision["reason"]
    assert decision["follow_up_count"] == 0


@pytest.mark.asyncio
async def test_followup_needed_with_gaps(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo
):
    """Test that follow-up is generated when gaps exist."""
    # No existing follow-ups
    follow_up_repo.add_follow_ups(parent_question_id, [])

    # Create latest answer with confirmed gaps
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Incomplete answer",
        similarity_score=0.5,
        gaps={"concepts": ["recursion", "base case"], "confirmed": True},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is True
    assert "2 missing concepts" in decision["reason"]
    assert decision["follow_up_count"] == 0
    assert set(decision["cumulative_gaps"]) == {"recursion", "base case"}


@pytest.mark.asyncio
async def test_gap_accumulation_across_followups(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo, answer_repo
):
    """Test that gaps accumulate across multiple follow-up answers."""
    # Create 2 existing follow-ups
    follow_up1_id = uuid4()
    follow_up2_id = uuid4()
    follow_ups = [
        FollowUpQuestion(
            id=follow_up1_id,
            parent_question_id=parent_question_id,
            interview_id=interview_id,
            text="Follow-up 1",
            generated_reason="Missing concepts",
            order_in_sequence=1,
        ),
        FollowUpQuestion(
            id=follow_up2_id,
            parent_question_id=parent_question_id,
            interview_id=interview_id,
            text="Follow-up 2",
            generated_reason="Missing concepts",
            order_in_sequence=2,
        ),
    ]
    follow_up_repo.add_follow_ups(parent_question_id, follow_ups)

    # Add answers for follow-ups with different gaps
    answer_repo.add_answer(
        follow_up1_id,
        Answer(
            interview_id=interview_id,
            question_id=follow_up1_id,
            candidate_id=candidate_id,
            text="Answer 1",
            gaps={"concepts": ["concept1", "concept2"], "confirmed": True},
        ),
    )
    answer_repo.add_answer(
        follow_up2_id,
        Answer(
            interview_id=interview_id,
            question_id=follow_up2_id,
            candidate_id=candidate_id,
            text="Answer 2",
            gaps={"concepts": ["concept2", "concept3"], "confirmed": True},
        ),
    )

    # Create latest answer with new gaps
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Latest answer",
        similarity_score=0.6,
        gaps={"concepts": ["concept3", "concept4"], "confirmed": True},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is True
    assert decision["follow_up_count"] == 2
    # Should have unique concepts: concept1, concept2, concept3, concept4
    assert set(decision["cumulative_gaps"]) == {"concept1", "concept2", "concept3", "concept4"}


@pytest.mark.asyncio
async def test_no_followup_when_count_2_and_no_new_gaps(
    use_case, interview_id, parent_question_id, candidate_id, follow_up_repo
):
    """Test that no follow-up when count=2 but no new gaps in latest answer."""
    # Create 2 existing follow-ups
    follow_ups = [
        FollowUpQuestion(
            id=uuid4(),
            parent_question_id=parent_question_id,
            interview_id=interview_id,
            text=f"Follow-up {i+1}",
            generated_reason="Missing concepts",
            order_in_sequence=i + 1,
        )
        for i in range(2)
    ]
    follow_up_repo.add_follow_ups(parent_question_id, follow_ups)

    # Create latest answer with no gaps
    latest_answer = Answer(
        interview_id=interview_id,
        question_id=uuid4(),
        candidate_id=candidate_id,
        text="Complete answer now",
        similarity_score=0.75,
        gaps={"concepts": [], "confirmed": False},
    )

    # Execute decision
    decision = await use_case.execute(
        interview_id=interview_id,
        parent_question_id=parent_question_id,
        latest_answer=latest_answer,
    )

    assert decision["needs_followup"] is False
    assert decision["follow_up_count"] == 2
</file>

<file path="tests/unit/domain/test_interview_state_transitions.py">
"""Unit tests for Interview state transitions."""

import pytest
from uuid import uuid4

from src.domain.models.interview import Interview, InterviewStatus


class TestStateTransitionValidation:
    """Test state transition validation logic."""

    def test_valid_transition_planning_to_idle(self):
        """Test valid PLANNING â†’ IDLE transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.PLANNING)
        cv_analysis_id = uuid4()

        interview.mark_ready(cv_analysis_id)

        assert interview.status == InterviewStatus.IDLE
        assert interview.cv_analysis_id == cv_analysis_id

    def test_valid_transition_idle_to_questioning(self):
        """Test valid IDLE â†’ QUESTIONING transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)

        interview.start()

        assert interview.status == InterviewStatus.QUESTIONING
        assert interview.started_at is not None

    def test_valid_transition_questioning_to_evaluating(self):
        """Test valid QUESTIONING â†’ EVALUATING transition."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.QUESTIONING,
            question_ids=[uuid4()],
            current_question_index=0,
        )

        interview.add_answer(uuid4())

        assert interview.status == InterviewStatus.EVALUATING
        assert interview.current_question_index == 1

    def test_valid_transition_evaluating_to_follow_up(self):
        """Test valid EVALUATING â†’ FOLLOW_UP transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q_id = uuid4()
        followup_id = uuid4()

        interview.ask_followup(followup_id, parent_q_id)

        assert interview.status == InterviewStatus.FOLLOW_UP
        assert interview.current_parent_question_id == parent_q_id
        assert interview.current_followup_count == 1
        assert followup_id in interview.adaptive_follow_ups

    def test_valid_transition_evaluating_to_questioning(self):
        """Test valid EVALUATING â†’ QUESTIONING transition."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4(), uuid4()],
            current_question_index=0,
        )

        interview.proceed_to_next_question()

        assert interview.status == InterviewStatus.QUESTIONING
        assert interview.current_followup_count == 0
        assert interview.current_parent_question_id is None

    def test_valid_transition_evaluating_to_complete(self):
        """Test valid EVALUATING â†’ COMPLETE transition."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4()],
            current_question_index=1,  # No more questions
        )

        interview.proceed_to_next_question()

        assert interview.status == InterviewStatus.COMPLETE
        assert interview.completed_at is not None

    def test_valid_transition_follow_up_to_evaluating(self):
        """Test valid FOLLOW_UP â†’ EVALUATING transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.FOLLOW_UP)

        interview.answer_followup()

        assert interview.status == InterviewStatus.EVALUATING

    def test_invalid_transition_idle_to_evaluating(self):
        """Test invalid IDLE â†’ EVALUATING transition raises error."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)

        with pytest.raises(ValueError, match="Invalid transition"):
            interview.transition_to(InterviewStatus.EVALUATING)

    def test_invalid_transition_complete_to_questioning(self):
        """Test terminal state COMPLETE cannot transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.COMPLETE)

        with pytest.raises(ValueError, match="Invalid transition"):
            interview.transition_to(InterviewStatus.QUESTIONING)

    def test_invalid_transition_cancelled_to_idle(self):
        """Test terminal state CANCELLED cannot transition."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.CANCELLED)

        with pytest.raises(ValueError, match="Invalid transition"):
            interview.transition_to(InterviewStatus.IDLE)

    def test_all_valid_transitions(self):
        """Test all valid state transitions succeed."""
        valid_paths = [
            (InterviewStatus.PLANNING, InterviewStatus.IDLE),
            (InterviewStatus.IDLE, InterviewStatus.QUESTIONING),
            (InterviewStatus.QUESTIONING, InterviewStatus.EVALUATING),
            (InterviewStatus.EVALUATING, InterviewStatus.FOLLOW_UP),
            (InterviewStatus.EVALUATING, InterviewStatus.QUESTIONING),
            (InterviewStatus.EVALUATING, InterviewStatus.COMPLETE),
            (InterviewStatus.FOLLOW_UP, InterviewStatus.EVALUATING),
        ]

        for from_status, to_status in valid_paths:
            interview = Interview(candidate_id=uuid4(), status=from_status)
            interview.transition_to(to_status)
            assert interview.status == to_status

    def test_cancel_from_any_non_terminal_state(self):
        """Test cancel() works from any non-terminal state."""
        states = [
            InterviewStatus.PLANNING,
            InterviewStatus.IDLE,
            InterviewStatus.QUESTIONING,
            InterviewStatus.EVALUATING,
            InterviewStatus.FOLLOW_UP,
        ]

        for state in states:
            interview = Interview(candidate_id=uuid4(), status=state)
            interview.cancel()
            assert interview.status == InterviewStatus.CANCELLED


class TestFollowUpTracking:
    """Test follow-up question counter logic."""

    def test_first_followup_sets_parent_and_count(self):
        """Test first follow-up sets parent question and count to 1."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q = uuid4()
        followup_id = uuid4()

        interview.ask_followup(followup_id, parent_q)

        assert interview.current_parent_question_id == parent_q
        assert interview.current_followup_count == 1
        assert interview.status == InterviewStatus.FOLLOW_UP

    def test_followup_counter_increments_for_same_parent(self):
        """Test follow-up counter increments for same parent question."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q = uuid4()

        # First follow-up
        interview.ask_followup(uuid4(), parent_q)
        assert interview.current_followup_count == 1

        # Second follow-up (same parent)
        interview.answer_followup()  # Back to EVALUATING
        interview.ask_followup(uuid4(), parent_q)
        assert interview.current_followup_count == 2

        # Third follow-up (same parent)
        interview.answer_followup()
        interview.ask_followup(uuid4(), parent_q)
        assert interview.current_followup_count == 3

    def test_followup_counter_resets_on_new_parent(self):
        """Test counter resets when parent question changes."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q1 = uuid4()
        parent_q2 = uuid4()

        # Add follow-up for first parent
        interview.ask_followup(uuid4(), parent_q1)
        assert interview.current_followup_count == 1

        # Add follow-up for different parent
        interview.answer_followup()
        interview.ask_followup(uuid4(), parent_q2)
        assert interview.current_followup_count == 1  # Reset to 1
        assert interview.current_parent_question_id == parent_q2

    def test_max_followups_enforced(self):
        """Test max 3 follow-ups per question enforced."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q = uuid4()

        # Add 3 follow-ups
        for i in range(3):
            interview.ask_followup(uuid4(), parent_q)
            interview.answer_followup()

        # 4th should fail
        with pytest.raises(ValueError, match="Max 3 follow-ups"):
            interview.ask_followup(uuid4(), parent_q)

    def test_proceed_to_next_question_resets_counters(self):
        """Test counters reset when moving to next main question."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4(), uuid4()],
            current_question_index=0,
        )
        parent_q = uuid4()

        # Add follow-ups
        interview.ask_followup(uuid4(), parent_q)
        assert interview.current_followup_count == 1

        # Proceed to next question
        interview.answer_followup()
        interview.proceed_to_next_question()

        assert interview.current_followup_count == 0
        assert interview.current_parent_question_id is None
        assert interview.status == InterviewStatus.QUESTIONING

    def test_can_ask_more_followups_true_when_less_than_three(self):
        """Test can_ask_more_followups() returns True when count < 3."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q = uuid4()

        # No follow-ups yet
        assert interview.can_ask_more_followups() is True

        # One follow-up
        interview.ask_followup(uuid4(), parent_q)
        interview.answer_followup()
        assert interview.can_ask_more_followups() is True

        # Two follow-ups
        interview.ask_followup(uuid4(), parent_q)
        interview.answer_followup()
        assert interview.can_ask_more_followups() is True

    def test_can_ask_more_followups_false_when_three(self):
        """Test can_ask_more_followups() returns False when count = 3."""
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.EVALUATING)
        parent_q = uuid4()

        # Add 3 follow-ups
        for i in range(3):
            interview.ask_followup(uuid4(), parent_q)
            interview.answer_followup()

        assert interview.can_ask_more_followups() is False


class TestExistingMethods:
    """Test existing methods still work correctly."""

    def test_start_only_works_from_idle(self):
        """Test start() only works from IDLE state."""
        # Success from IDLE
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)
        interview.start()
        assert interview.status == InterviewStatus.QUESTIONING

        # Fail from other states
        interview2 = Interview(candidate_id=uuid4(), status=InterviewStatus.PLANNING)
        with pytest.raises(ValueError, match="Cannot start interview"):
            interview2.start()

    def test_complete_only_works_from_evaluating_with_no_questions(self):
        """Test complete() validation."""
        # Success case
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4()],
            current_question_index=1,
        )
        interview.complete()
        assert interview.status == InterviewStatus.COMPLETE

        # Fail from wrong state
        interview2 = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)
        with pytest.raises(ValueError, match="Cannot complete interview with status"):
            interview2.complete()

        # Fail with remaining questions
        interview3 = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4(), uuid4()],
            current_question_index=0,
        )
        with pytest.raises(ValueError, match="questions remain"):
            interview3.complete()

    def test_answer_followup_only_works_from_follow_up_state(self):
        """Test answer_followup() validation."""
        # Success
        interview = Interview(candidate_id=uuid4(), status=InterviewStatus.FOLLOW_UP)
        interview.answer_followup()
        assert interview.status == InterviewStatus.EVALUATING

        # Fail from wrong state
        interview2 = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)
        with pytest.raises(ValueError, match="Not in FOLLOW_UP state"):
            interview2.answer_followup()

    def test_proceed_to_next_question_only_works_from_evaluating(self):
        """Test proceed_to_next_question() validation."""
        # Success
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
            question_ids=[uuid4(), uuid4()],
        )
        interview.proceed_to_next_question()
        assert interview.status == InterviewStatus.QUESTIONING

        # Fail from wrong state
        interview2 = Interview(candidate_id=uuid4(), status=InterviewStatus.IDLE)
        with pytest.raises(ValueError, match="Cannot proceed from status"):
            interview2.proceed_to_next_question()
</file>

<file path="tests/unit/use_cases/test_generate_summary.py">
"""Tests for Phase 06: GenerateSummaryUseCase."""

from datetime import datetime
from uuid import uuid4

import pytest

from src.application.use_cases.generate_summary import GenerateSummaryUseCase
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.follow_up_question import FollowUpQuestion


class TestGenerateSummaryUseCase:
    """Test comprehensive interview summary generation."""

    @pytest.mark.asyncio
    async def test_generate_summary_happy_path(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test happy path with 3 main questions, 2 evaluated answers, 1 with follow-up."""
        # Setup interview
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create 3 main questions
        q1_id = sample_interview_adaptive.question_ids[0]
        q2_id = sample_interview_adaptive.question_ids[1]
        q3_id = sample_interview_adaptive.question_ids[2]

        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        q2 = sample_question_with_ideal_answer
        q2.id = q2_id
        q2.text = "Explain async/await in Python"
        await mock_question_repo.save(q2)

        q3 = sample_question_with_ideal_answer
        q3.id = q3_id
        q3.text = "What are Python decorators?"
        await mock_question_repo.save(q3)

        # Create 2 evaluated main answers
        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Recursion is when a function calls itself.",
            is_voice=True,
            similarity_score=0.65,
            gaps={"concepts": ["base case", "call stack"], "keywords": ["base"], "confirmed": True},
            voice_metrics={
                "overall_score": 70.0,
                "clarity_score": 75.0,
                "pace_score": 65.0,
            },
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=65.0,
                semantic_similarity=0.65,
                completeness=0.6,
                relevance=0.8,
                sentiment="uncertain",
                reasoning="Missing key concepts",
                strengths=["Correct basic definition"],
                weaknesses=["Missing base case", "No examples"],
                improvement_suggestions=["Explain base case"],
            )
        )
        await mock_answer_repo.save(answer1)

        answer2 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q2_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Async/await allows non-blocking I/O operations.",
            is_voice=True,
            similarity_score=0.85,
            gaps={"concepts": [], "keywords": [], "confirmed": False},
            voice_metrics={
                "overall_score": 80.0,
                "clarity_score": 85.0,
                "pace_score": 75.0,
            },
        )
        answer2.evaluate(
            AnswerEvaluation(
                score=85.0,
                semantic_similarity=0.85,
                completeness=0.9,
                relevance=0.95,
                sentiment="confident",
                reasoning="Strong answer",
                strengths=["Clear explanation", "Good examples"],
                weaknesses=["Could add more detail"],
                improvement_suggestions=["Discuss event loop"],
            )
        )
        await mock_answer_repo.save(answer2)

        # Create 1 follow-up question for q1
        follow_up = FollowUpQuestion(
            parent_question_id=q1_id,
            interview_id=sample_interview_adaptive.id,
            text="Can you explain what a base case is?",
            generated_reason="Missing concept: base case",
            order_in_sequence=1,
        )
        await mock_follow_up_question_repo.save(follow_up)

        # Follow-up answer (improved)
        follow_up_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=follow_up.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Base case is the condition that stops recursion.",
            is_voice=True,
            similarity_score=0.75,
            gaps={"concepts": ["call stack"], "keywords": [], "confirmed": True},
            voice_metrics={
                "overall_score": 75.0,
                "clarity_score": 80.0,
                "pace_score": 70.0,
            },
        )
        follow_up_answer.evaluate(
            AnswerEvaluation(
                score=75.0,
                semantic_similarity=0.75,
                completeness=0.7,
                relevance=0.85,
                sentiment="confident",
                reasoning="Better understanding",
                strengths=["Correct explanation"],
                weaknesses=["Could elaborate more"],
                improvement_suggestions=["Add examples"],
            )
        )
        await mock_answer_repo.save(follow_up_answer)

        # Update interview with follow-up
        sample_interview_adaptive.adaptive_follow_ups = [follow_up.id]
        await mock_interview_repo.update(sample_interview_adaptive)

        # Execute use case
        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        summary = await use_case.execute(sample_interview_adaptive.id)

        # Verify structure
        assert summary["interview_id"] == str(sample_interview_adaptive.id)
        assert summary["total_questions"] == 3
        assert summary["total_follow_ups"] == 1

        # Verify aggregate metrics
        # Expected: (65 + 85 + 75) / 3 = 75.0 theoretical
        #           (70 + 80 + 75) / 3 = 75.0 speaking
        #           75 * 0.7 + 75 * 0.3 = 75.0 overall
        assert summary["theoretical_score_avg"] == 75.0
        assert summary["speaking_score_avg"] == 75.0
        assert summary["overall_score"] == 75.0

        # Verify gap progression
        assert summary["gap_progression"]["questions_with_followups"] == 1
        assert summary["gap_progression"]["gaps_filled"] == 1  # "base case" was filled
        assert summary["gap_progression"]["gaps_remaining"] == 1  # "call stack" remains
        assert summary["gap_progression"]["avg_followups_per_question"] == 1.0

        # Verify question summaries
        assert len(summary["question_summaries"]) == 3
        q1_summary = next(s for s in summary["question_summaries"] if s["question_id"] == str(q1_id))
        assert q1_summary["main_answer_score"] == 65.0
        assert q1_summary["follow_up_count"] == 1
        assert "base case" in q1_summary["initial_gaps"]
        assert "base case" not in q1_summary["final_gaps"]
        assert q1_summary["improvement"] is True

        # Verify LLM recommendations
        assert "strengths" in summary
        assert "weaknesses" in summary
        assert "study_recommendations" in summary
        assert "technique_tips" in summary
        assert isinstance(summary["strengths"], list)

        # Verify completion time
        assert "completion_time" in summary

    @pytest.mark.asyncio
    async def test_generate_summary_interview_not_found(
        self,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test error when interview not found."""
        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        with pytest.raises(ValueError, match="Interview .* not found"):
            await use_case.execute(uuid4())

    @pytest.mark.asyncio
    async def test_generate_summary_no_answers(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test summary with no answers -> 0.0 scores, empty recommendations."""
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        summary = await use_case.execute(sample_interview_adaptive.id)

        # Verify zero scores
        assert summary["overall_score"] == 0.0
        assert summary["theoretical_score_avg"] == 0.0
        assert summary["speaking_score_avg"] == 0.0

        # Verify empty structures
        assert summary["total_questions"] == 3
        assert summary["total_follow_ups"] == 0
        assert summary["gap_progression"]["questions_with_followups"] == 0

    @pytest.mark.asyncio
    async def test_generate_summary_no_followups(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test summary with no follow-ups -> gap progression shows 0."""
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create main question and answer
        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Good answer",
            is_voice=False,
            similarity_score=0.85,
            gaps={"concepts": [], "keywords": [], "confirmed": False},
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=85.0,
                semantic_similarity=0.85,
                completeness=0.9,
                relevance=0.95,
                sentiment="confident",
                reasoning="Strong",
                strengths=["Good"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        summary = await use_case.execute(sample_interview_adaptive.id)

        # Verify no follow-ups
        assert summary["total_follow_ups"] == 0
        assert summary["gap_progression"]["questions_with_followups"] == 0
        assert summary["gap_progression"]["gaps_filled"] == 0
        assert summary["gap_progression"]["gaps_remaining"] == 0
        assert summary["gap_progression"]["avg_followups_per_question"] == 0.0

    @pytest.mark.asyncio
    async def test_generate_summary_no_voice_metrics(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test speaking_score defaults to 50.0 when no voice metrics."""
        await mock_interview_repo.save(sample_interview_adaptive)

        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        # Answer without voice metrics
        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Good answer",
            is_voice=False,
            similarity_score=0.80,
            gaps={"concepts": [], "keywords": [], "confirmed": False},
            voice_metrics=None,  # No voice metrics
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=80.0,
                semantic_similarity=0.80,
                completeness=0.85,
                relevance=0.9,
                sentiment="confident",
                reasoning="Good",
                strengths=["Clear"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        summary = await use_case.execute(sample_interview_adaptive.id)

        # Verify default speaking score
        assert summary["speaking_score_avg"] == 50.0
        # Overall = 80 * 0.7 + 50 * 0.3 = 56 + 15 = 71.0
        assert summary["overall_score"] == 71.0

    @pytest.mark.asyncio
    async def test_generate_summary_missing_gaps_none(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test handles None gaps gracefully."""
        await mock_interview_repo.save(sample_interview_adaptive)

        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        # Answer with None gaps
        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Good answer",
            is_voice=False,
            similarity_score=0.85,
            gaps=None,  # None gaps
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=85.0,
                semantic_similarity=0.85,
                completeness=0.9,
                relevance=0.95,
                sentiment="confident",
                reasoning="Strong",
                strengths=["Good"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        use_case = GenerateSummaryUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        # Should not raise error
        summary = await use_case.execute(sample_interview_adaptive.id)
        assert summary["overall_score"] > 0.0


class TestMetricsCalculation:
    """Test aggregate metrics calculations."""

    @pytest.mark.asyncio
    async def test_calculate_metrics_overall_weighted_score(
        self,
        sample_interview_adaptive,
    ):
        """Test overall_score = 70% theoretical + 30% speaking."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        # Create answers with specific scores
        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Answer 1",
            is_voice=True,
            voice_metrics={"overall_score": 80.0},
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=70.0,
                semantic_similarity=0.7,
                completeness=0.7,
                relevance=0.8,
                sentiment="confident",
                reasoning="Test",
                strengths=["Good"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )

        answer2 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Answer 2",
            is_voice=True,
            voice_metrics={"overall_score": 60.0},
        )
        answer2.evaluate(
            AnswerEvaluation(
                score=90.0,
                semantic_similarity=0.9,
                completeness=0.9,
                relevance=0.95,
                sentiment="confident",
                reasoning="Test",
                strengths=["Excellent"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )

        metrics = use_case._calculate_aggregate_metrics([answer1, answer2])

        # Theoretical avg = (70 + 90) / 2 = 80.0
        # Speaking avg = (80 + 60) / 2 = 70.0
        # Overall = 80 * 0.7 + 70 * 0.3 = 56 + 21 = 77.0
        assert metrics["theoretical_avg"] == 80.0
        assert metrics["speaking_avg"] == 70.0
        assert metrics["overall_score"] == 77.0

    @pytest.mark.asyncio
    async def test_calculate_metrics_no_evaluated_answers(
        self,
        sample_interview_adaptive,
    ):
        """Test returns 0.0 for all scores when no evaluated answers."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        # Unevaluated answer
        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="No evaluation",
            is_voice=False,
        )

        metrics = use_case._calculate_aggregate_metrics([answer1])

        assert metrics["overall_score"] == 0.0
        assert metrics["theoretical_avg"] == 0.0
        assert metrics["speaking_avg"] == 0.0


class TestGapProgressionAnalysis:
    """Test gap progression analysis logic."""

    @pytest.mark.asyncio
    async def test_analyze_gap_progression_gaps_filled(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
    ):
        """Test gap progression tracks concepts filled after follow-ups."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        # Main answer with gaps
        main_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Main answer",
            is_voice=False,
            gaps={"concepts": ["base case", "call stack", "recursion depth"]},
        )

        # Follow-up answer with fewer gaps
        follow_up_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Follow-up answer",
            is_voice=False,
            gaps={"concepts": ["recursion depth"]},  # 2 gaps filled
        )

        question_groups = {
            sample_question_with_ideal_answer.id: {
                "question": sample_question_with_ideal_answer,
                "main_answer": main_answer,
                "follow_ups": [],
                "follow_up_answers": [follow_up_answer],
            }
        }

        progression = await use_case._analyze_gap_progression(question_groups)

        assert progression["questions_with_followups"] == 1
        assert progression["gaps_filled"] == 2  # "base case", "call stack" filled
        assert progression["gaps_remaining"] == 1  # "recursion depth" remains
        assert progression["avg_followups_per_question"] == 1.0

    @pytest.mark.asyncio
    async def test_analyze_gap_progression_no_followups(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
    ):
        """Test gap progression with no follow-ups."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        main_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Main answer",
            is_voice=False,
            gaps={"concepts": ["base case"]},
        )

        question_groups = {
            sample_question_with_ideal_answer.id: {
                "question": sample_question_with_ideal_answer,
                "main_answer": main_answer,
                "follow_ups": [],
                "follow_up_answers": [],  # No follow-ups
            }
        }

        progression = await use_case._analyze_gap_progression(question_groups)

        assert progression["questions_with_followups"] == 0
        assert progression["gaps_filled"] == 0
        assert progression["gaps_remaining"] == 0
        assert progression["avg_followups_per_question"] == 0.0

    @pytest.mark.asyncio
    async def test_analyze_gap_progression_multiple_followups(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
    ):
        """Test gap progression with multiple follow-ups."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        main_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Main answer",
            is_voice=False,
            gaps={"concepts": ["concept1", "concept2", "concept3"]},
        )

        fu_answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="FU 1",
            is_voice=False,
            gaps={"concepts": ["concept2", "concept3"]},
        )

        fu_answer2 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="FU 2",
            is_voice=False,
            gaps={"concepts": ["concept3"]},
        )

        fu_answer3 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="FU 3",
            is_voice=False,
            gaps={"concepts": []},  # All filled
        )

        question_groups = {
            sample_question_with_ideal_answer.id: {
                "question": sample_question_with_ideal_answer,
                "main_answer": main_answer,
                "follow_ups": [],
                "follow_up_answers": [fu_answer1, fu_answer2, fu_answer3],
            }
        }

        progression = await use_case._analyze_gap_progression(question_groups)

        assert progression["questions_with_followups"] == 1
        assert progression["gaps_filled"] == 3  # All 3 concepts filled
        assert progression["gaps_remaining"] == 0
        assert progression["avg_followups_per_question"] == 3.0


class TestQuestionSummaries:
    """Test per-question summary generation."""

    @pytest.mark.asyncio
    async def test_create_question_summaries_with_improvement(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
    ):
        """Test question summary shows improvement when gaps filled."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        main_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Main answer",
            is_voice=False,
            gaps={"concepts": ["gap1", "gap2"]},
        )
        main_answer.evaluate(
            AnswerEvaluation(
                score=60.0,
                semantic_similarity=0.6,
                completeness=0.6,
                relevance=0.8,
                sentiment="uncertain",
                reasoning="Weak",
                strengths=[],
                weaknesses=["Missing concepts"],
                improvement_suggestions=["Add details"],
            )
        )

        follow_up_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Follow-up answer",
            is_voice=False,
            gaps={"concepts": ["gap2"]},  # 1 gap filled
        )

        question_groups = {
            sample_question_with_ideal_answer.id: {
                "question": sample_question_with_ideal_answer,
                "main_answer": main_answer,
                "follow_ups": [],
                "follow_up_answers": [follow_up_answer],
            }
        }

        summaries = await use_case._create_question_summaries(question_groups)

        assert len(summaries) == 1
        summary = summaries[0]
        assert summary["question_id"] == str(sample_question_with_ideal_answer.id)
        assert summary["question_text"] == sample_question_with_ideal_answer.text
        assert summary["main_answer_score"] == 60.0
        assert summary["follow_up_count"] == 1
        assert summary["initial_gaps"] == ["gap1", "gap2"]
        assert summary["final_gaps"] == ["gap2"]
        assert summary["improvement"] is True  # len(final) < len(initial)

    @pytest.mark.asyncio
    async def test_create_question_summaries_no_improvement(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
    ):
        """Test question summary shows no improvement when gaps remain same."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
        )

        main_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Main answer",
            is_voice=False,
            gaps={"concepts": ["gap1"]},
        )
        main_answer.evaluate(
            AnswerEvaluation(
                score=65.0,
                semantic_similarity=0.65,
                completeness=0.65,
                relevance=0.8,
                sentiment="uncertain",
                reasoning="OK",
                strengths=[],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )

        follow_up_answer = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Follow-up answer",
            is_voice=False,
            gaps={"concepts": ["gap1"]},  # Same gaps
        )

        question_groups = {
            sample_question_with_ideal_answer.id: {
                "question": sample_question_with_ideal_answer,
                "main_answer": main_answer,
                "follow_ups": [],
                "follow_up_answers": [follow_up_answer],
            }
        }

        summaries = await use_case._create_question_summaries(question_groups)

        assert len(summaries) == 1
        summary = summaries[0]
        assert summary["improvement"] is False  # len(final) == len(initial)


class TestLLMRecommendations:
    """Test LLM-powered recommendations generation."""

    @pytest.mark.asyncio
    async def test_generate_recommendations_called_with_context(
        self,
        sample_interview_adaptive,
        mock_llm,
    ):
        """Test LLM.generate_interview_recommendations called with correct context."""
        from src.application.use_cases.generate_summary import GenerateSummaryUseCase
        from unittest.mock import AsyncMock

        # Mock LLM with explicit return
        mock_llm.generate_interview_recommendations = AsyncMock(
            return_value={
                "strengths": ["Clear communication", "Good examples"],
                "weaknesses": ["Missing depth", "Needs more detail"],
                "study_topics": ["Recursion patterns", "Algorithm complexity"],
                "technique_tips": ["Speak slower", "Use more examples"],
            }
        )

        use_case = GenerateSummaryUseCase(
            interview_repository=None,  # type: ignore
            answer_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=mock_llm,
        )

        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=uuid4(),
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Answer 1",
            is_voice=False,
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=75.0,
                semantic_similarity=0.75,
                completeness=0.8,
                relevance=0.9,
                sentiment="confident",
                reasoning="Good",
                strengths=["Clear"],
                weaknesses=["Could improve"],
                improvement_suggestions=["Add details"],
            )
        )

        gap_progression = {
            "questions_with_followups": 1,
            "gaps_filled": 2,
            "gaps_remaining": 1,
            "avg_followups_per_question": 1.0,
        }

        recommendations = await use_case._generate_recommendations(
            sample_interview_adaptive,
            [answer1],
            gap_progression,
        )

        # Verify LLM called once
        assert mock_llm.generate_interview_recommendations.call_count == 1

        # Verify context structure
        call_args = mock_llm.generate_interview_recommendations.call_args
        context = call_args[0][0]
        assert context["interview_id"] == str(sample_interview_adaptive.id)
        assert context["total_answers"] == 1
        assert context["gap_progression"] == gap_progression
        assert len(context["evaluations"]) == 1

        # Verify recommendations structure
        assert "strengths" in recommendations
        assert "weaknesses" in recommendations
        assert "study_topics" in recommendations
        assert "technique_tips" in recommendations
</file>

<file path="alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the tzdata library which can be installed by adding
# `alembic[tz]` to the pip requirements.
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# use sequential revision IDs (0001, 0002, etc.) instead of hash-based IDs
# set to 'true' to enable automatic sequential numbering for new migrations
use_sequential_revisions = true

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
# Database URL is loaded from environment variables in env.py
# sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="alembic/versions/0001_create_tables.py">
"""Create all database tables

Revision ID: 0001
Revises:
Create Date: 2025-11-12 00:00:00.000000
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '0001'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema - create all tables with all columns."""

    # Create candidates table
    op.create_table(
        'candidates',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('email', sa.String(255), nullable=False, unique=True),
        sa.Column('cv_file_path', sa.String(500), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
    )
    op.create_index('idx_candidates_email', 'candidates', ['email'])
    op.create_index('idx_candidates_created_at', 'candidates', ['created_at'])

    # Create questions table
    # Note: reference_answer column is excluded (was dropped in migration 0005)
    op.create_table(
        'questions',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('text', sa.Text(), nullable=False),
        sa.Column('question_type', sa.String(50), nullable=False),
        sa.Column('difficulty', sa.String(50), nullable=False),
        sa.Column('skills', postgresql.ARRAY(sa.String(100)), nullable=False, server_default='{}'),
        sa.Column('tags', postgresql.ARRAY(sa.String(100)), nullable=False, server_default='{}'),
        sa.Column('evaluation_criteria', sa.Text(), nullable=True),
        sa.Column('version', sa.Integer(), nullable=False, server_default='1'),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        # Added in 0003
        sa.Column('ideal_answer', sa.Text(), nullable=True),
        sa.Column('rationale', sa.Text(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
    )
    op.create_index('idx_questions_type', 'questions', ['question_type'])
    op.create_index('idx_questions_difficulty', 'questions', ['difficulty'])
    op.create_index('idx_questions_skills', 'questions', ['skills'], postgresql_using='gin')
    op.create_index('idx_questions_tags', 'questions', ['tags'], postgresql_using='gin')

    # Create cv_analyses table
    op.create_table(
        'cv_analyses',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('cv_file_path', sa.String(500), nullable=False),
        sa.Column('extracted_text', sa.Text(), nullable=False),
        sa.Column('skills', postgresql.JSONB(), nullable=False, server_default='[]'),
        sa.Column('work_experience_years', sa.Float(), nullable=True),
        sa.Column('education_level', sa.String(100), nullable=True),
        sa.Column('suggested_topics', postgresql.ARRAY(sa.String(200)), nullable=False, server_default='{}'),
        sa.Column('suggested_difficulty', sa.String(50), nullable=False, server_default="'medium'"),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('summary', sa.Text(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=False, server_default='{}'),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
    )
    op.create_index('idx_cv_analyses_candidate_id', 'cv_analyses', ['candidate_id'])
    op.create_index('idx_cv_analyses_created_at', 'cv_analyses', ['created_at'])

    # Create interviews table
    op.create_table(
        'interviews',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('status', sa.String(50), nullable=False),
        sa.Column('cv_analysis_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('question_ids', postgresql.ARRAY(postgresql.UUID(as_uuid=True)), nullable=False, server_default='{}'),
        sa.Column('answer_ids', postgresql.ARRAY(postgresql.UUID(as_uuid=True)), nullable=False, server_default='{}'),
        sa.Column('current_question_index', sa.Integer(), nullable=False, server_default='0'),
        # Added in 0003
        sa.Column('plan_metadata', postgresql.JSONB(astext_type=sa.Text()), server_default='{}', nullable=False),
        sa.Column('adaptive_follow_ups', postgresql.ARRAY(postgresql.UUID(as_uuid=True)), server_default='{}', nullable=False),
        sa.Column('current_parent_question_id', postgresql.UUID(as_uuid=True), nullable=True),
        sa.Column('current_followup_count', sa.Integer(), nullable=False, server_default='0'),
        sa.Column('started_at', sa.DateTime(), nullable=True),
        sa.Column('completed_at', sa.DateTime(), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['cv_analysis_id'], ['cv_analyses.id'], ondelete='SET NULL'),
    )
    op.create_index('idx_interviews_candidate_id', 'interviews', ['candidate_id'])
    op.create_index('idx_interviews_status', 'interviews', ['status'])
    op.create_index('idx_interviews_created_at', 'interviews', ['created_at'])

    # Create answers table
    op.create_table(
        'answers',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('interview_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('question_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('candidate_id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('text', sa.Text(), nullable=False),
        sa.Column('is_voice', sa.Boolean(), nullable=False, server_default='false'),
        sa.Column('audio_file_path', sa.String(500), nullable=True),
        sa.Column('duration_seconds', sa.Float(), nullable=True),
        sa.Column('evaluation', postgresql.JSONB(), nullable=True),
        sa.Column('embedding', postgresql.ARRAY(sa.Float()), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=False, server_default='{}'),
        # Added in 0003
        sa.Column('similarity_score', sa.Float(), nullable=True),
        sa.Column('gaps', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('evaluated_at', sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(['interview_id'], ['interviews.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['question_id'], ['questions.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['candidate_id'], ['candidates.id'], ondelete='CASCADE'),
    )
    op.create_index('idx_answers_interview_id', 'answers', ['interview_id'])
    op.create_index('idx_answers_question_id', 'answers', ['question_id'])
    op.create_index('idx_answers_candidate_id', 'answers', ['candidate_id'])
    op.create_index('idx_answers_created_at', 'answers', ['created_at'])
    # Indexes added in 0003
    op.create_index(
        'idx_answers_similarity_score',
        'answers',
        ['similarity_score'],
        postgresql_where=sa.text('similarity_score IS NOT NULL'),
    )
    op.create_index(
        'idx_answers_gaps',
        'answers',
        ['gaps'],
        postgresql_using='gin',
        postgresql_where=sa.text('gaps IS NOT NULL'),
    )
    # Constraint added in 0003
    op.create_check_constraint(
        'check_similarity_score_bounds',
        'answers',
        'similarity_score IS NULL OR (similarity_score >= 0 AND similarity_score <= 1)',
    )

    # Create follow_up_questions table (added in 0003)
    op.create_table(
        'follow_up_questions',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, nullable=False),
        sa.Column(
            'parent_question_id',
            postgresql.UUID(as_uuid=True),
            sa.ForeignKey('questions.id', ondelete='CASCADE'),
            nullable=False,
        ),
        sa.Column(
            'interview_id',
            postgresql.UUID(as_uuid=True),
            sa.ForeignKey('interviews.id', ondelete='CASCADE'),
            nullable=False,
        ),
        sa.Column('text', sa.Text(), nullable=False),
        sa.Column('generated_reason', sa.Text(), nullable=False),
        sa.Column('order_in_sequence', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.DateTime(), nullable=False),
    )
    op.create_index(
        'idx_follow_up_questions_parent_question_id',
        'follow_up_questions',
        ['parent_question_id'],
    )
    op.create_index(
        'idx_follow_up_questions_interview_id',
        'follow_up_questions',
        ['interview_id'],
    )
    op.create_index(
        'idx_follow_up_questions_created_at',
        'follow_up_questions',
        ['created_at'],
    )


def downgrade() -> None:
    """Downgrade schema - drop all tables."""
    op.drop_table('follow_up_questions')
    op.drop_table('answers')
    op.drop_table('interviews')
    op.drop_table('cv_analyses')
    op.drop_table('questions')
    op.drop_table('candidates')
</file>

<file path="alembic/versions/0002_insert_seed_data.py">
"""Insert all seed data

Revision ID: 0002
Revises: 0001
Create Date: 2025-11-12 00:00:00.000000
"""
from typing import Sequence, Union
from datetime import datetime, timedelta
import uuid

from alembic import op
import sqlalchemy as sa
from sqlalchemy import Table, Column, MetaData
from sqlalchemy.dialects.postgresql import UUID, ARRAY, JSONB
from sqlalchemy import String, Text, Integer, Float, Boolean, DateTime


# revision identifiers, used by Alembic.
revision: str = '0002'
down_revision: Union[str, Sequence[str], None] = '0001'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Seed all data for development and testing."""

    # Get connection
    conn = op.get_bind()
    metadata = MetaData()
    now = datetime.utcnow()

    # Define table schemas for bulk insert
    candidates_table = Table(
        'candidates', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('name', String),
        Column('email', String),
        Column('cv_file_path', String),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    questions_table = Table(
        'questions', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('text', Text),
        Column('question_type', String),
        Column('difficulty', String),
        Column('skills', ARRAY(String)),
        Column('tags', ARRAY(String)),
        Column('evaluation_criteria', Text),
        Column('ideal_answer', Text),
        Column('rationale', Text),
        Column('version', Integer),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    cv_analyses_table = Table(
        'cv_analyses', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('cv_file_path', String),
        Column('extracted_text', Text),
        Column('skills', JSONB),
        Column('work_experience_years', Float),
        Column('education_level', String),
        Column('suggested_topics', ARRAY(String)),
        Column('suggested_difficulty', String),
        Column('summary', Text),
        Column('metadata', JSONB),
        Column('created_at', DateTime),
    )

    interviews_table = Table(
        'interviews', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('status', String),
        Column('cv_analysis_id', UUID(as_uuid=True)),
        Column('question_ids', ARRAY(UUID(as_uuid=True))),
        Column('answer_ids', ARRAY(UUID(as_uuid=True))),
        Column('current_question_index', Integer),
        Column('plan_metadata', JSONB),
        Column('adaptive_follow_ups', ARRAY(UUID(as_uuid=True))),
        Column('started_at', DateTime),
        Column('completed_at', DateTime),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
    )

    answers_table = Table(
        'answers', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('interview_id', UUID(as_uuid=True)),
        Column('question_id', UUID(as_uuid=True)),
        Column('candidate_id', UUID(as_uuid=True)),
        Column('text', Text),
        Column('is_voice', Boolean),
        Column('audio_file_path', String),
        Column('duration_seconds', Float),
        Column('evaluation', JSONB),
        Column('similarity_score', Float),
        Column('gaps', JSONB),
        Column('metadata', JSONB),
        Column('created_at', DateTime),
        Column('evaluated_at', DateTime),
    )

    follow_up_questions_table = Table(
        'follow_up_questions', metadata,
        Column('id', UUID(as_uuid=True)),
        Column('parent_question_id', UUID(as_uuid=True)),
        Column('interview_id', UUID(as_uuid=True)),
        Column('text', Text),
        Column('generated_reason', Text),
        Column('order_in_sequence', Integer),
        Column('created_at', DateTime),
    )

    # =============================================
    # SEED DATA - CANDIDATES (10 total: 3 + 7)
    # =============================================
    op.bulk_insert(candidates_table, [
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'name': 'John Doe',
            'email': 'john.doe@example.com',
            'cv_file_path': '/uploads/cvs/john_doe_cv.pdf',
            'created_at': now - timedelta(days=30),
            'updated_at': now - timedelta(days=30),
        },
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'name': 'Jane Smith',
            'email': 'jane.smith@example.com',
            'cv_file_path': '/uploads/cvs/jane_smith_cv.pdf',
            'created_at': now - timedelta(days=25),
            'updated_at': now - timedelta(days=25),
        },
        {
            'id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'name': 'Bob Johnson',
            'email': 'bob.johnson@example.com',
            'cv_file_path': '/uploads/cvs/bob_johnson_cv.pdf',
            'created_at': now - timedelta(days=20),
            'updated_at': now - timedelta(days=20),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'name': 'Alice Chen',
            'email': 'alice.chen@example.com',
            'cv_file_path': '/uploads/cvs/alice_chen_cv.pdf',
            'created_at': now - timedelta(days=14),
            'updated_at': now - timedelta(days=14),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'name': 'Michael Rodriguez',
            'email': 'michael.rodriguez@example.com',
            'cv_file_path': '/uploads/cvs/michael_rodriguez_cv.pdf',
            'created_at': now - timedelta(days=12),
            'updated_at': now - timedelta(days=12),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'name': 'Sarah Williams',
            'email': 'sarah.williams@example.com',
            'cv_file_path': '/uploads/cvs/sarah_williams_cv.pdf',
            'created_at': now - timedelta(days=10),
            'updated_at': now - timedelta(days=10),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'name': 'David Kim',
            'email': 'david.kim@example.com',
            'cv_file_path': '/uploads/cvs/david_kim_cv.pdf',
            'created_at': now - timedelta(days=8),
            'updated_at': now - timedelta(days=8),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440005'),
            'name': 'Emily Thompson',
            'email': 'emily.thompson@example.com',
            'cv_file_path': '/uploads/cvs/emily_thompson_cv.pdf',
            'created_at': now - timedelta(days=6),
            'updated_at': now - timedelta(days=6),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440006'),
            'name': 'James Anderson',
            'email': 'james.anderson@example.com',
            'cv_file_path': '/uploads/cvs/james_anderson_cv.pdf',
            'created_at': now - timedelta(days=4),
            'updated_at': now - timedelta(days=4),
        },
        {
            'id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'name': 'Lisa Martinez',
            'email': 'lisa.martinez@example.com',
            'cv_file_path': '/uploads/cvs/lisa_martinez_cv.pdf',
            'created_at': now - timedelta(days=2),
            'updated_at': now - timedelta(days=2),
        },
    ])

    print("[OK] Seeded 10 candidates")

    # =============================================
    # SEED DATA - QUESTIONS (41 total: 23 + 18)
    # =============================================
    questions_data = [
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'text': 'What is the difference between var, let, and const in JavaScript?',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['JavaScript', 'ES6', 'Variables'],
            'tags': ['javascript', 'basics', 'es6'],
            'evaluation_criteria': 'Check understanding of scope, hoisting, and immutability concepts',
            'ideal_answer': 'var is function-scoped and can be redeclared. It is hoisted and initialized as undefined. let and const are block-scoped (ES6) and cannot be redeclared in the same scope. let can be reassigned, while const cannot be reassigned after declaration. const does not make objects/arrays immutable, only prevents reassignment of the binding. Best practice: use const by default, let when reassignment is needed, avoid var. The Temporal Dead Zone (TDZ) prevents accessing let/const before declaration.',
            'rationale': 'Tests fundamental JavaScript knowledge essential for writing modern ES6+ code and avoiding common scoping pitfalls.',
            'version': 1,
            'created_at': now - timedelta(days=60),
            'updated_at': now - timedelta(days=60),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440002'),
            'text': 'Explain what REST API is and its main HTTP methods.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['API', 'REST', 'HTTP'],
            'tags': ['api', 'rest', 'http'],
            'evaluation_criteria': 'Evaluate understanding of RESTful principles and HTTP verbs',
            'ideal_answer': 'REST (Representational State Transfer) is an architectural style for designing web services. Key principles: stateless communication, resource-based URLs, standard HTTP methods, JSON/XML data format. Main HTTP methods: GET (retrieve data, idempotent), POST (create resource, not idempotent), PUT (update/replace entire resource, idempotent), PATCH (partial update, not idempotent), DELETE (remove resource, idempotent). REST uses status codes (200 OK, 201 Created, 404 Not Found, 500 Error) and follows uniform interface principles.',
            'rationale': 'Evaluates foundational API design knowledge critical for building and consuming web services.',
            'version': 1,
            'created_at': now - timedelta(days=60),
            'updated_at': now - timedelta(days=60),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'text': 'How does async/await work in JavaScript? Compare it with Promises.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['JavaScript', 'Async', 'Promises'],
            'tags': ['javascript', 'async', 'promises'],
            'evaluation_criteria': 'Assess understanding of asynchronous programming',
            'ideal_answer': 'async/await is syntactic sugar built on Promises that makes asynchronous code look synchronous. An async function always returns a Promise. await pauses execution until the Promise resolves/rejects. Comparison: Promises use .then()/.catch() chains which can lead to callback hell. async/await provides cleaner, more readable code with try/catch error handling. Both handle asynchronous operations, but async/await is easier to debug and read. Under the hood, async/await still uses Promises and the event loop. Use Promise.all() for parallel execution with async/await.',
            'rationale': 'Tests understanding of modern JavaScript asynchronous patterns essential for handling async operations effectively.',
            'version': 1,
            'created_at': now - timedelta(days=55),
            'updated_at': now - timedelta(days=55),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
            'text': 'What is a closure in Python? Provide an example.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Python', 'Closures', 'Functional Programming'],
            'tags': ['python', 'closures', 'functional'],
            'evaluation_criteria': 'Check understanding of lexical scoping and closure mechanics',
            'ideal_answer': 'A closure is a nested function that captures and remembers variables from its enclosing (outer) scope even after the outer function has finished executing. It combines a function with its lexical environment. Example: def outer(x): def inner(y): return x + y; return inner. When outer(10) is called, it returns inner which "remembers" x=10. Closures are useful for: data privacy, function factories, decorators, and maintaining state. They enable functional programming patterns in Python.',
            'rationale': 'Evaluates understanding of Python\'s scoping rules and functional programming concepts used in advanced patterns.',
            'version': 1,
            'created_at': now - timedelta(days=58),
            'updated_at': now - timedelta(days=58),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440005'),
            'text': 'Describe the difference between list comprehension and generator expressions in Python.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Python', 'List Comprehension', 'Generators'],
            'tags': ['python', 'list-comprehension', 'generators'],
            'evaluation_criteria': 'Evaluate understanding of memory efficiency and iteration patterns',
            'ideal_answer': 'List comprehension [x*2 for x in range(10)] creates a list immediately, storing all values in memory. Generator expression (x*2 for x in range(10)) creates an iterator that generates values lazily on-demand, using minimal memory. Differences: list comprehension uses square brackets [], generator uses parentheses (). List comprehension returns a list, generator returns a generator object. Use list comprehension when you need the full list or multiple iterations. Use generators for large datasets, one-time iteration, or memory-constrained scenarios. Generators are more memory-efficient but can only be iterated once.',
            'rationale': 'Tests understanding of Python iteration patterns and memory optimization techniques important for handling large datasets.',
            'version': 1,
            'created_at': now - timedelta(days=57),
            'updated_at': now - timedelta(days=57),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
            'text': 'What is the difference between SQL JOIN types: INNER, LEFT, RIGHT, and FULL OUTER?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['SQL', 'Database', 'JOIN'],
            'tags': ['sql', 'database', 'join'],
            'evaluation_criteria': 'Assess knowledge of SQL JOIN operations and their use cases',
            'ideal_answer': 'INNER JOIN returns only matching rows from both tables. LEFT JOIN returns all rows from left table plus matching rows from right (NULL for non-matching). RIGHT JOIN returns all rows from right table plus matching rows from left (NULL for non-matching). FULL OUTER JOIN returns all rows from both tables, with NULLs where no match exists. Use INNER JOIN for strict relationships. Use LEFT/RIGHT JOIN when you need all records from one side. Use FULL OUTER JOIN to see all data from both tables. LEFT JOIN is most common in practice.',
            'rationale': 'Evaluates essential SQL knowledge for querying relational databases and understanding data relationships.',
            'version': 1,
            'created_at': now - timedelta(days=56),
            'updated_at': now - timedelta(days=56),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
            'text': 'Explain what is a React Hook and name three common hooks you have used.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['React', 'Hooks', 'Frontend'],
            'tags': ['react', 'hooks', 'frontend'],
            'evaluation_criteria': 'Evaluate understanding of React Hooks and their practical application',
            'ideal_answer': 'React Hooks are functions that let you use state and lifecycle features in functional components. They enable functional components to have stateful logic without class components. Common hooks: 1) useState - manages component state, 2) useEffect - handles side effects (API calls, subscriptions, DOM manipulation), 3) useContext - accesses React context values. Other important hooks: useMemo (memoize expensive calculations), useCallback (memoize functions), useRef (access DOM or persist values). Hooks follow rules: only call at top level, only in React functions. They make code more reusable and easier to test.',
            'rationale': 'Tests fundamental React knowledge essential for modern React development and functional component patterns.',
            'version': 1,
            'created_at': now - timedelta(days=54),
            'updated_at': now - timedelta(days=54),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            'text': 'How would you design a system to handle 1 million requests per second?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['System Design', 'Scalability', 'Architecture'],
            'tags': ['system-design', 'scalability', 'architecture'],
            'evaluation_criteria': 'Assess system design thinking, scalability patterns, and trade-off considerations',
            'ideal_answer': 'Design horizontally scalable architecture: 1) Load balancers (multiple layers) distribute traffic, 2) Application servers (stateless, auto-scaling groups), 3) Caching layers (Redis cluster for hot data, CDN for static content), 4) Database (sharding, read replicas, connection pooling), 5) Message queues (Kafka/RabbitMQ) for async processing, 6) Rate limiting and circuit breakers, 7) Monitoring and distributed tracing. Key principles: horizontal scaling, caching at multiple layers, database optimization, async processing, fault tolerance. Estimate: ~10,000 servers needed (100 req/s per server). Consider geographic distribution, data consistency requirements, and cost optimization.',
            'rationale': 'Evaluates system design skills and ability to architect scalable systems handling high traffic loads.',
            'version': 1,
            'created_at': now - timedelta(days=52),
            'updated_at': now - timedelta(days=52),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440009'),
            'text': 'What is the difference between process and thread? When would you use each?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Operating Systems', 'Concurrency', 'Threading'],
            'tags': ['operating-systems', 'concurrency', 'threading'],
            'evaluation_criteria': 'Evaluate understanding of concurrency models and their trade-offs',
            'ideal_answer': 'A process is an independent program execution unit with its own memory space, isolated from other processes. A thread is a lightweight execution unit within a process, sharing memory space with other threads in the same process. Differences: processes have separate memory (isolated), threads share memory (need synchronization). Process creation is heavier, thread creation is lighter. Process failure doesn\'t affect others, thread failure can crash the process. Use processes for: isolation, security, independent tasks, CPU-bound parallel processing. Use threads for: shared data access, I/O-bound tasks, lightweight concurrency, faster communication.',
            'rationale': 'Tests understanding of concurrency fundamentals essential for building efficient multi-threaded and multi-process applications.',
            'version': 1,
            'created_at': now - timedelta(days=51),
            'updated_at': now - timedelta(days=51),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            'text': 'Explain the SOLID principles in object-oriented programming.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['OOP', 'Design Patterns', 'SOLID'],
            'tags': ['oop', 'design-patterns', 'solid'],
            'evaluation_criteria': 'Check understanding of OOP principles and their practical application',
            'ideal_answer': 'SOLID principles guide object-oriented design: S - Single Responsibility (class should have one reason to change, one responsibility), O - Open/Closed (open for extension, closed for modification, use inheritance/interfaces), L - Liskov Substitution (derived classes must be substitutable for base classes without breaking functionality), I - Interface Segregation (clients shouldn\'t depend on interfaces they don\'t use, prefer specific interfaces), D - Dependency Inversion (depend on abstractions, not concretions, high-level modules shouldn\'t depend on low-level). These principles promote maintainable, flexible, and testable code.',
            'rationale': 'Evaluates understanding of fundamental OOP design principles essential for writing maintainable and scalable code.',
            'version': 1,
            'created_at': now - timedelta(days=50),
            'updated_at': now - timedelta(days=50),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440011'),
            'text': 'What is Docker and how does it differ from a virtual machine?',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Docker', 'Containers', 'DevOps'],
            'tags': ['docker', 'containers', 'devops'],
            'evaluation_criteria': 'Assess understanding of containerization vs virtualization',
            'ideal_answer': 'Docker is a containerization platform that packages applications with dependencies into lightweight, portable containers. Differences: VMs virtualize hardware (full OS, hypervisor, heavy ~GBs), containers virtualize OS (share host kernel, lightweight ~MBs). VMs have slower startup (minutes), containers start in seconds. VMs provide stronger isolation, containers share kernel (less isolation). VMs use more resources, containers are resource-efficient. VMs are better for different OS requirements, containers are better for microservices, CI/CD, and consistent environments. Docker uses images (templates) to create containers (running instances).',
            'rationale': 'Tests understanding of modern deployment technologies and containerization concepts essential for DevOps practices.',
            'version': 1,
            'created_at': now - timedelta(days=49),
            'updated_at': now - timedelta(days=49),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440012'),
            'text': 'Explain the difference between time complexity and space complexity with examples.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Algorithms', 'Big O', 'Complexity Analysis'],
            'tags': ['algorithms', 'big-o', 'complexity'],
            'evaluation_criteria': 'Evaluate algorithmic thinking and complexity analysis skills',
            'ideal_answer': 'Time complexity measures how execution time grows with input size (e.g., O(n) linear, O(nÂ²) quadratic, O(log n) logarithmic). Space complexity measures how memory usage grows with input size. Examples: Linear search is O(n) time, O(1) space. Binary search is O(log n) time, O(1) space. Merge sort is O(n log n) time, O(n) space. Bubble sort is O(nÂ²) time, O(1) space. Trade-offs: sometimes optimize time at expense of space (caching) or vice versa (streaming). Big O notation describes worst-case asymptotic behavior. Consider both when choosing algorithms.',
            'rationale': 'Evaluates algorithmic analysis skills essential for writing efficient code and making informed algorithm choices.',
            'version': 1,
            'created_at': now - timedelta(days=48),
            'updated_at': now - timedelta(days=48),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440013'),
            'text': 'What is the difference between REST and GraphQL? When would you choose one over the other?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['API', 'REST', 'GraphQL'],
            'tags': ['api', 'rest', 'graphql'],
            'evaluation_criteria': 'Assess understanding of API design patterns and trade-offs',
            'ideal_answer': 'REST uses multiple endpoints (resources), fixed responses, HTTP methods (GET/POST/PUT/DELETE), and may over/under-fetch data. GraphQL uses single endpoint, flexible queries (client specifies needed fields), single POST request, and fetches exactly what\'s needed. REST is simpler, cacheable (HTTP), mature ecosystem. GraphQL reduces over-fetching, enables rapid frontend iteration, strong typing. Choose REST for: simple CRUD, caching needs, established patterns, microservices. Choose GraphQL for: complex data relationships, mobile apps (bandwidth), rapid prototyping, when clients need different data shapes. Consider team expertise and infrastructure.',
            'rationale': 'Tests understanding of modern API design patterns and ability to make informed technology choices based on requirements.',
            'version': 1,
            'created_at': now - timedelta(days=47),
            'updated_at': now - timedelta(days=47),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440014'),
            'text': 'Describe how Node.js handles asynchronous operations. What is the event loop?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['Node.js', 'Event Loop', 'Asynchronous'],
            'tags': ['nodejs', 'event-loop', 'async'],
            'evaluation_criteria': 'Evaluate deep understanding of Node.js internals and asynchronous execution',
            'ideal_answer': 'Node.js uses an event-driven, non-blocking I/O model with a single-threaded event loop. The event loop continuously checks: 1) Call stack (synchronous code), 2) Callback queue (async callbacks), 3) Microtask queue (Promises, queueMicrotask). When stack is empty, event loop moves callbacks to stack. Phases: timers (setTimeout/setInterval), pending callbacks, poll (I/O), check (setImmediate), close callbacks. Microtasks have priority over regular callbacks. This allows Node.js to handle thousands of concurrent connections efficiently despite being single-threaded. Use worker threads for CPU-intensive tasks to avoid blocking the event loop.',
            'rationale': 'Evaluates deep understanding of Node.js runtime internals critical for writing efficient, non-blocking code and debugging async issues.',
            'version': 1,
            'created_at': now - timedelta(days=46),
            'updated_at': now - timedelta(days=46),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440015'),
            'text': 'What is unit testing and why is it important? Name a testing framework you have used.',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Testing', 'Unit Testing', 'QA'],
            'tags': ['testing', 'unit-testing', 'qa'],
            'evaluation_criteria': 'Check understanding of testing principles and practical experience',
            'ideal_answer': 'Unit testing tests individual functions/components in isolation with mocked dependencies. Importance: catches bugs early, enables refactoring with confidence, documents expected behavior, improves code design (testability), reduces debugging time, supports continuous integration. Frameworks: Jest (JavaScript), pytest (Python), JUnit (Java), Mocha/Chai (JavaScript), unittest (Python). Good unit tests are: fast, isolated, repeatable, self-validating, timely (written before/during development). Follow AAA pattern: Arrange (setup), Act (execute), Assert (verify).',
            'rationale': 'Tests understanding of software quality practices essential for building reliable, maintainable applications.',
            'version': 1,
            'created_at': now - timedelta(days=45),
            'updated_at': now - timedelta(days=45),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            'text': 'Tell me about a time when you had to work under pressure to meet a deadline.',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Time Management', 'Stress Management', 'Communication'],
            'tags': ['behavioral', 'deadlines', 'pressure'],
            'evaluation_criteria': 'Assess ability to handle pressure, prioritize tasks, and communicate effectively under stress',
            'ideal_answer': 'Use STAR method: Situation - describe tight deadline context (e.g., production bug, client demo). Task - explain what needed to be accomplished and constraints. Action - detail approach: break down work, prioritize critical path, communicate with stakeholders about risks/timeline, focus on MVP, eliminate distractions, ask for help if needed, maintain code quality standards. Result - outcome: deadline met, lessons learned, process improvements. Show resilience, systematic thinking, and ability to maintain quality under pressure. Avoid working excessive hours as primary solution.',
            'rationale': 'Evaluates stress management, prioritization skills, and ability to deliver quality work under pressure - critical for real-world development.',
            'version': 1,
            'created_at': now - timedelta(days=44),
            'updated_at': now - timedelta(days=44),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440017'),
            'text': 'Describe a situation where you had to learn a new technology quickly for a project.',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'EASY',
            'skills': ['Learning', 'Adaptability', 'Problem Solving'],
            'tags': ['behavioral', 'learning', 'adaptability'],
            'evaluation_criteria': 'Evaluate learning agility, initiative, and ability to adapt to new technologies',
            'ideal_answer': 'Use STAR method: Situation - project requiring unfamiliar technology (e.g., new framework, language, tool). Task - learn enough to be productive quickly. Action - learning strategy: official documentation, tutorials, hands-on practice (build small projects), code examples, community resources (Stack Overflow, forums), pair programming with experienced developers, focused learning (core concepts first). Result - became productive within timeframe, delivered feature, continued learning. Show self-directed learning, systematic approach, and ability to apply knowledge practically. Demonstrate growth mindset.',
            'rationale': 'Assesses learning agility and adaptability - essential traits for developers in rapidly evolving technology landscape.',
            'version': 1,
            'created_at': now - timedelta(days=43),
            'updated_at': now - timedelta(days=43),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440018'),
            'text': 'Give an example of a time when you disagreed with a team member. How did you resolve it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Conflict Resolution', 'Teamwork', 'Communication'],
            'tags': ['behavioral', 'conflict', 'teamwork'],
            'evaluation_criteria': 'Assess conflict resolution skills and ability to work collaboratively',
            'ideal_answer': 'Use STAR method: Situation - describe disagreement objectively (technical approach, design decision, process). Task - resolve conflict while maintaining team harmony and project progress. Action - approach: listen actively to understand their perspective, present your viewpoint with evidence/data, find common ground, propose compromise or alternative solution, involve team lead if needed, focus on problem not person, maintain professionalism. Result - reached agreement, relationship maintained/improved, project benefited. Show emotional intelligence, communication skills, and focus on team success over being right.',
            'rationale': 'Evaluates interpersonal skills and ability to navigate workplace conflicts constructively - essential for collaborative development.',
            'version': 1,
            'created_at': now - timedelta(days=42),
            'updated_at': now - timedelta(days=42),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440019'),
            'text': 'What is your biggest weakness as a developer, and how are you working to improve it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Self-Awareness', 'Growth Mindset', 'Honesty'],
            'tags': ['behavioral', 'self-reflection', 'growth'],
            'evaluation_criteria': 'Evaluate self-awareness, growth mindset, and honesty',
            'ideal_answer': 'Be honest but strategic: choose a real weakness that\'s not critical for the role, show self-awareness, and demonstrate active improvement. Example: "I tend to dive deep into technical details and sometimes need to step back for the bigger picture. I\'m improving by: setting time limits for research, asking for feedback on communication, focusing on business impact. I\'ve seen progress in my recent projects where I balanced depth with delivery." Avoid: saying you have no weaknesses, mentioning critical skills gaps, or weaknesses you\'re not addressing. Show growth mindset and commitment to continuous improvement.',
            'rationale': 'Tests self-awareness, honesty, and growth mindset - indicators of mature professional development and coachability.',
            'version': 1,
            'created_at': now - timedelta(days=41),
            'updated_at': now - timedelta(days=41),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440020'),
            'text': 'If you discovered a critical bug in production right before a major release, what would you do?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'MEDIUM',
            'skills': ['Problem Solving', 'Risk Management', 'Decision Making'],
            'tags': ['situational', 'bug', 'production'],
            'evaluation_criteria': 'Check ability to make decisions under pressure while considering business and technical implications',
            'ideal_answer': 'Immediate steps: 1) Assess severity and impact (data loss, security, user experience), 2) Document the bug clearly, 3) Notify team lead/manager immediately, 4) Evaluate options: fix now (if quick), delay release (if critical), release with known issue (if low impact with workaround). Decision factors: bug severity, fix complexity, release importance, user impact, business cost. Process: communicate transparently with stakeholders, work with team on fix, test thoroughly, deploy fix or delay release, conduct post-mortem. Show: quick assessment, clear communication, risk evaluation, team collaboration, focus on user/business impact over personal reputation.',
            'rationale': 'Evaluates crisis management, decision-making under pressure, and ability to balance technical and business considerations.',
            'version': 1,
            'created_at': now - timedelta(days=40),
            'updated_at': now - timedelta(days=40),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440021'),
            'text': 'You have limited time to complete a feature. How do you decide what to prioritize?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'EASY',
            'skills': ['Prioritization', 'Time Management', 'Analytical Thinking'],
            'tags': ['situational', 'prioritization', 'time-management'],
            'evaluation_criteria': 'Assess prioritization skills and ability to make trade-off decisions',
            'ideal_answer': 'Prioritization framework: 1) Understand requirements (must-have vs nice-to-have), 2) Evaluate factors: business value, user impact, dependencies, risks, effort vs value, 3) Use frameworks: MoSCoW (Must/Should/Could/Won\'t), Eisenhower Matrix (urgent/important), value vs effort matrix, 4) Break into smaller tasks, 5) Focus on MVP/core functionality first, 6) Communicate with stakeholders about scope/timeline, 7) Negotiate if needed, 8) Document trade-offs. Show: systematic thinking, stakeholder awareness, ability to make informed decisions, transparency about limitations. Avoid: trying to do everything, not communicating constraints.',
            'rationale': 'Tests prioritization and time management skills essential for delivering value under constraints.',
            'version': 1,
            'created_at': now - timedelta(days=39),
            'updated_at': now - timedelta(days=39),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440022'),
            'text': 'How would you handle a situation where a client requests a feature that conflicts with your technical recommendations?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'HARD',
            'skills': ['Client Communication', 'Technical Leadership', 'Negotiation'],
            'tags': ['situational', 'client-management', 'leadership'],
            'evaluation_criteria': 'Evaluate communication skills, technical credibility, and ability to navigate stakeholder relationships',
            'ideal_answer': 'Approach: 1) Understand client\'s underlying need (why they want this), 2) Listen actively and acknowledge their perspective, 3) Explain technical concerns clearly (security, maintainability, scalability, cost) with concrete examples, 4) Propose alternatives that meet their needs while addressing technical concerns, 5) Present trade-offs (pros/cons of each approach), 6) Use data/evidence to support recommendations, 7) Find compromise if possible, 8) Escalate to manager if critical, 9) Document decision and rationale. Show: respect for client, technical expertise, communication skills, problem-solving, ability to balance business and technical needs. Avoid: being dismissive, using jargon, refusing without explanation.',
            'rationale': 'Evaluates technical leadership, stakeholder management, and ability to navigate complex business-technical conflicts.',
            'version': 1,
            'created_at': now - timedelta(days=38),
            'updated_at': now - timedelta(days=38),
        },
        {
            'id': uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
            'text': 'Explain what is garbage collection in Java and how it works.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Java', 'Memory Management', 'JVM'],
            'tags': ['java', 'memory-management', 'jvm'],
            'evaluation_criteria': 'Assess understanding of memory management and JVM internals',
            'ideal_answer': 'Garbage collection automatically manages memory by reclaiming objects no longer referenced. Java uses generational GC: objects start in Young Generation (Eden space), survive collections move to Survivor spaces, long-lived objects move to Old Generation. GC algorithms: Serial (single-threaded), Parallel (multi-threaded), G1 (low-latency, large heaps), ZGC (ultra-low latency). GC pauses application execution (stop-the-world). Tuning: adjust heap size, choose GC algorithm based on latency/throughput needs, monitor GC logs. Benefits: prevents memory leaks, automatic memory management. Trade-offs: unpredictable pauses, overhead. Understanding GC helps optimize Java application performance.',
            'rationale': 'Tests understanding of JVM internals and memory management essential for optimizing Java application performance.',
            'version': 1,
            'created_at': now - timedelta(days=37),
            'updated_at': now - timedelta(days=37),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440001'),
            'text': 'Explain the concept of dependency injection and its benefits in software design.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Design Patterns', 'OOP', 'Software Architecture'],
            'tags': ['dependency-injection', 'design-patterns', 'architecture'],
            'evaluation_criteria': 'Assess understanding of DI principles, inversion of control, and practical benefits like testability and loose coupling.',
            'ideal_answer': 'Dependency injection is a design pattern where objects receive their dependencies from external sources rather than creating them internally. Benefits include: improved testability (easy to mock dependencies), loose coupling (components depend on abstractions), better separation of concerns, and easier maintenance. Common implementations include constructor injection, setter injection, and interface injection. This pattern follows the Dependency Inversion Principle from SOLID principles.',
            'rationale': 'Tests understanding of fundamental design patterns and their practical application in building maintainable software systems.',
            'version': 1,
            'created_at': now - timedelta(days=40),
            'updated_at': now - timedelta(days=40),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'text': 'What is the difference between microservices and monolithic architecture? When would you choose each?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['System Design', 'Architecture', 'Microservices'],
            'tags': ['microservices', 'architecture', 'system-design'],
            'evaluation_criteria': 'Evaluate understanding of architectural patterns, trade-offs, and decision-making criteria.',
            'ideal_answer': 'Monolithic architecture has all components in a single deployable unit, while microservices split functionality into independent, loosely coupled services. Monoliths are simpler to develop, test, and deploy initially, but harder to scale and maintain at large scale. Microservices offer independent scaling, technology diversity, and fault isolation, but add complexity in deployment, monitoring, and inter-service communication. Choose monolith for small teams, simple applications, or when starting. Choose microservices for large teams, complex domains, or when you need independent scaling of components.',
            'rationale': 'Assesses system design thinking and ability to make architectural decisions based on context and requirements.',
            'version': 1,
            'created_at': now - timedelta(days=38),
            'updated_at': now - timedelta(days=38),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440003'),
            'text': 'How does a hash table work internally? Explain collision resolution strategies.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Data Structures', 'Algorithms', 'Hash Tables'],
            'tags': ['data-structures', 'hash-tables', 'algorithms'],
            'evaluation_criteria': 'Check understanding of hash table internals, hash functions, and collision handling mechanisms.',
            'ideal_answer': 'A hash table uses a hash function to map keys to array indices. When inserting, the hash function computes an index, and the value is stored at that position. Collisions occur when different keys hash to the same index. Resolution strategies include: 1) Chaining - store multiple items in a linked list at each bucket, 2) Open addressing - find next available slot using linear probing, quadratic probing, or double hashing. Chaining is simpler but uses extra memory. Open addressing is more memory-efficient but can degrade performance with high load factors.',
            'rationale': 'Tests fundamental data structure knowledge essential for understanding performance characteristics and algorithm design.',
            'version': 1,
            'created_at': now - timedelta(days=36),
            'updated_at': now - timedelta(days=36),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440004'),
            'text': 'Describe how you would implement authentication and authorization in a web application.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Security', 'Authentication', 'Authorization', 'Web Development'],
            'tags': ['security', 'authentication', 'authorization', 'web'],
            'evaluation_criteria': 'Assess knowledge of security best practices, token-based authentication, and authorization patterns.',
            'ideal_answer': 'Authentication verifies user identity (who you are), while authorization determines permissions (what you can do). Implementation: 1) User registration/login with password hashing (bcrypt/argon2), 2) JWT tokens or session-based auth, 3) Store tokens securely (httpOnly cookies or localStorage with CSRF protection), 4) Implement role-based access control (RBAC) or attribute-based (ABAC), 5) Use middleware to protect routes, 6) Implement refresh tokens for security, 7) Add rate limiting and account lockout. Best practices: never store passwords in plain text, use HTTPS, implement proper session management, and follow OWASP guidelines.',
            'rationale': 'Evaluates critical security knowledge essential for building secure applications.',
            'version': 1,
            'created_at': now - timedelta(days=34),
            'updated_at': now - timedelta(days=34),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
            'text': 'What is the CAP theorem? Explain each component and provide examples.',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['Distributed Systems', 'Database', 'System Design'],
            'tags': ['cap-theorem', 'distributed-systems', 'database'],
            'evaluation_criteria': 'Evaluate understanding of distributed system trade-offs and real-world system characteristics.',
            'ideal_answer': 'CAP theorem states that in a distributed system, you can only guarantee two out of three: Consistency (all nodes see same data simultaneously), Availability (system remains operational), Partition tolerance (system continues despite network failures). Examples: CP systems (like MongoDB, HBase) prioritize consistency and partition tolerance, sacrificing availability during partitions. AP systems (like Cassandra, DynamoDB) prioritize availability and partition tolerance, allowing eventual consistency. CA systems (traditional RDBMS) prioritize consistency and availability but don\'t handle network partitions well. In practice, partition tolerance is unavoidable in distributed systems, so the choice is between C and A.',
            'rationale': 'Tests understanding of fundamental distributed systems principles and their practical implications.',
            'version': 1,
            'created_at': now - timedelta(days=32),
            'updated_at': now - timedelta(days=32),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440006'),
            'text': 'Explain the difference between SQL and NoSQL databases. When would you use each?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Database', 'SQL', 'NoSQL', 'Data Modeling'],
            'tags': ['database', 'sql', 'nosql', 'data-modeling'],
            'evaluation_criteria': 'Assess understanding of database types, their characteristics, and appropriate use cases.',
            'ideal_answer': 'SQL databases are relational, use structured schemas, support ACID transactions, and use SQL for queries. They excel at complex queries, data integrity, and structured data. Examples: PostgreSQL, MySQL. NoSQL databases are non-relational, schema-flexible, often prioritize performance and scalability, and use various data models (document, key-value, column, graph). They excel at horizontal scaling, unstructured data, and high write throughput. Examples: MongoDB, Redis, Cassandra. Use SQL for: complex queries, transactions, structured data, data integrity requirements. Use NoSQL for: high scalability needs, flexible schemas, large volumes of unstructured data, rapid development.',
            'rationale': 'Evaluates database selection knowledge crucial for making appropriate technology choices.',
            'version': 1,
            'created_at': now - timedelta(days=30),
            'updated_at': now - timedelta(days=30),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
            'text': 'What is the difference between unit testing, integration testing, and end-to-end testing?',
            'question_type': 'TECHNICAL',
            'difficulty': 'EASY',
            'skills': ['Testing', 'QA', 'Software Engineering'],
            'tags': ['testing', 'unit-testing', 'integration-testing', 'e2e'],
            'evaluation_criteria': 'Check understanding of testing pyramid and different testing levels.',
            'ideal_answer': 'Unit testing tests individual components in isolation (functions, classes) with mocked dependencies. Fast, numerous, catch bugs early. Integration testing verifies interactions between components (database, APIs, services). Slower, fewer tests, catch integration issues. End-to-end testing tests complete user workflows through the entire system. Slowest, fewest tests, catch system-level issues. The testing pyramid suggests many unit tests, fewer integration tests, and minimal E2E tests. Each level serves different purposes and catches different types of bugs.',
            'rationale': 'Assesses testing knowledge essential for building reliable software.',
            'version': 1,
            'created_at': now - timedelta(days=28),
            'updated_at': now - timedelta(days=28),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440008'),
            'text': 'How does garbage collection work in Python? Explain the reference counting and generational GC.',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Python', 'Memory Management', 'Garbage Collection'],
            'tags': ['python', 'memory-management', 'garbage-collection'],
            'evaluation_criteria': 'Evaluate understanding of Python internals and memory management.',
            'ideal_answer': 'Python uses a combination of reference counting and generational garbage collection. Reference counting immediately deallocates objects when reference count reaches zero, but cannot handle circular references. Generational GC (gc module) handles circular references by tracking objects in generations (0, 1, 2). New objects start in generation 0. Objects that survive GC move to older generations. GC runs more frequently on younger generations. This approach optimizes for the fact that most objects die young. The gc.collect() can manually trigger collection, but Python handles it automatically.',
            'rationale': 'Tests understanding of language internals important for performance optimization and debugging.',
            'version': 1,
            'created_at': now - timedelta(days=26),
            'updated_at': now - timedelta(days=26),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
            'text': 'What is the event loop in JavaScript? How does it handle asynchronous operations?',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['JavaScript', 'Event Loop', 'Asynchronous Programming'],
            'tags': ['javascript', 'event-loop', 'async', 'nodejs'],
            'evaluation_criteria': 'Assess deep understanding of JavaScript runtime and asynchronous execution model.',
            'ideal_answer': 'The event loop is JavaScript\'s mechanism for handling asynchronous operations. It continuously checks the call stack and task queues. When the call stack is empty, it moves tasks from queues to the stack. Queues include: Callback Queue (for setTimeout, DOM events), Microtask Queue (for Promises, queueMicrotask), and Job Queue (for async/await). Microtasks have higher priority than regular callbacks. The event loop processes all microtasks before moving to the next callback. This single-threaded model allows non-blocking I/O operations. Understanding this is crucial for debugging async code and avoiding common pitfalls.',
            'rationale': 'Evaluates critical JavaScript knowledge for understanding async behavior and performance.',
            'version': 1,
            'created_at': now - timedelta(days=24),
            'updated_at': now - timedelta(days=24),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440010'),
            'text': 'Explain the concept of database indexing. How does it improve query performance?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['Database', 'SQL', 'Performance Optimization'],
            'tags': ['database', 'indexing', 'performance', 'sql'],
            'evaluation_criteria': 'Check understanding of indexing mechanisms and their impact on database performance.',
            'ideal_answer': 'Database indexing creates data structures (like B-trees) that allow faster data retrieval. Instead of scanning entire tables (full table scan), indexes provide direct access paths to data. Benefits: faster SELECT queries, faster JOINs, faster WHERE clause filtering, faster ORDER BY operations. Trade-offs: indexes consume storage space, slow down INSERT/UPDATE/DELETE operations (indexes must be maintained), and require maintenance. Common index types: B-tree (default, good for range queries), Hash (exact matches), Bitmap (low cardinality). Best practices: index frequently queried columns, foreign keys, columns in WHERE clauses, but avoid over-indexing.',
            'rationale': 'Tests database optimization knowledge essential for building performant applications.',
            'version': 1,
            'created_at': now - timedelta(days=22),
            'updated_at': now - timedelta(days=22),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440011'),
            'text': 'Tell me about a time when you had to learn a new technology or framework quickly for a project. How did you approach it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'EASY',
            'skills': ['Learning', 'Adaptability', 'Problem Solving'],
            'tags': ['behavioral', 'learning', 'adaptability'],
            'evaluation_criteria': 'Assess learning agility, resource utilization, and ability to apply new knowledge effectively.',
            'ideal_answer': 'Use STAR method: Situation - describe the project context and technology needed. Task - explain what needed to be learned and why. Action - detail learning approach: official documentation, tutorials, hands-on practice, code examples, community resources, pair programming. Result - describe successful implementation, time taken, and lessons learned. Show self-directed learning, systematic approach, and ability to apply knowledge quickly.',
            'rationale': 'Evaluates learning agility and adaptability, crucial traits for software developers in a rapidly evolving field.',
            'version': 1,
            'created_at': now - timedelta(days=20),
            'updated_at': now - timedelta(days=20),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440012'),
            'text': 'Describe a situation where you had to work with a difficult team member. How did you handle it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Communication', 'Teamwork', 'Conflict Resolution'],
            'tags': ['behavioral', 'teamwork', 'conflict'],
            'evaluation_criteria': 'Evaluate emotional intelligence, communication skills, and collaborative problem-solving approach.',
            'ideal_answer': 'Use STAR method. Situation: describe the conflict objectively. Task: explain the challenge and impact on team/project. Action: detail approach - active listening, understanding their perspective, clear communication, finding common ground, involving manager if needed, focusing on solutions not blame. Result: resolution achieved, relationship improved, project success. Show empathy, professionalism, and focus on team success.',
            'rationale': 'Assesses interpersonal skills and ability to navigate workplace conflicts constructively.',
            'version': 1,
            'created_at': now - timedelta(days=18),
            'updated_at': now - timedelta(days=18),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440013'),
            'text': 'Give an example of a time when you made a mistake in your code that caused a production issue. How did you handle it?',
            'question_type': 'BEHAVIORAL',
            'difficulty': 'MEDIUM',
            'skills': ['Accountability', 'Problem Solving', 'Learning from Mistakes'],
            'tags': ['behavioral', 'mistakes', 'accountability'],
            'evaluation_criteria': 'Check honesty, accountability, problem-solving under pressure, and learning from mistakes.',
            'ideal_answer': 'Use STAR method. Situation: describe the mistake and its impact honestly. Task: explain the urgency and what needed to be fixed. Action: immediate response (acknowledge mistake, assess impact, communicate to team, fix the issue, deploy hotfix, monitor), post-mortem analysis, implement preventive measures (tests, code review, monitoring). Result: issue resolved, lessons learned, process improvements. Show accountability, quick problem-solving, and focus on prevention.',
            'rationale': 'Evaluates accountability, crisis management, and ability to learn from failures.',
            'version': 1,
            'created_at': now - timedelta(days=16),
            'updated_at': now - timedelta(days=16),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440014'),
            'text': 'How do you prioritize tasks when you have multiple urgent deadlines?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'EASY',
            'skills': ['Prioritization', 'Time Management', 'Decision Making'],
            'tags': ['situational', 'prioritization', 'time-management'],
            'evaluation_criteria': 'Assess prioritization skills and ability to make trade-off decisions under pressure.',
            'ideal_answer': 'Evaluate factors: business impact, dependencies, urgency vs importance, stakeholder needs, resource availability. Use frameworks like Eisenhower Matrix (urgent/important), communicate with stakeholders about priorities, negotiate deadlines if needed, break down tasks, focus on high-value work first. Be transparent about trade-offs and seek help when overwhelmed. Show systematic thinking and stakeholder management.',
            'rationale': 'Tests decision-making and time management skills essential for handling competing priorities.',
            'version': 1,
            'created_at': now - timedelta(days=14),
            'updated_at': now - timedelta(days=14),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440015'),
            'text': 'If you discovered a security vulnerability in production, what steps would you take?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'MEDIUM',
            'skills': ['Security', 'Risk Management', 'Incident Response'],
            'tags': ['situational', 'security', 'incident-response'],
            'evaluation_criteria': 'Check security awareness, incident response procedures, and risk assessment capabilities.',
            'ideal_answer': 'Immediate steps: 1) Assess severity and potential impact, 2) Document the vulnerability, 3) Notify security team/manager immediately, 4) Do not publicly disclose until fixed, 5) Work with team to develop patch, 6) Test fix thoroughly, 7) Deploy fix following security protocols, 8) Monitor for exploitation, 9) Conduct post-incident review, 10) Update security practices. Show understanding of responsible disclosure and security-first mindset.',
            'rationale': 'Evaluates security awareness and proper incident response procedures.',
            'version': 1,
            'created_at': now - timedelta(days=12),
            'updated_at': now - timedelta(days=12),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440016'),
            'text': 'You need to refactor a large legacy codebase with no tests. How would you approach it?',
            'question_type': 'SITUATIONAL',
            'difficulty': 'HARD',
            'skills': ['Refactoring', 'Legacy Code', 'Testing', 'Risk Management'],
            'tags': ['situational', 'refactoring', 'legacy-code'],
            'evaluation_criteria': 'Assess refactoring strategy, risk management, and systematic approach to technical debt.',
            'ideal_answer': 'Approach: 1) Understand the codebase (documentation, code analysis, team knowledge), 2) Add tests incrementally (start with critical paths, use characterization tests), 3) Identify refactoring priorities (high-risk, high-value areas first), 4) Refactor in small, incremental changes, 5) Maintain backward compatibility, 6) Use feature flags for risky changes, 7) Monitor for regressions, 8) Document changes. Show systematic approach, risk awareness, and focus on incremental improvement rather than big-bang rewrites.',
            'rationale': 'Tests ability to handle technical debt and refactoring challenges systematically.',
            'version': 1,
            'created_at': now - timedelta(days=10),
            'updated_at': now - timedelta(days=10),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440017'),
            'text': 'Explain how you would design a caching strategy for a high-traffic web application.',
            'question_type': 'TECHNICAL',
            'difficulty': 'HARD',
            'skills': ['System Design', 'Caching', 'Performance', 'Architecture'],
            'tags': ['system-design', 'caching', 'performance'],
            'evaluation_criteria': 'Evaluate system design thinking and understanding of caching patterns and trade-offs.',
            'ideal_answer': 'Multi-layer caching strategy: 1) Browser cache (static assets, long TTL), 2) CDN (geographic distribution, static content), 3) Application cache (Redis/Memcached for frequently accessed data, session data), 4) Database query cache. Considerations: cache invalidation strategy (TTL, event-based, manual), cache warming for critical data, cache-aside vs write-through patterns, handling cache misses, cache key design, monitoring hit rates. Choose cache locations based on data access patterns, update frequency, and consistency requirements. Balance between performance gains and complexity.',
            'rationale': 'Assesses system design skills and understanding of performance optimization techniques.',
            'version': 1,
            'created_at': now - timedelta(days=8),
            'updated_at': now - timedelta(days=8),
        },
        {
            'id': uuid.UUID('760e8400-e29b-41d4-a716-446655440018'),
            'text': 'What is the difference between horizontal and vertical scaling? When would you use each?',
            'question_type': 'TECHNICAL',
            'difficulty': 'MEDIUM',
            'skills': ['System Design', 'Scalability', 'Infrastructure'],
            'tags': ['scalability', 'system-design', 'infrastructure'],
            'evaluation_criteria': 'Check understanding of scaling strategies and their trade-offs.',
            'ideal_answer': 'Vertical scaling (scale up) increases resources of existing server (more CPU, RAM, storage). Pros: simpler, no code changes needed, better for single-threaded apps. Cons: limited by hardware, expensive, single point of failure. Horizontal scaling (scale out) adds more servers. Pros: nearly unlimited scaling, cost-effective, better fault tolerance. Cons: requires stateless design, load balancing, distributed system complexity. Use vertical scaling for: small to medium apps, stateful applications, quick fixes. Use horizontal scaling for: large-scale systems, cloud-native apps, high availability requirements.',
            'rationale': 'Tests fundamental scalability knowledge essential for designing scalable systems.',
            'version': 1,
            'created_at': now - timedelta(days=6),
            'updated_at': now - timedelta(days=6),
        },
    ]

    op.bulk_insert(questions_table, questions_data)

    print("[OK] Seeded 41 questions")

    # =============================================
    # SEED DATA - CV ANALYSES (10 total: 3 + 7)
    # =============================================
    op.bulk_insert(cv_analyses_table, [
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'cv_file_path': '/uploads/cvs/john_doe_cv.pdf',
            'extracted_text': 'John Doe - Senior Full Stack Developer. 5+ years experience in React, Node.js, PostgreSQL.',
            'skills': [
                {"skill": "React", "proficiency": "expert", "years": 5},
                {"skill": "Node.js", "proficiency": "expert", "years": 5},
            ],
            'work_experience_years': 5.5,
            'education_level': 'Bachelor',
            'suggested_topics': ['Microservices', 'React Hooks', 'Database Design'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Experienced full-stack developer with strong React and Node.js skills.',
            'metadata': {"keywords": ["React", "Node.js"]},
            'created_at': now - timedelta(days=29),
        },
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'cv_file_path': '/uploads/cvs/jane_smith_cv.pdf',
            'extracted_text': 'Jane Smith - Backend Engineer. 3+ years experience in Python, Java, SQL, REST APIs, and microservices architecture.',
            'skills': [
                {"skill": "Python", "proficiency": "advanced", "years": 3},
                {"skill": "Java", "proficiency": "advanced", "years": 3},
                {"skill": "SQL", "proficiency": "intermediate", "years": 2},
                {"skill": "REST API", "proficiency": "advanced", "years": 3},
            ],
            'work_experience_years': 3.5,
            'education_level': 'Master',
            'suggested_topics': ['System Design', 'SOLID Principles', 'Database Optimization', 'API Design'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Backend engineer with strong Python and Java skills, experienced in building scalable APIs.',
            'metadata': {"keywords": ["Python", "Java", "Backend", "API"]},
            'created_at': now - timedelta(days=24),
        },
        {
            'id': uuid.UUID('750e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'cv_file_path': '/uploads/cvs/bob_johnson_cv.pdf',
            'extracted_text': 'Bob Johnson - Full Stack Developer. 7+ years experience in JavaScript, React, Node.js, Docker, and cloud technologies.',
            'skills': [
                {"skill": "JavaScript", "proficiency": "expert", "years": 7},
                {"skill": "React", "proficiency": "expert", "years": 6},
                {"skill": "Node.js", "proficiency": "expert", "years": 6},
                {"skill": "Docker", "proficiency": "advanced", "years": 4},
            ],
            'work_experience_years': 7.5,
            'education_level': 'Bachelor',
            'suggested_topics': ['Event Loop', 'System Design', 'Microservices', 'Async Programming'],
            'suggested_difficulty': 'HARD',
            'summary': 'Senior full-stack developer with extensive JavaScript ecosystem expertise and system design experience.',
            'metadata': {"keywords": ["JavaScript", "React", "Node.js", "System Design"]},
            'created_at': now - timedelta(days=19),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'cv_file_path': '/uploads/cvs/alice_chen_cv.pdf',
            'extracted_text': 'Alice Chen - Junior Full Stack Developer. 1.5 years experience in React, Node.js, and PostgreSQL. Recent computer science graduate with strong foundation in web development. Experience building responsive web applications and RESTful APIs.',
            'skills': [
                {"skill": "React", "proficiency": "intermediate", "years": 1.5, "category": "technical"},
                {"skill": "Node.js", "proficiency": "intermediate", "years": 1.5, "category": "technical"},
                {"skill": "JavaScript", "proficiency": "intermediate", "years": 1.5, "category": "technical"},
                {"skill": "PostgreSQL", "proficiency": "beginner", "years": 1, "category": "technical"},
                {"skill": "HTML/CSS", "proficiency": "intermediate", "years": 1.5, "category": "technical"},
            ],
            'work_experience_years': 1.5,
            'education_level': "Bachelor's",
            'suggested_topics': ['React Hooks', 'Async/Await', 'REST API Design', 'Database Basics', 'Testing'],
            'suggested_difficulty': 'EASY',
            'summary': 'Junior full-stack developer with 1.5 years of experience. Strong in React and Node.js fundamentals. Good foundation but needs experience with advanced concepts and system design.',
            'metadata': {"keywords": ["React", "Node.js", "Junior", "Full Stack"]},
            'created_at': now - timedelta(days=13),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'cv_file_path': '/uploads/cvs/michael_rodriguez_cv.pdf',
            'extracted_text': 'Michael Rodriguez - Mid-level Backend Engineer. 4 years experience in Python, Django, PostgreSQL, and AWS. Specialized in building scalable REST APIs and microservices. Experience with Docker, Kubernetes, and CI/CD pipelines.',
            'skills': [
                {"skill": "Python", "proficiency": "advanced", "years": 4, "category": "technical"},
                {"skill": "Django", "proficiency": "advanced", "years": 4, "category": "technical"},
                {"skill": "PostgreSQL", "proficiency": "advanced", "years": 4, "category": "technical"},
                {"skill": "AWS", "proficiency": "intermediate", "years": 2, "category": "technical"},
                {"skill": "Docker", "proficiency": "intermediate", "years": 2, "category": "technical"},
                {"skill": "REST API", "proficiency": "advanced", "years": 4, "category": "technical"},
            ],
            'work_experience_years': 4.0,
            'education_level': "Bachelor's",
            'suggested_topics': ['System Design', 'Database Optimization', 'Microservices', 'API Design', 'SOLID Principles'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Mid-level backend engineer with 4 years of Python and Django experience. Strong in API design and database optimization. Ready for system design and architecture discussions.',
            'metadata': {"keywords": ["Python", "Backend", "Django", "API"]},
            'created_at': now - timedelta(days=11),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'cv_file_path': '/uploads/cvs/sarah_williams_cv.pdf',
            'extracted_text': 'Sarah Williams - Senior Full Stack Developer. 8 years experience in JavaScript, TypeScript, React, Node.js, and cloud technologies. Led multiple teams and architected large-scale applications. Expert in system design and performance optimization.',
            'skills': [
                {"skill": "JavaScript", "proficiency": "expert", "years": 8, "category": "technical"},
                {"skill": "TypeScript", "proficiency": "expert", "years": 6, "category": "technical"},
                {"skill": "React", "proficiency": "expert", "years": 7, "category": "technical"},
                {"skill": "Node.js", "proficiency": "expert", "years": 8, "category": "technical"},
                {"skill": "System Design", "proficiency": "expert", "years": 5, "category": "technical"},
                {"skill": "AWS", "proficiency": "advanced", "years": 4, "category": "technical"},
                {"skill": "Docker", "proficiency": "advanced", "years": 4, "category": "technical"},
            ],
            'work_experience_years': 8.0,
            'education_level': "Master's",
            'suggested_topics': ['System Design', 'Event Loop', 'Microservices Architecture', 'Performance Optimization', 'Scalability Patterns'],
            'suggested_difficulty': 'HARD',
            'summary': 'Senior full-stack developer with 8 years of experience. Expert in JavaScript ecosystem and system design. Strong leadership experience and ability to architect scalable solutions.',
            'metadata': {"keywords": ["Senior", "Full Stack", "System Design", "JavaScript"]},
            'created_at': now - timedelta(days=9),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'cv_file_path': '/uploads/cvs/david_kim_cv.pdf',
            'extracted_text': 'David Kim - Frontend Specialist. 3 years experience in React, Vue.js, TypeScript, and modern frontend tooling. Strong UI/UX focus with expertise in state management, performance optimization, and responsive design.',
            'skills': [
                {"skill": "React", "proficiency": "advanced", "years": 3, "category": "technical"},
                {"skill": "Vue.js", "proficiency": "advanced", "years": 3, "category": "technical"},
                {"skill": "TypeScript", "proficiency": "advanced", "years": 2.5, "category": "technical"},
                {"skill": "CSS/SCSS", "proficiency": "advanced", "years": 3, "category": "technical"},
                {"skill": "Redux", "proficiency": "intermediate", "years": 2, "category": "technical"},
                {"skill": "Webpack", "proficiency": "intermediate", "years": 2, "category": "technical"},
            ],
            'work_experience_years': 3.0,
            'education_level': "Bachelor's",
            'suggested_topics': ['React Hooks', 'State Management', 'Performance Optimization', 'Frontend Architecture', 'Testing'],
            'suggested_difficulty': 'MEDIUM',
            'summary': 'Frontend specialist with 3 years of React and Vue.js experience. Strong focus on UI/UX and frontend performance. Good understanding of modern frontend patterns.',
            'metadata': {"keywords": ["Frontend", "React", "Vue.js", "UI/UX"]},
            'created_at': now - timedelta(days=7),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440005'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440005'),
            'cv_file_path': '/uploads/cvs/emily_thompson_cv.pdf',
            'extracted_text': 'Emily Thompson - DevOps Engineer. 5 years experience in CI/CD, Kubernetes, Docker, AWS, Terraform, and infrastructure automation. Expert in cloud architecture and monitoring solutions.',
            'skills': [
                {"skill": "Kubernetes", "proficiency": "expert", "years": 4, "category": "technical"},
                {"skill": "Docker", "proficiency": "expert", "years": 5, "category": "technical"},
                {"skill": "AWS", "proficiency": "expert", "years": 5, "category": "technical"},
                {"skill": "Terraform", "proficiency": "advanced", "years": 3, "category": "technical"},
                {"skill": "CI/CD", "proficiency": "expert", "years": 5, "category": "technical"},
                {"skill": "Linux", "proficiency": "advanced", "years": 5, "category": "technical"},
                {"skill": "Python", "proficiency": "intermediate", "years": 3, "category": "technical"},
            ],
            'work_experience_years': 5.0,
            'education_level': "Bachelor's",
            'suggested_topics': ['System Design', 'Scalability', 'Infrastructure', 'Cloud Architecture', 'Container Orchestration'],
            'suggested_difficulty': 'HARD',
            'summary': 'DevOps engineer with 5 years of experience in cloud infrastructure and automation. Expert in Kubernetes, Docker, and AWS. Strong in infrastructure as code and CI/CD pipelines.',
            'metadata': {"keywords": ["DevOps", "Kubernetes", "AWS", "Infrastructure"]},
            'created_at': now - timedelta(days=5),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440006'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440006'),
            'cv_file_path': '/uploads/cvs/james_anderson_cv.pdf',
            'extracted_text': 'James Anderson - Full Stack Developer. 2.5 years experience in Java, Spring Boot, React, and PostgreSQL. Experience building enterprise applications and RESTful services.',
            'skills': [
                {"skill": "Java", "proficiency": "intermediate", "years": 2.5, "category": "technical"},
                {"skill": "Spring Boot", "proficiency": "intermediate", "years": 2.5, "category": "technical"},
                {"skill": "React", "proficiency": "intermediate", "years": 2, "category": "technical"},
                {"skill": "PostgreSQL", "proficiency": "intermediate", "years": 2.5, "category": "technical"},
                {"skill": "REST API", "proficiency": "intermediate", "years": 2.5, "category": "technical"},
            ],
            'work_experience_years': 2.5,
            'education_level': "Bachelor's",
            'suggested_topics': ['Java Fundamentals', 'Spring Framework', 'Database Design', 'API Design', 'Testing'],
            'suggested_difficulty': 'EASY',
            'summary': 'Full-stack developer with 2.5 years of Java and Spring Boot experience. Good foundation in enterprise development. Still learning advanced concepts.',
            'metadata': {"keywords": ["Java", "Spring Boot", "Full Stack"]},
            'created_at': now - timedelta(days=3),
        },
        {
            'id': uuid.UUID('860e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'cv_file_path': '/uploads/cvs/lisa_martinez_cv.pdf',
            'extracted_text': 'Lisa Martinez - Backend Engineer. 6 years experience in Python, FastAPI, PostgreSQL, Redis, and distributed systems. Expert in building high-performance APIs and microservices architecture.',
            'skills': [
                {"skill": "Python", "proficiency": "expert", "years": 6, "category": "technical"},
                {"skill": "FastAPI", "proficiency": "expert", "years": 4, "category": "technical"},
                {"skill": "PostgreSQL", "proficiency": "expert", "years": 6, "category": "technical"},
                {"skill": "Redis", "proficiency": "advanced", "years": 4, "category": "technical"},
                {"skill": "Microservices", "proficiency": "advanced", "years": 3, "category": "technical"},
                {"skill": "Docker", "proficiency": "advanced", "years": 4, "category": "technical"},
            ],
            'work_experience_years': 6.0,
            'education_level': "Master's",
            'suggested_topics': ['System Design', 'Microservices', 'Database Optimization', 'Caching Strategies', 'API Performance'],
            'suggested_difficulty': 'HARD',
            'summary': 'Senior backend engineer with 6 years of Python experience. Expert in FastAPI and building scalable microservices. Strong in database optimization and caching strategies.',
            'metadata': {"keywords": ["Python", "Backend", "Microservices", "FastAPI"]},
            'created_at': now - timedelta(days=1),
        },
    ])

    print("[OK] Seeded 10 CV analyses")

    # =============================================
    # SEED DATA - INTERVIEWS (14 total: 4 + 10)
    # =============================================
    interviews_data = [
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440001'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440002'),
            ],
            'current_question_index': 2,
            'plan_metadata': {},
            'adaptive_follow_ups': [],
            'started_at': now - timedelta(days=28),
            'completed_at': now - timedelta(days=28) + timedelta(minutes=30),
            'created_at': now - timedelta(days=28),
            'updated_at': now - timedelta(days=28) + timedelta(minutes=30),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440003'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('950e8400-e29b-41d4-a716-446655440006'),
            ],
            'current_question_index': 4,
            'plan_metadata': {},
            'adaptive_follow_ups': [],
            'started_at': now - timedelta(days=23),
            'completed_at': now - timedelta(days=23) + timedelta(minutes=45),
            'created_at': now - timedelta(days=23),
            'updated_at': now - timedelta(days=23) + timedelta(minutes=45),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'status': 'QUESTIONING',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440003'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440014'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440018'),
            ],
            'answer_ids': [
                uuid.UUID('950e8400-e29b-41d4-a716-446655440007'),
            ],
            'current_question_index': 1,
            'plan_metadata': {},
            'adaptive_follow_ups': [],
            'started_at': now - timedelta(days=18),
            'completed_at': None,
            'created_at': now - timedelta(days=18),
            'updated_at': now - timedelta(days=18) + timedelta(minutes=20),
        },
        {
            'id': uuid.UUID('850e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'status': 'IDLE',
            'cv_analysis_id': uuid.UUID('750e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440009'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440013'),
            ],
            'answer_ids': [],
            'current_question_index': 0,
            'plan_metadata': {},
            'adaptive_follow_ups': [],
            'started_at': None,
            'completed_at': None,
            'created_at': now - timedelta(days=15),
            'updated_at': now - timedelta(days=15),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440001'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440017'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440003'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440004'),
            ],
            'current_question_index': 4,
            'plan_metadata': {
                'n': 4,
                'generated_at': (now - timedelta(days=12)).isoformat(),
                'strategy': 'beginner_focused',
                'difficulty': 'easy',
            },
            'adaptive_follow_ups': [
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440003'),
            ],
            'started_at': now - timedelta(days=12, hours=2),
            'completed_at': now - timedelta(days=12, hours=2, minutes=35),
            'created_at': now - timedelta(days=12),
            'updated_at': now - timedelta(days=12, hours=2, minutes=35),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('760e8400-e29b-41d4-a716-446655440001'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440003'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440014'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440008'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440009'),
            ],
            'current_question_index': 5,
            'plan_metadata': {
                'n': 5,
                'generated_at': (now - timedelta(days=10)).isoformat(),
                'strategy': 'balanced_technical',
                'difficulty': 'medium',
            },
            'adaptive_follow_ups': [],
            'started_at': now - timedelta(days=10, hours=3),
            'completed_at': now - timedelta(days=10, hours=3, minutes=48),
            'created_at': now - timedelta(days=10),
            'updated_at': now - timedelta(days=10, hours=3, minutes=48),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440003'),
            'question_ids': [
                uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440017'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440011'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440012'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440013'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440014'),
            ],
            'current_question_index': 5,
            'plan_metadata': {
                'n': 5,
                'generated_at': (now - timedelta(days=8)).isoformat(),
                'strategy': 'senior_design_focused',
                'difficulty': 'hard',
            },
            'adaptive_follow_ups': [],
            'started_at': now - timedelta(days=8, hours=1),
            'completed_at': now - timedelta(days=8, hours=1, minutes=52),
            'created_at': now - timedelta(days=8),
            'updated_at': now - timedelta(days=8, hours=1, minutes=52),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'status': 'COMPLETE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440004'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440015'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440016'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440017'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440018'),
            ],
            'current_question_index': 4,
            'plan_metadata': {
                'n': 4,
                'generated_at': (now - timedelta(days=6)).isoformat(),
                'strategy': 'frontend_focused',
                'difficulty': 'medium',
            },
            'adaptive_follow_ups': [
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440004'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440005'),
            ],
            'started_at': now - timedelta(days=6, hours=2, minutes=30),
            'completed_at': now - timedelta(days=6, hours=3, minutes=15),
            'created_at': now - timedelta(days=6),
            'updated_at': now - timedelta(days=6, hours=3, minutes=15),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440005'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440005'),
            'status': 'FOLLOW_UP',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440005'),
            'question_ids': [
                uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440018'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440011'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440017'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440019'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440020'),
            ],
            'current_question_index': 2,
            'plan_metadata': {
                'n': 4,
                'generated_at': (now - timedelta(days=4)).isoformat(),
                'strategy': 'devops_infrastructure',
                'difficulty': 'hard',
            },
            'adaptive_follow_ups': [
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440007'),
            ],
            'started_at': now - timedelta(days=4, hours=1),
            'completed_at': None,
            'created_at': now - timedelta(days=4),
            'updated_at': now - timedelta(days=4, hours=1, minutes=25),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440006'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440006'),
            'status': 'FOLLOW_UP',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440006'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440021'),
            ],
            'current_question_index': 1,
            'plan_metadata': {
                'n': 4,
                'generated_at': (now - timedelta(days=2)).isoformat(),
                'strategy': 'java_backend_focused',
                'difficulty': 'easy',
            },
            'adaptive_follow_ups': [
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440008'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440009'),
            ],
            'started_at': now - timedelta(days=2, hours=2),
            'completed_at': None,
            'created_at': now - timedelta(days=2),
            'updated_at': now - timedelta(days=2, hours=2, minutes=12),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'status': 'FOLLOW_UP',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440007'),
            'question_ids': [
                uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440008'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440017'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
            ],
            'answer_ids': [
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440022'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440023'),
                uuid.UUID('a60e8400-e29b-41d4-a716-446655440024'),
            ],
            'current_question_index': 3,
            'plan_metadata': {
                'n': 5,
                'generated_at': (now - timedelta(days=1)).isoformat(),
                'strategy': 'senior_backend_design',
                'difficulty': 'hard',
            },
            'adaptive_follow_ups': [
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440011'),
                uuid.UUID('b60e8400-e29b-41d4-a716-446655440012'),
            ],
            'started_at': now - timedelta(days=1, hours=3),
            'completed_at': None,
            'created_at': now - timedelta(days=1),
            'updated_at': now - timedelta(days=1, hours=3, minutes=38),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440008'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'status': 'IDLE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440001'),
            'question_ids': [
                uuid.UUID('650e8400-e29b-41d4-a716-446655440002'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
                uuid.UUID('650e8400-e29b-41d4-a716-446655440005'),
            ],
            'answer_ids': [],
            'current_question_index': 0,
            'plan_metadata': {
                'n': 3,
                'generated_at': (now - timedelta(hours=2)).isoformat(),
                'strategy': 'follow_up_basics',
                'difficulty': 'easy',
            },
            'adaptive_follow_ups': [],
            'started_at': None,
            'completed_at': None,
            'created_at': now - timedelta(hours=2),
            'updated_at': now - timedelta(hours=2),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440009'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'status': 'IDLE',
            'cv_analysis_id': uuid.UUID('860e8400-e29b-41d4-a716-446655440002'),
            'question_ids': [
                uuid.UUID('760e8400-e29b-41d4-a716-446655440006'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440010'),
                uuid.UUID('760e8400-e29b-41d4-a716-446655440012'),
            ],
            'answer_ids': [],
            'current_question_index': 0,
            'plan_metadata': {
                'n': 3,
                'generated_at': (now - timedelta(hours=1)).isoformat(),
                'strategy': 'database_deep_dive',
                'difficulty': 'medium',
            },
            'adaptive_follow_ups': [],
            'started_at': None,
            'completed_at': None,
            'created_at': now - timedelta(hours=1),
            'updated_at': now - timedelta(hours=1),
        },
        {
            'id': uuid.UUID('960e8400-e29b-41d4-a716-446655440010'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'status': 'IDLE',
            'cv_analysis_id': None,
            'question_ids': [],
            'answer_ids': [],
            'current_question_index': 0,
            'plan_metadata': {},
            'adaptive_follow_ups': [],
            'started_at': None,
            'completed_at': None,
            'created_at': now - timedelta(minutes=30),
            'updated_at': now - timedelta(minutes=30),
        },
    ]

    op.bulk_insert(interviews_table, interviews_data)

    print("[OK] Seeded interviews")

    # =============================================
    # SEED DATA - ANSWERS (31 total: 7 + 24)
    # =============================================
    answers_data = [
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440001'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'text': 'var is function-scoped, let and const are block-scoped. const cannot be reassigned.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 85,
                "feedback": "Good understanding of scope and hoisting.",
                "strengths": ["Clear explanation"],
            },
            'similarity_score': 0.72,
            'gaps': {
                "missing_concepts": ["detail", "examples"],
                "improvement_areas": ["depth", "clarity"]
            },
            'metadata': {"response_time_seconds": 45},
            'created_at': now - timedelta(days=28),
            'evaluated_at': now - timedelta(days=28) + timedelta(minutes=5),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440002'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440001'),
            'text': 'async/await makes async code more readable. It is built on top of Promises.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 80,
                "feedback": "Solid understanding.",
            },
            'similarity_score': 0.70,
            'gaps': {
                "missing_concepts": ["advanced_details"],
                "improvement_areas": ["specificity"]
            },
            'metadata': {"response_time_seconds": 60},
            'created_at': now - timedelta(days=28) + timedelta(minutes=15),
            'evaluated_at': now - timedelta(days=28) + timedelta(minutes=20),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440003'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'A closure in Python is a nested function that captures variables from its enclosing scope. For example, a counter function can maintain state between calls.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 88,
                "feedback": "Good understanding of closures with practical example.",
                "strengths": ["Clear explanation", "Provided example"],
            },
            'similarity_score': 0.85,
            'gaps': {
                "missing_concepts": [],
                "improvement_areas": []
            },
            'metadata': {"response_time_seconds": 50},
            'created_at': now - timedelta(days=23) + timedelta(minutes=5),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=8),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440004'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440006'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'INNER JOIN returns matching rows from both tables. LEFT JOIN returns all rows from left table. RIGHT JOIN returns all rows from right table. FULL OUTER JOIN returns all rows from both tables.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 92,
                "feedback": "Excellent understanding of SQL JOIN operations.",
                "strengths": ["Complete coverage of all JOIN types", "Clear explanation"],
            },
            'similarity_score': 0.90,
            'gaps': {
                "missing_concepts": [],
                "improvement_areas": []
            },
            'metadata': {"response_time_seconds": 55},
            'created_at': now - timedelta(days=23) + timedelta(minutes=15),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=18),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440005'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'SOLID principles are: Single Responsibility - one reason to change, Open/Closed - open for extension, Liskov Substitution - derived classes must be substitutable, Interface Segregation - no forced implementation, Dependency Inversion - depend on abstractions.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 85,
                "feedback": "Solid grasp of SOLID principles with clear explanations.",
                "strengths": ["Covered all principles", "Practical understanding"],
            },
            'similarity_score': 0.85,
            'gaps': {
                "missing_concepts": [],
                "improvement_areas": []
            },
            'metadata': {"response_time_seconds": 75},
            'created_at': now - timedelta(days=23) + timedelta(minutes=25),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=30),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440006'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440016'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440002'),
            'text': 'Last month I had to deliver a feature under tight deadline. I broke it down into smaller tasks, communicated blockers early, and worked extra hours when needed. We delivered on time by prioritizing the MVP.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 78,
                "feedback": "Good example showing problem-solving under pressure.",
                "strengths": ["Clear situation", "Practical approach"],
                "areas_for_improvement": ["Could elaborate more on communication strategies"],
            },
            'similarity_score': 0.78,
            'gaps': {
                "missing_concepts": [],
                "improvement_areas": []
            },
            'metadata': {"response_time_seconds": 90},
            'created_at': now - timedelta(days=23) + timedelta(minutes=35),
            'evaluated_at': now - timedelta(days=23) + timedelta(minutes=40),
        },
        {
            'id': uuid.UUID('950e8400-e29b-41d4-a716-446655440007'),
            'interview_id': uuid.UUID('850e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            'candidate_id': uuid.UUID('550e8400-e29b-41d4-a716-446655440003'),
            'text': 'To handle 1M requests per second, I would use horizontal scaling with load balancers, implement caching at multiple layers (CDN, Redis), use database replication and sharding, implement message queues for async processing, and design with microservices for independent scaling.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': None,
            'evaluation': {
                "score": 90,
                "feedback": "Excellent system design thinking covering key scalability patterns.",
                "strengths": ["Comprehensive approach", "Mentioned key technologies", "Thought about multiple layers"],
            },
            'similarity_score': 0.90,
            'gaps': {
                "missing_concepts": [],
                "improvement_areas": []
            },
            'metadata': {"response_time_seconds": 120},
            'created_at': now - timedelta(days=18) + timedelta(minutes=5),
            'evaluated_at': now - timedelta(days=18) + timedelta(minutes=12),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440001'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'text': 'var is function-scoped and can be redeclared. let and const are block-scoped. const cannot be reassigned after declaration. I use let for variables that change and const for constants.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 45.0,
            'evaluation': {
                'score': 75.0,
                'semantic_similarity': 0.72,
                'completeness': 0.70,
                'relevance': 0.80,
                'sentiment': 'confident',
                'reasoning': 'Good basic understanding of scope differences. Could mention hoisting behavior.',
                'strengths': ['Clear explanation of scope', 'Practical usage mentioned'],
                'weaknesses': ['Missing hoisting details', 'Could explain temporal dead zone'],
                'improvement_suggestions': ['Explain hoisting behavior', 'Mention TDZ for let/const']
            },
            'similarity_score': 0.72,
            'gaps': {
                'concepts': ['hoisting', 'temporal_dead_zone'],
                'confirmed': True,
                'keywords': ['hoisting', 'TDZ'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 45, 'word_count': 32},
            'created_at': now - timedelta(days=12, hours=2, minutes=5),
            'evaluated_at': now - timedelta(days=12, hours=2, minutes=6),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440002'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'text': 'React Hooks let you use state and lifecycle in functional components. I use useState for state, useEffect for side effects like API calls and subscriptions, useContext for sharing data without prop drilling. They make code cleaner than class components.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 60.0,
            'evaluation': {
                'score': 78.0,
                'semantic_similarity': 0.75,
                'completeness': 0.75,
                'relevance': 0.85,
                'sentiment': 'confident',
                'reasoning': 'Solid understanding of common hooks. Could mention more hooks like useMemo, useCallback.',
                'strengths': ['Correct hook examples', 'Practical understanding'],
                'weaknesses': ['Limited to basic hooks', 'Could mention optimization hooks'],
                'improvement_suggestions': ['Learn useMemo and useCallback', 'Understand custom hooks']
            },
            'similarity_score': 0.75,
            'gaps': {
                'concepts': ['useMemo', 'useCallback', 'custom_hooks'],
                'confirmed': False,
                'keywords': ['optimization', 'performance'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 60, 'word_count': 38},
            'created_at': now - timedelta(days=12, hours=2, minutes=15),
            'evaluated_at': now - timedelta(days=12, hours=2, minutes=16),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440003'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'text': 'Unit tests test individual functions in isolation. Integration tests test how components work together. E2E tests test the whole application flow. I use Jest for unit tests and Cypress for E2E.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 55.0,
            'evaluation': {
                'score': 82.0,
                'semantic_similarity': 0.80,
                'completeness': 0.80,
                'relevance': 0.90,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of testing pyramid. Practical tool knowledge.',
                'strengths': ['Clear distinction between test types', 'Tool knowledge'],
                'weaknesses': ['Could mention mocking strategies', 'Test coverage concepts'],
                'improvement_suggestions': ['Learn mocking techniques', 'Understand test coverage metrics']
            },
            'similarity_score': 0.80,
            'gaps': {
                'concepts': ['mocking', 'test_coverage'],
                'confirmed': False,
                'keywords': ['mocking', 'coverage'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 55, 'word_count': 35},
            'created_at': now - timedelta(days=12, hours=2, minutes=25),
            'evaluated_at': now - timedelta(days=12, hours=2, minutes=26),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440004'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440017'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440001'),
            'text': 'Last year I had to learn TypeScript for a project. I started with the official docs, built small projects, and asked my team for help. I was productive within two weeks. The type system helped catch bugs early.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 90.0,
            'evaluation': {
                'score': 85.0,
                'semantic_similarity': 0.82,
                'completeness': 0.85,
                'relevance': 0.90,
                'sentiment': 'positive',
                'reasoning': 'Good STAR method usage. Shows initiative and practical learning approach.',
                'strengths': ['Specific example', 'Clear learning process', 'Measurable outcome'],
                'weaknesses': ['Could mention challenges faced', 'More reflection on lessons learned'],
                'improvement_suggestions': ['Include challenges and how overcome', 'Reflect on key learnings']
            },
            'similarity_score': 0.82,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 90, 'word_count': 48},
            'created_at': now - timedelta(days=12, hours=2, minutes=30),
            'evaluated_at': now - timedelta(days=12, hours=2, minutes=32),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440005'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440001'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'text': 'Dependency injection is a design pattern where dependencies are provided to a class rather than created internally. This improves testability because you can inject mocks, reduces coupling by depending on abstractions, and makes code more maintainable. I use constructor injection primarily, which ensures dependencies are available when the object is created. This follows the Dependency Inversion Principle from SOLID.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 75.0,
            'evaluation': {
                'score': 88.0,
                'semantic_similarity': 0.85,
                'completeness': 0.88,
                'relevance': 0.92,
                'sentiment': 'confident',
                'reasoning': 'Excellent understanding of DI principles and practical application. Links to SOLID principles.',
                'strengths': ['Comprehensive explanation', 'Practical experience', 'SOLID connection'],
                'weaknesses': ['Could mention service locator anti-pattern', 'Container frameworks'],
                'improvement_suggestions': ['Learn DI container frameworks', 'Understand anti-patterns to avoid']
            },
            'similarity_score': 0.85,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 75, 'word_count': 68},
            'created_at': now - timedelta(days=10, hours=3, minutes=5),
            'evaluated_at': now - timedelta(days=10, hours=3, minutes=7),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440006'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'text': 'Hash tables use a hash function to map keys to array indices. When inserting, the hash function computes an index and stores the value there. Collisions happen when different keys hash to the same index. I know two main strategies: chaining uses linked lists at each bucket, and open addressing finds the next available slot using linear or quadratic probing. Chaining is simpler but uses more memory, while open addressing is more memory-efficient but can degrade with high load factors.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 90.0,
            'evaluation': {
                'score': 90.0,
                'semantic_similarity': 0.88,
                'completeness': 0.90,
                'relevance': 0.95,
                'sentiment': 'confident',
                'reasoning': 'Strong understanding of hash table internals and collision resolution. Good trade-off analysis.',
                'strengths': ['Complete explanation', 'Trade-off understanding', 'Practical knowledge'],
                'weaknesses': ['Could mention double hashing', 'Load factor specifics'],
                'improvement_suggestions': ['Learn double hashing technique', 'Understand optimal load factors']
            },
            'similarity_score': 0.88,
            'gaps': {
                'concepts': ['double_hashing', 'optimal_load_factor'],
                'confirmed': False,
                'keywords': ['double hashing', 'load factor'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 90, 'word_count': 85},
            'created_at': now - timedelta(days=10, hours=3, minutes=15),
            'evaluated_at': now - timedelta(days=10, hours=3, minutes=17),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440007'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440004'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'text': 'Authentication verifies who the user is, authorization determines what they can do. I implement auth using JWT tokens stored in httpOnly cookies for security. For authorization, I use role-based access control (RBAC) with middleware that checks user roles before allowing access to protected routes. I also hash passwords with bcrypt and use OAuth2 for third-party login.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 95.0,
            'evaluation': {
                'score': 87.0,
                'semantic_similarity': 0.84,
                'completeness': 0.85,
                'relevance': 0.90,
                'sentiment': 'confident',
                'reasoning': 'Good practical implementation knowledge. Clear distinction between auth and authz.',
                'strengths': ['Clear auth/authz distinction', 'Practical implementation', 'Security awareness'],
                'weaknesses': ['Could mention session management', 'Token refresh strategies'],
                'improvement_suggestions': ['Learn token refresh patterns', 'Understand session vs token trade-offs']
            },
            'similarity_score': 0.84,
            'gaps': {
                'concepts': ['token_refresh', 'session_management'],
                'confirmed': False,
                'keywords': ['refresh token', 'session'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 95, 'word_count': 72},
            'created_at': now - timedelta(days=10, hours=3, minutes=25),
            'evaluated_at': now - timedelta(days=10, hours=3, minutes=27),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440008'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440010'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'text': 'SOLID principles guide object-oriented design. Single Responsibility means a class should have one reason to change. Open/Closed means open for extension, closed for modification. Liskov Substitution means derived classes must be substitutable for base classes. Interface Segregation means clients shouldn\'t depend on interfaces they don\'t use. Dependency Inversion means depend on abstractions, not concretions. I apply these in my code reviews and refactoring.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 100.0,
            'evaluation': {
                'score': 92.0,
                'semantic_similarity': 0.90,
                'completeness': 0.92,
                'relevance': 0.95,
                'sentiment': 'confident',
                'reasoning': 'Excellent comprehensive understanding of all SOLID principles. Shows practical application.',
                'strengths': ['Complete coverage', 'Practical application', 'Clear explanations'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.90,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 100, 'word_count': 88},
            'created_at': now - timedelta(days=10, hours=3, minutes=35),
            'evaluated_at': now - timedelta(days=10, hours=3, minutes=37),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440009'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440002'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440014'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440002'),
            'text': 'When prioritizing with limited time, I consider business value, user impact, dependencies, and risks. I focus on MVP features first, then iterate. I communicate with stakeholders to understand priorities and break work into smaller tasks. I also consider technical debt and maintenance needs.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 80.0,
            'evaluation': {
                'score': 85.0,
                'semantic_similarity': 0.82,
                'completeness': 0.83,
                'relevance': 0.88,
                'sentiment': 'confident',
                'reasoning': 'Good prioritization framework. Shows stakeholder awareness and practical approach.',
                'strengths': ['Clear framework', 'Stakeholder communication', 'MVP approach'],
                'weaknesses': ['Could mention urgency vs importance', 'More specific examples'],
                'improvement_suggestions': ['Learn Eisenhower matrix', 'Practice with real scenarios']
            },
            'similarity_score': 0.82,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 80, 'word_count': 58},
            'created_at': now - timedelta(days=10, hours=3, minutes=40),
            'evaluated_at': now - timedelta(days=10, hours=3, minutes=42),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440010'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'text': 'Monolithic architecture bundles all components into a single deployable unit, which simplifies initial development, testing, and deployment. However, it becomes harder to scale and maintain at large scale. Microservices split functionality into independent, loosely coupled services that can be developed, deployed, and scaled independently. This offers better fault isolation, technology diversity, and team autonomy, but adds complexity in service communication, distributed tracing, and eventual consistency. I choose monoliths for small teams, simple domains, or when starting a project. I choose microservices when dealing with large teams, complex domains requiring different scaling patterns, or when services have distinct lifecycles. The key is not to over-engineer early.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 120.0,
            'evaluation': {
                'score': 95.0,
                'semantic_similarity': 0.93,
                'completeness': 0.95,
                'relevance': 0.98,
                'sentiment': 'very_confident',
                'reasoning': 'Exceptional understanding of architectural patterns with deep trade-off analysis. Shows senior-level thinking.',
                'strengths': ['Comprehensive trade-off analysis', 'Practical decision criteria', 'Senior-level insights'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.93,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 120, 'word_count': 142},
            'created_at': now - timedelta(days=8, hours=1, minutes=5),
            'evaluated_at': now - timedelta(days=8, hours=1, minutes=8),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440011'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'text': 'CAP theorem states that in a distributed system, you can only guarantee two out of three: Consistency (all nodes see same data), Availability (system remains operational), and Partition tolerance (system continues despite network failures). Since partition tolerance is unavoidable in distributed systems, you choose between CP (consistency and partition tolerance) or AP (availability and partition tolerance). CP systems like traditional databases prioritize consistency, while AP systems like DynamoDB prioritize availability. I design systems based on use case: financial transactions need CP, while social media feeds can use AP with eventual consistency.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 110.0,
            'evaluation': {
                'score': 94.0,
                'semantic_similarity': 0.91,
                'completeness': 0.93,
                'relevance': 0.96,
                'sentiment': 'very_confident',
                'reasoning': 'Excellent understanding of CAP theorem with practical application examples. Shows deep distributed systems knowledge.',
                'strengths': ['Complete CAP explanation', 'Practical examples', 'Use case understanding'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.91,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 110, 'word_count': 118},
            'created_at': now - timedelta(days=8, hours=1, minutes=20),
            'evaluated_at': now - timedelta(days=8, hours=1, minutes=23),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440012'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'text': 'The event loop is Node.js\'s core mechanism for handling asynchronous operations. It\'s a single-threaded loop that continuously checks the call stack and callback queue. When the call stack is empty, it moves callbacks from the queue to the stack. The event loop has phases: timers (setTimeout/setInterval), pending callbacks, idle/prepare, poll (I/O), check (setImmediate), and close callbacks. This allows Node.js to handle thousands of concurrent connections efficiently despite being single-threaded. I use worker threads for CPU-intensive tasks to avoid blocking the event loop.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 105.0,
            'evaluation': {
                'score': 93.0,
                'semantic_similarity': 0.90,
                'completeness': 0.92,
                'relevance': 0.95,
                'sentiment': 'very_confident',
                'reasoning': 'Deep understanding of event loop internals with practical optimization knowledge.',
                'strengths': ['Complete phase explanation', 'Practical optimization', 'Worker threads knowledge'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.90,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 105, 'word_count': 108},
            'created_at': now - timedelta(days=8, hours=1, minutes=35),
            'evaluated_at': now - timedelta(days=8, hours=1, minutes=38),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440013'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440017'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'text': 'I implement multi-layer caching: browser cache for static assets, CDN for global distribution, Redis for application-level caching with TTL, and database query caching. I use cache-aside pattern where the application checks cache first, then database. For invalidation, I use TTL-based expiration and event-driven invalidation when data changes. I also implement cache warming for frequently accessed data and monitor cache hit rates. For distributed systems, I use consistent hashing to distribute cache across nodes.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 115.0,
            'evaluation': {
                'score': 96.0,
                'semantic_similarity': 0.94,
                'completeness': 0.96,
                'relevance': 0.98,
                'sentiment': 'very_confident',
                'reasoning': 'Exceptional comprehensive caching strategy covering all layers and patterns. Shows production experience.',
                'strengths': ['Multi-layer approach', 'Pattern knowledge', 'Monitoring awareness', 'Distributed systems'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.94,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 115, 'word_count': 98},
            'created_at': now - timedelta(days=8, hours=1, minutes=45),
            'evaluated_at': now - timedelta(days=8, hours=1, minutes=48),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440014'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440003'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440008'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440003'),
            'text': 'To handle 1M requests per second, I\'d design a horizontally scalable architecture. Load balancers distribute traffic across multiple application servers. I\'d use caching layers (Redis cluster) for frequently accessed data, CDN for static content, and database sharding with read replicas. Message queues (Kafka) handle async processing. I\'d implement rate limiting, circuit breakers, and auto-scaling. Database would use connection pooling and query optimization. I\'d monitor with distributed tracing and metrics. The key is identifying bottlenecks and scaling each layer independently.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 130.0,
            'evaluation': {
                'score': 97.0,
                'semantic_similarity': 0.95,
                'completeness': 0.97,
                'relevance': 0.99,
                'sentiment': 'very_confident',
                'reasoning': 'Outstanding system design thinking covering all critical aspects. Shows expert-level architecture knowledge.',
                'strengths': ['Comprehensive architecture', 'All key components', 'Monitoring and resilience', 'Bottleneck awareness'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.95,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 130, 'word_count': 112},
            'created_at': now - timedelta(days=8, hours=1, minutes=50),
            'evaluated_at': now - timedelta(days=8, hours=1, minutes=53),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440015'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'text': 'React Hooks are functions that enable functional components to use state and lifecycle features. I use useState for component state, useEffect for side effects like API calls and subscriptions, useContext for sharing data without prop drilling, useMemo for expensive computations, and useCallback to memoize functions. Custom hooks let me extract reusable logic. Hooks follow rules: only call at top level and only in React functions. They make code more reusable and easier to test than class components.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 95.0,
            'evaluation': {
                'score': 91.0,
                'semantic_similarity': 0.88,
                'completeness': 0.90,
                'relevance': 0.94,
                'sentiment': 'confident',
                'reasoning': 'Excellent comprehensive understanding of React Hooks with optimization hooks and best practices.',
                'strengths': ['Complete hook coverage', 'Optimization hooks', 'Rules of hooks', 'Custom hooks'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.88,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 95, 'word_count': 96},
            'created_at': now - timedelta(days=6, hours=2, minutes=30, seconds=5),
            'evaluated_at': now - timedelta(days=6, hours=2, minutes=30, seconds=7),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440016'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'text': 'The event loop handles asynchronous operations in JavaScript. It continuously checks the call stack and callback queue. When the stack is empty, it moves callbacks to the stack. The loop has phases: timers, pending callbacks, poll, check, and close. This allows JavaScript to be non-blocking despite being single-threaded. I use async/await for cleaner async code and avoid blocking the event loop with heavy computations.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 70.0,
            'evaluation': {
                'score': 86.0,
                'semantic_similarity': 0.83,
                'completeness': 0.84,
                'relevance': 0.90,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of event loop basics. Could go deeper into phases and microtasks.',
                'strengths': ['Core concept clear', 'Practical usage', 'Non-blocking understanding'],
                'weaknesses': ['Could detail microtask queue', 'More phase specifics'],
                'improvement_suggestions': ['Learn microtask vs macrotask', 'Study event loop phases in depth']
            },
            'similarity_score': 0.83,
            'gaps': {
                'concepts': ['microtask_queue', 'macrotask'],
                'confirmed': False,
                'keywords': ['microtask', 'macrotask'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 70, 'word_count': 68},
            'created_at': now - timedelta(days=6, hours=2, minutes=45),
            'evaluated_at': now - timedelta(days=6, hours=2, minutes=46),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440017'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440007'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'text': 'Unit tests test individual functions or components in isolation, typically with mocks. Integration tests verify how multiple components work together. E2E tests simulate real user workflows. I use Jest and React Testing Library for unit and integration tests, and Cypress for E2E. I aim for high unit test coverage, fewer integration tests, and critical path E2E tests. This follows the testing pyramid.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 75.0,
            'evaluation': {
                'score': 89.0,
                'semantic_similarity': 0.86,
                'completeness': 0.87,
                'relevance': 0.92,
                'sentiment': 'confident',
                'reasoning': 'Strong testing knowledge with practical tool usage and pyramid understanding.',
                'strengths': ['Clear test type distinctions', 'Tool knowledge', 'Testing pyramid', 'Coverage strategy'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.86,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 75, 'word_count': 71},
            'created_at': now - timedelta(days=6, hours=3, minutes=0),
            'evaluated_at': now - timedelta(days=6, hours=3, minutes=2),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440018'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440004'),
            'text': 'async/await is syntactic sugar over Promises that makes asynchronous code look synchronous. An async function returns a Promise. await pauses execution until the Promise resolves. I use try/catch for error handling. It\'s cleaner than promise chains and easier to read. Under the hood, it still uses Promises and the event loop. I use it for API calls, file operations, and any async operations.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 65.0,
            'evaluation': {
                'score': 88.0,
                'semantic_similarity': 0.85,
                'completeness': 0.86,
                'relevance': 0.92,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of async/await with practical usage. Could mention parallel execution.',
                'strengths': ['Clear explanation', 'Error handling', 'Practical usage'],
                'weaknesses': ['Could mention Promise.all', 'Parallel execution patterns'],
                'improvement_suggestions': ['Learn Promise.all for parallel execution', 'Understand async iteration']
            },
            'similarity_score': 0.85,
            'gaps': {
                'concepts': ['Promise.all', 'parallel_execution'],
                'confirmed': False,
                'keywords': ['Promise.all', 'parallel'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 65, 'word_count': 66},
            'created_at': now - timedelta(days=6, hours=3, minutes=10),
            'evaluated_at': now - timedelta(days=6, hours=3, minutes=12),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440019'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440005'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440005'),
            'text': 'Microservices split applications into independent services that can be deployed separately. Each service has its own database and communicates via APIs. This allows teams to work independently and scale services individually. Monoliths are simpler but harder to scale. I\'ve worked with Kubernetes to orchestrate microservices and use service meshes for communication.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 85.0,
            'evaluation': {
                'score': 87.0,
                'semantic_similarity': 0.84,
                'completeness': 0.85,
                'relevance': 0.90,
                'sentiment': 'confident',
                'reasoning': 'Good understanding with practical Kubernetes experience. Could mention more trade-offs.',
                'strengths': ['Clear explanation', 'Kubernetes experience', 'Practical knowledge'],
                'weaknesses': ['Could detail more trade-offs', 'Service mesh specifics'],
                'improvement_suggestions': ['Learn service mesh patterns', 'Understand distributed tracing']
            },
            'similarity_score': 0.84,
            'gaps': {
                'concepts': ['service_mesh', 'distributed_tracing'],
                'confirmed': False,
                'keywords': ['service mesh', 'tracing'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 85, 'word_count': 58},
            'created_at': now - timedelta(days=4, hours=1, minutes=5),
            'evaluated_at': now - timedelta(days=4, hours=1, minutes=7),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440020'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440005'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440018'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440005'),
            'text': 'Horizontal scaling adds more servers or instances, while vertical scaling increases resources on existing servers. Horizontal is better for cloud because you can add instances on demand. Vertical has limits based on hardware. I prefer horizontal for microservices because it provides better fault tolerance and can scale individual services. I use auto-scaling groups in AWS to handle traffic spikes.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 90.0,
            'evaluation': {
                'score': 89.0,
                'semantic_similarity': 0.86,
                'completeness': 0.88,
                'relevance': 0.93,
                'sentiment': 'confident',
                'reasoning': 'Excellent understanding with practical cloud experience. Good fault tolerance awareness.',
                'strengths': ['Clear distinction', 'Cloud experience', 'Fault tolerance', 'Practical application'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.86,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 90, 'word_count': 72},
            'created_at': now - timedelta(days=4, hours=1, minutes=20),
            'evaluated_at': now - timedelta(days=4, hours=1, minutes=22),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440021'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440006'),
            'question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440006'),
            'text': 'Garbage collection automatically frees memory by removing unused objects. Java uses generational GC with young generation (Eden, Survivor spaces) and old generation. Objects start in Eden, survive collections move to Survivor, and long-lived objects go to old generation. The GC pauses application execution. I tune GC settings based on application needs and monitor GC logs.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 80.0,
            'evaluation': {
                'score': 83.0,
                'semantic_similarity': 0.80,
                'completeness': 0.81,
                'relevance': 0.88,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of Java GC with generational model. Could mention GC algorithms.',
                'strengths': ['Generational model clear', 'Practical tuning', 'Monitoring awareness'],
                'weaknesses': ['Could mention GC algorithms', 'More on pause times'],
                'improvement_suggestions': ['Learn different GC algorithms', 'Understand GC tuning strategies']
            },
            'similarity_score': 0.80,
            'gaps': {
                'concepts': ['gc_algorithms', 'pause_time_optimization'],
                'confirmed': True,
                'keywords': ['GC algorithms', 'pause times'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 80, 'word_count': 70},
            'created_at': now - timedelta(days=2, hours=2, minutes=5),
            'evaluated_at': now - timedelta(days=2, hours=2, minutes=7),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440022'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'text': 'CAP theorem says in distributed systems you can only have two of three: Consistency, Availability, Partition tolerance. Since partitions are inevitable, you choose CP or AP. CP systems prioritize consistency, AP systems prioritize availability. I design based on use case: financial data needs CP, social feeds can use AP with eventual consistency.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 70.0,
            'evaluation': {
                'score': 88.0,
                'semantic_similarity': 0.85,
                'completeness': 0.86,
                'relevance': 0.91,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of CAP theorem with practical application. Could mention more examples.',
                'strengths': ['Clear CAP explanation', 'Practical examples', 'Use case understanding'],
                'weaknesses': ['Could mention more database examples', 'Consistency models'],
                'improvement_suggestions': ['Learn consistency models', 'Study real-world CAP implementations']
            },
            'similarity_score': 0.85,
            'gaps': {
                'concepts': ['consistency_models', 'real_world_implementations'],
                'confirmed': False,
                'keywords': ['consistency models'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 70, 'word_count': 58},
            'created_at': now - timedelta(days=1, hours=3, minutes=5),
            'evaluated_at': now - timedelta(days=1, hours=3, minutes=7),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440023'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440008'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'text': 'Python uses reference counting and generational garbage collection. Each object has a reference count. When it reaches zero, memory is freed. The cyclic garbage collector handles circular references. The GC has three generations. I can control it with gc module and disable it for performance-critical code.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 65.0,
            'evaluation': {
                'score': 85.0,
                'semantic_similarity': 0.82,
                'completeness': 0.83,
                'relevance': 0.89,
                'sentiment': 'confident',
                'reasoning': 'Good understanding of Python GC mechanisms. Could mention more details on generations.',
                'strengths': ['Reference counting clear', 'Cyclic GC mentioned', 'Practical control'],
                'weaknesses': ['Could detail generation specifics', 'More on performance tuning'],
                'improvement_suggestions': ['Learn GC generation details', 'Understand GC tuning for performance']
            },
            'similarity_score': 0.82,
            'gaps': {
                'concepts': ['generation_details', 'gc_tuning'],
                'confirmed': False,
                'keywords': ['generations', 'tuning'],
                'entities': []
            },
            'metadata': {'response_time_seconds': 65, 'word_count': 52},
            'created_at': now - timedelta(days=1, hours=3, minutes=20),
            'evaluated_at': now - timedelta(days=1, hours=3, minutes=22),
        },
        {
            'id': uuid.UUID('a60e8400-e29b-41d4-a716-446655440024'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'candidate_id': uuid.UUID('660e8400-e29b-41d4-a716-446655440007'),
            'text': 'Microservices architecture splits applications into independent services. Each service has its own database and team. Benefits include independent deployment, technology diversity, and fault isolation. Challenges include service communication, distributed transactions, and monitoring complexity. I use API gateways, service discovery, and distributed tracing. I prefer starting with monolith and extracting services when needed.',
            'is_voice': False,
            'audio_file_path': None,
            'duration_seconds': 95.0,
            'evaluation': {
                'score': 90.0,
                'semantic_similarity': 0.87,
                'completeness': 0.89,
                'relevance': 0.93,
                'sentiment': 'confident',
                'reasoning': 'Excellent understanding with practical patterns and pragmatic approach to adoption.',
                'strengths': ['Complete explanation', 'Practical patterns', 'Pragmatic approach', 'Challenges awareness'],
                'weaknesses': [],
                'improvement_suggestions': []
            },
            'similarity_score': 0.87,
            'gaps': {
                'concepts': [],
                'confirmed': False,
                'keywords': [],
                'entities': []
            },
            'metadata': {'response_time_seconds': 95, 'word_count': 76},
            'created_at': now - timedelta(days=1, hours=3, minutes=35),
            'evaluated_at': now - timedelta(days=1, hours=3, minutes=37),
        },
    ]

    op.bulk_insert(answers_table, answers_data)

    print("[OK] Seeded answers")

    # =============================================
    # SEED DATA - FOLLOW-UP QUESTIONS (12 total)
    # =============================================
    follow_up_data = [
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440001'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'text': 'You mentioned the scope differences. Can you explain what hoisting is in JavaScript and how it differs between var, let, and const?',
            'generated_reason': 'Missing key concept: hoisting behavior. The answer covered scope but did not mention how variable declarations are hoisted differently for var vs let/const.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=12, hours=2, minutes=7),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440002'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440001'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'text': 'What is the Temporal Dead Zone (TDZ) and how does it relate to let and const?',
            'generated_reason': 'Missing key concept: Temporal Dead Zone. The answer did not explain why accessing let/const before declaration throws a ReferenceError, which is a critical understanding gap.',
            'order_in_sequence': 2,
            'created_at': now - timedelta(days=12, hours=2, minutes=8),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440003'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440007'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440001'),
            'text': 'You mentioned useState and useEffect. Can you explain when you would use useMemo and useCallback, and what problems they solve?',
            'generated_reason': 'Missing optimization hooks: useMemo and useCallback. The answer covered basic hooks but did not demonstrate understanding of performance optimization hooks, which are important for React applications.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=12, hours=2, minutes=17),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440004'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440009'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'text': 'You mentioned the event loop phases. Can you explain the difference between the microtask queue and the macrotask queue, and how they are processed?',
            'generated_reason': 'Missing key concept: microtask vs macrotask queues. The answer covered event loop phases but did not explain the critical distinction between microtasks (Promise callbacks) and macrotasks (setTimeout), which is essential for understanding async execution order.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=6, hours=2, minutes=47),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440005'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440003'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440004'),
            'text': 'You explained async/await well. How would you execute multiple async operations in parallel, and what would you use for that?',
            'generated_reason': 'Missing parallel execution pattern: Promise.all. The answer covered sequential async/await usage but did not demonstrate understanding of parallel execution, which is a common optimization pattern.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=6, hours=3, minutes=13),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440006'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440005'),
            'text': 'You mentioned Kubernetes for orchestration. Can you explain what a service mesh is and how it helps with microservices communication and observability?',
            'generated_reason': 'Missing key concept: service mesh. The answer showed good understanding of microservices and Kubernetes but did not cover service mesh patterns, which are important for production microservices architectures.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=4, hours=1, minutes=8),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440007'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440002'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440005'),
            'text': 'How would you implement distributed tracing across microservices to debug issues that span multiple services?',
            'generated_reason': 'Missing observability pattern: distributed tracing. The answer covered microservices architecture but did not address how to trace requests across service boundaries, which is critical for debugging distributed systems.',
            'order_in_sequence': 2,
            'created_at': now - timedelta(days=4, hours=1, minutes=9),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440008'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440006'),
            'text': 'You explained the generational GC model. Can you name and compare the different GC algorithms available in the JVM, such as G1, Parallel, and ZGC?',
            'generated_reason': 'Missing key knowledge: GC algorithms. The answer covered generational GC basics but did not demonstrate knowledge of different GC algorithms and their trade-offs, which is important for JVM tuning.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=2, hours=2, minutes=8),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440009'),
            'parent_question_id': uuid.UUID('650e8400-e29b-41d4-a716-446655440023'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440006'),
            'text': 'How would you optimize GC pause times for a low-latency application? What strategies would you use?',
            'generated_reason': 'Missing optimization knowledge: GC pause time reduction. The answer mentioned monitoring GC logs but did not cover strategies for minimizing pause times, which is critical for latency-sensitive applications.',
            'order_in_sequence': 2,
            'created_at': now - timedelta(days=2, hours=2, minutes=9),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440010'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440005'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'text': 'You explained CP and AP systems well. Can you describe different consistency models, such as eventual consistency, strong consistency, and causal consistency?',
            'generated_reason': 'Missing key concept: consistency models. The answer covered CAP theorem basics but did not explain different consistency models, which are important for understanding how distributed systems handle data consistency.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=1, hours=3, minutes=8),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440011'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440008'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'text': 'You mentioned three generations. Can you explain in detail how the generational GC works in Python, including the threshold values and collection frequency for each generation?',
            'generated_reason': 'Missing detailed knowledge: GC generation specifics. The answer covered basic GC mechanisms but did not explain the detailed workings of generational GC, including thresholds and collection strategies, which is important for Python performance tuning.',
            'order_in_sequence': 1,
            'created_at': now - timedelta(days=1, hours=3, minutes=23),
        },
        {
            'id': uuid.UUID('b60e8400-e29b-41d4-a716-446655440012'),
            'parent_question_id': uuid.UUID('760e8400-e29b-41d4-a716-446655440008'),
            'interview_id': uuid.UUID('960e8400-e29b-41d4-a716-446655440007'),
            'text': 'How would you tune Python\'s garbage collector for a high-throughput application? What gc module functions and parameters would you adjust?',
            'generated_reason': 'Missing practical knowledge: GC tuning strategies. The answer mentioned the gc module but did not demonstrate understanding of how to tune GC for performance, which is important for production Python applications.',
            'order_in_sequence': 2,
            'created_at': now - timedelta(days=1, hours=3, minutes=24),
        },
    ]

    op.bulk_insert(follow_up_questions_table, follow_up_data)

    print("[OK] Seeded follow-up questions")
    print("[OK] All seed data inserted successfully")


def downgrade() -> None:
    """Downgrade - delete only seeded data generated by this migration."""
    conn = op.get_bind()

    # Delete in reverse order of dependencies to respect foreign key constraints
    print("[INFO] Removing seeded data...")

    # Define all IDs that were inserted by this migration
    follow_up_question_ids = [
        'b60e8400-e29b-41d4-a716-446655440001', 'b60e8400-e29b-41d4-a716-446655440002',
        'b60e8400-e29b-41d4-a716-446655440003', 'b60e8400-e29b-41d4-a716-446655440004',
        'b60e8400-e29b-41d4-a716-446655440005', 'b60e8400-e29b-41d4-a716-446655440006',
        'b60e8400-e29b-41d4-a716-446655440007', 'b60e8400-e29b-41d4-a716-446655440008',
        'b60e8400-e29b-41d4-a716-446655440009', 'b60e8400-e29b-41d4-a716-446655440010',
        'b60e8400-e29b-41d4-a716-446655440011', 'b60e8400-e29b-41d4-a716-446655440012',
    ]

    answer_ids = [
        '950e8400-e29b-41d4-a716-446655440001', '950e8400-e29b-41d4-a716-446655440002',
        '950e8400-e29b-41d4-a716-446655440003', '950e8400-e29b-41d4-a716-446655440004',
        '950e8400-e29b-41d4-a716-446655440005', '950e8400-e29b-41d4-a716-446655440006',
        '950e8400-e29b-41d4-a716-446655440007',
        'a60e8400-e29b-41d4-a716-446655440001', 'a60e8400-e29b-41d4-a716-446655440002',
        'a60e8400-e29b-41d4-a716-446655440003', 'a60e8400-e29b-41d4-a716-446655440004',
        'a60e8400-e29b-41d4-a716-446655440005', 'a60e8400-e29b-41d4-a716-446655440006',
        'a60e8400-e29b-41d4-a716-446655440007', 'a60e8400-e29b-41d4-a716-446655440008',
        'a60e8400-e29b-41d4-a716-446655440009', 'a60e8400-e29b-41d4-a716-446655440010',
        'a60e8400-e29b-41d4-a716-446655440011', 'a60e8400-e29b-41d4-a716-446655440012',
        'a60e8400-e29b-41d4-a716-446655440013', 'a60e8400-e29b-41d4-a716-446655440014',
        'a60e8400-e29b-41d4-a716-446655440015', 'a60e8400-e29b-41d4-a716-446655440016',
        'a60e8400-e29b-41d4-a716-446655440017', 'a60e8400-e29b-41d4-a716-446655440018',
        'a60e8400-e29b-41d4-a716-446655440019', 'a60e8400-e29b-41d4-a716-446655440020',
        'a60e8400-e29b-41d4-a716-446655440021', 'a60e8400-e29b-41d4-a716-446655440022',
        'a60e8400-e29b-41d4-a716-446655440023', 'a60e8400-e29b-41d4-a716-446655440024',
    ]

    interview_ids = [
        '850e8400-e29b-41d4-a716-446655440001', '850e8400-e29b-41d4-a716-446655440002',
        '850e8400-e29b-41d4-a716-446655440003', '850e8400-e29b-41d4-a716-446655440004',
        '960e8400-e29b-41d4-a716-446655440001', '960e8400-e29b-41d4-a716-446655440002',
        '960e8400-e29b-41d4-a716-446655440003', '960e8400-e29b-41d4-a716-446655440004',
        '960e8400-e29b-41d4-a716-446655440005', '960e8400-e29b-41d4-a716-446655440006',
        '960e8400-e29b-41d4-a716-446655440007', '960e8400-e29b-41d4-a716-446655440008',
        '960e8400-e29b-41d4-a716-446655440009', '960e8400-e29b-41d4-a716-446655440010',
    ]

    cv_analysis_ids = [
        '750e8400-e29b-41d4-a716-446655440001', '750e8400-e29b-41d4-a716-446655440002',
        '750e8400-e29b-41d4-a716-446655440003',
        '860e8400-e29b-41d4-a716-446655440001', '860e8400-e29b-41d4-a716-446655440002',
        '860e8400-e29b-41d4-a716-446655440003', '860e8400-e29b-41d4-a716-446655440004',
        '860e8400-e29b-41d4-a716-446655440005', '860e8400-e29b-41d4-a716-446655440006',
        '860e8400-e29b-41d4-a716-446655440007',
    ]

    question_ids = [
        '650e8400-e29b-41d4-a716-446655440001', '650e8400-e29b-41d4-a716-446655440002',
        '650e8400-e29b-41d4-a716-446655440003', '650e8400-e29b-41d4-a716-446655440004',
        '650e8400-e29b-41d4-a716-446655440005', '650e8400-e29b-41d4-a716-446655440006',
        '650e8400-e29b-41d4-a716-446655440007', '650e8400-e29b-41d4-a716-446655440008',
        '650e8400-e29b-41d4-a716-446655440009', '650e8400-e29b-41d4-a716-446655440010',
        '650e8400-e29b-41d4-a716-446655440011', '650e8400-e29b-41d4-a716-446655440012',
        '650e8400-e29b-41d4-a716-446655440013', '650e8400-e29b-41d4-a716-446655440014',
        '650e8400-e29b-41d4-a716-446655440015', '650e8400-e29b-41d4-a716-446655440016',
        '650e8400-e29b-41d4-a716-446655440017', '650e8400-e29b-41d4-a716-446655440018',
        '650e8400-e29b-41d4-a716-446655440019', '650e8400-e29b-41d4-a716-446655440020',
        '650e8400-e29b-41d4-a716-446655440021', '650e8400-e29b-41d4-a716-446655440022',
        '650e8400-e29b-41d4-a716-446655440023',
        '760e8400-e29b-41d4-a716-446655440001', '760e8400-e29b-41d4-a716-446655440002',
        '760e8400-e29b-41d4-a716-446655440003', '760e8400-e29b-41d4-a716-446655440004',
        '760e8400-e29b-41d4-a716-446655440005', '760e8400-e29b-41d4-a716-446655440006',
        '760e8400-e29b-41d4-a716-446655440007', '760e8400-e29b-41d4-a716-446655440008',
        '760e8400-e29b-41d4-a716-446655440009', '760e8400-e29b-41d4-a716-446655440010',
        '760e8400-e29b-41d4-a716-446655440011', '760e8400-e29b-41d4-a716-446655440012',
        '760e8400-e29b-41d4-a716-446655440013', '760e8400-e29b-41d4-a716-446655440014',
        '760e8400-e29b-41d4-a716-446655440015', '760e8400-e29b-41d4-a716-446655440016',
        '760e8400-e29b-41d4-a716-446655440017', '760e8400-e29b-41d4-a716-446655440018',
    ]

    candidate_ids = [
        '550e8400-e29b-41d4-a716-446655440001', '550e8400-e29b-41d4-a716-446655440002',
        '550e8400-e29b-41d4-a716-446655440003',
        '660e8400-e29b-41d4-a716-446655440001', '660e8400-e29b-41d4-a716-446655440002',
        '660e8400-e29b-41d4-a716-446655440003', '660e8400-e29b-41d4-a716-446655440004',
        '660e8400-e29b-41d4-a716-446655440005', '660e8400-e29b-41d4-a716-446655440006',
        '660e8400-e29b-41d4-a716-446655440007',
    ]

    # Delete follow-up questions first (depends on interviews and questions)
    if follow_up_question_ids:
        ids_list = "', '".join(follow_up_question_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM follow_up_questions WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} follow-up questions")
        else:
            print("[SKIP] No matching follow-up questions found")

    # Delete answers (depends on interviews, questions, candidates)
    if answer_ids:
        ids_list = "', '".join(answer_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM answers WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} answers")
        else:
            print("[SKIP] No matching answers found")

    # Delete interviews (depends on candidates, cv_analyses)
    if interview_ids:
        ids_list = "', '".join(interview_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM interviews WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} interviews")
        else:
            print("[SKIP] No matching interviews found")

    # Delete CV analyses (depends on candidates)
    if cv_analysis_ids:
        ids_list = "', '".join(cv_analysis_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM cv_analyses WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} CV analyses")
        else:
            print("[SKIP] No matching CV analyses found")

    # Delete questions (no dependencies from seed data)
    if question_ids:
        ids_list = "', '".join(question_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM questions WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} questions")
        else:
            print("[SKIP] No matching questions found")

    # Delete candidates last (no dependencies from seed data)
    if candidate_ids:
        ids_list = "', '".join(candidate_ids)
        result = conn.execute(sa.text(
            f"DELETE FROM candidates WHERE id::text IN ('{ids_list}')"
        ))
        deleted_count = result.rowcount
        if deleted_count > 0:
            print(f"[OK] Deleted {deleted_count} candidates")
        else:
            print("[SKIP] No matching candidates found")

    print("[OK] All seeded data removal completed")
</file>

<file path="src/adapters/api/rest/health_routes.py">
"""Health check routes."""

from datetime import datetime

from fastapi import APIRouter
from pydantic import BaseModel

from ....infrastructure.config import get_settings

router = APIRouter()


class HealthResponse(BaseModel):
    """Health check response model."""
    status: str
    version: str
    environment: str
    timestamp: datetime


@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse:
    """Health check endpoint.

    Returns:
        Health status information
    """
    settings = get_settings()

    return HealthResponse(
        status="healthy",
        version=settings.app_version,
        environment=settings.environment,
        timestamp=datetime.utcnow(),
    )


@router.get("/")
async def root():
    """Root endpoint.

    Returns:
        Welcome message
    """
    settings = get_settings()

    return {
        "message": f"Welcome to {settings.app_name}",
        "version": settings.app_version,
        "docs": "/docs",
        "health": "/health",
    }
</file>

<file path="src/adapters/cv_processing/__init__.py">
"""CV processing adapters package."""
</file>

<file path="src/adapters/mock/__init__.py">
"""Mock adapters for development and testing."""

from .mock_analytics import MockAnalyticsAdapter
from .mock_cv_analyzer import MockCVAnalyzerAdapter
from .mock_llm_adapter import MockLLMAdapter
from .mock_stt_adapter import MockSTTAdapter
from .mock_tts_adapter import MockTTSAdapter
from .mock_vector_search_adapter import MockVectorSearchAdapter

__all__ = [
    "MockAnalyticsAdapter",
    "MockCVAnalyzerAdapter",
    "MockLLMAdapter",
    "MockSTTAdapter",
    "MockTTSAdapter",
    "MockVectorSearchAdapter",
]
</file>

<file path="src/adapters/mock/mock_stt_adapter.py">
"""Mock Speech-to-Text adapter for development and testing."""

import random
from typing import Any

from ...domain.ports.speech_to_text_port import SpeechToTextPort


class MockSTTAdapter(SpeechToTextPort):
    """Mock STT adapter that returns placeholder transcriptions with voice metrics.

    This adapter simulates speech-to-text behavior for development
    and testing without requiring actual STT service calls.

    Voice metrics are generated using deterministic random values based on
    audio size to provide consistent test results.
    """

    def __init__(self, seed: int | None = None):
        """Initialize mock adapter.

        Args:
            seed: Random seed for deterministic metrics (default: None for random)
        """
        self.seed = seed
        if seed is not None:
            random.seed(seed)

    async def transcribe_audio(
        self,
        audio_bytes: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Mock transcription from audio bytes with voice metrics.

        Args:
            audio_bytes: Audio data as bytes
            language: Language code

        Returns:
            Dict with text, voice_metrics, and metadata
        """
        # Generate mock transcript based on audio size
        audio_size = len(audio_bytes)
        word_count = max(10, audio_size // 1000)  # Estimate words from size
        mock_text = f"Mock transcription with approximately {word_count} words from {audio_size} bytes"

        # Generate realistic voice metrics
        voice_metrics = self._generate_voice_metrics(audio_size, word_count)

        # Calculate duration (assume 16kHz mono WAV)
        duration_seconds = audio_size / (16000 * 2)  # 2 bytes per sample

        return {
            "text": mock_text,
            "voice_metrics": voice_metrics,
            "metadata": {
                "duration_seconds": duration_seconds,
                "audio_format": "wav",
            }
        }

    async def transcribe_stream(
        self,
        audio_stream: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Mock stream transcription with voice metrics.

        Args:
            audio_stream: Audio data stream
            language: Language code

        Returns:
            Same dict structure as transcribe_audio()
        """
        # Reuse transcribe_audio logic
        return await self.transcribe_audio(audio_stream, language)

    async def detect_language(
        self,
        audio_bytes: bytes,
    ) -> str | None:
        """Mock language detection."""
        return "en-US"

    def _generate_voice_metrics(self, audio_size: int, word_count: int) -> dict[str, float]:
        """Generate realistic voice metrics for testing.

        Args:
            audio_size: Size of audio in bytes
            word_count: Estimated word count

        Returns:
            Dict with intonation, fluency, confidence scores and speaking rate
        """
        # Use audio size as seed for consistent metrics per audio
        if self.seed is None:
            random.seed(audio_size)

        # Generate scores with realistic distributions
        # Higher audio size -> slightly better metrics (simulates longer, clearer speech)
        size_factor = min(audio_size / 100000, 1.0)  # Cap at 100KB

        intonation_score = 0.5 + (random.random() * 0.3) + (size_factor * 0.2)
        fluency_score = 0.6 + (random.random() * 0.25) + (size_factor * 0.15)
        confidence_score = 0.7 + (random.random() * 0.2) + (size_factor * 0.1)

        # Clamp to [0, 1]
        intonation_score = min(max(intonation_score, 0.0), 1.0)
        fluency_score = min(max(fluency_score, 0.0), 1.0)
        confidence_score = min(max(confidence_score, 0.0), 1.0)

        # Calculate speaking rate (typical range: 120-180 WPM)
        duration_seconds = audio_size / (16000 * 2)
        speaking_rate_wpm = int((word_count / duration_seconds) * 60) if duration_seconds > 0 else 150
        speaking_rate_wpm = min(max(speaking_rate_wpm, 80), 200)  # Clamp to realistic range

        return {
            "intonation_score": round(intonation_score, 3),
            "fluency_score": round(fluency_score, 3),
            "confidence_score": round(confidence_score, 3),
            "speaking_rate_wpm": speaking_rate_wpm,
        }
</file>

<file path="src/adapters/mock/mock_tts_adapter.py">
"""Mock Text-to-Speech adapter for development and testing."""

import struct

from ...domain.ports.text_to_speech_port import TextToSpeechPort


class MockTTSAdapter(TextToSpeechPort):
    """Mock TTS adapter that generates silent WAV audio data.

    This adapter simulates text-to-speech behavior for development
    and testing without requiring actual TTS service calls.

    Generates properly formatted WAV files with silent audio based on
    text length for predictable testing.
    """

    async def synthesize_speech(
        self,
        text: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> bytes:
        """Mock speech synthesis with proper WAV structure.

        Args:
            text: Text to synthesize
            voice: Voice name (ignored in mock)
            speed: Speaking rate (used to adjust audio length)

        Returns:
            WAV audio bytes (16kHz mono, 16-bit PCM)
        """
        # Estimate audio duration based on text length
        # Assume average speaking rate: 150 words/min = 2.5 words/sec
        word_count = len(text.split())
        duration_seconds = (word_count / 150.0) * 60.0 / speed  # Adjust for speed
        duration_seconds = max(duration_seconds, 0.5)  # Minimum 0.5 seconds

        # Generate silent audio data
        sample_rate = 16000  # 16kHz
        num_channels = 1  # Mono
        bits_per_sample = 16
        num_samples = int(sample_rate * duration_seconds)

        # Create WAV file in memory
        wav_bytes = self._create_wav_bytes(
            sample_rate=sample_rate,
            num_channels=num_channels,
            bits_per_sample=bits_per_sample,
            num_samples=num_samples,
        )

        return wav_bytes

    async def save_speech_to_file(
        self,
        text: str,
        output_path: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> str:
        """Mock save to file.

        Args:
            text: Text to synthesize
            output_path: Path where audio file should be saved
            voice: Voice name
            speed: Speaking rate

        Returns:
            Path to saved audio file
        """
        audio_bytes = await self.synthesize_speech(text, voice, speed)
        with open(output_path, "wb") as f:
            f.write(audio_bytes)
        return output_path

    async def get_available_voices(self) -> list[str]:
        """Get list of available voice names.

        Returns:
            List of mock voice name strings
        """
        return [
            "en-US-AriaNeural",
            "en-US-JennyNeural",
            "en-US-GuyNeural",
            "en-GB-SoniaNeural",
            "en-GB-RyanNeural",
        ]

    def _create_wav_bytes(
        self,
        sample_rate: int,
        num_channels: int,
        bits_per_sample: int,
        num_samples: int,
    ) -> bytes:
        """Create properly formatted WAV file bytes with silent audio.

        Args:
            sample_rate: Sample rate in Hz (e.g., 16000)
            num_channels: Number of audio channels (1 for mono)
            bits_per_sample: Bits per sample (16 for 16-bit PCM)
            num_samples: Total number of samples

        Returns:
            Complete WAV file as bytes
        """
        byte_rate = sample_rate * num_channels * (bits_per_sample // 8)
        block_align = num_channels * (bits_per_sample // 8)
        data_size = num_samples * block_align

        # Build WAV header
        wav = b"RIFF"
        wav += struct.pack("<I", 36 + data_size)  # File size - 8
        wav += b"WAVE"

        # fmt chunk
        wav += b"fmt "
        wav += struct.pack("<I", 16)  # fmt chunk size
        wav += struct.pack("<H", 1)  # Audio format (1 = PCM)
        wav += struct.pack("<H", num_channels)
        wav += struct.pack("<I", sample_rate)
        wav += struct.pack("<I", byte_rate)
        wav += struct.pack("<H", block_align)
        wav += struct.pack("<H", bits_per_sample)

        # data chunk
        wav += b"data"
        wav += struct.pack("<I", data_size)

        # Silent audio data (all zeros)
        wav += b"\x00" * data_size

        return wav
</file>

<file path="src/adapters/mock/mock_vector_search_adapter.py">
"""Mock vector search adapter for development and testing."""

import random
from typing import Any
from uuid import UUID

from ...domain.models.question import DifficultyLevel
from ...domain.ports.vector_search_port import VectorSearchPort


class MockVectorSearchAdapter(VectorSearchPort):
    """Mock vector search adapter that returns fake data.

    This adapter simulates vector database behavior without requiring
    external services like Pinecone, Weaviate, or ChromaDB.
    """

    def __init__(self):
        """Initialize mock adapter with seeded question IDs from database."""
        # Use actual question IDs from seed_data.sql
        # These IDs will exist in the database after running seed script
        self._mock_question_ids = [
            UUID("650e8400-e29b-41d4-a716-446655440001"),  # JS var/let/const - EASY
            UUID("650e8400-e29b-41d4-a716-446655440002"),  # REST API - EASY
            UUID("650e8400-e29b-41d4-a716-446655440003"),  # async/await - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440004"),  # SOLID principles - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440005"),  # Database normalization - HARD
            UUID("650e8400-e29b-41d4-a716-446655440006"),  # Microservices - HARD
            UUID("650e8400-e29b-41d4-a716-446655440007"),  # Behavioral - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440008"),  # Behavioral teamwork - MEDIUM
            UUID("650e8400-e29b-41d4-a716-446655440009"),  # Reverse string - EASY
            UUID("650e8400-e29b-41d4-a716-446655440010"),  # Non-repeating char - MEDIUM
        ]

    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Mock storing question embedding (no-op)."""
        # In mock mode, we just acknowledge the storage
        pass

    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Mock storing CV embedding (no-op)."""
        # In mock mode, we just acknowledge the storage
        pass

    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Return mock similar questions.

        Args:
            query_embedding: Query vector (ignored in mock)
            top_k: Number of results to return
            filters: Optional filters (difficulty, skills, etc.)

        Returns:
            List of mock question matches with metadata
        """
        # Generate mock results
        results = []
        difficulty = filters.get("difficulty", DifficultyLevel.MEDIUM) if filters else DifficultyLevel.MEDIUM

        for i in range(min(top_k, len(self._mock_question_ids))):
            # Generate realistic similarity scores (higher for first results)
            similarity = random.uniform(0.75, 0.95) - (i * 0.02)

            results.append({
                "question_id": str(self._mock_question_ids[i]),
                "similarity_score": similarity,
                "metadata": {
                    "difficulty": difficulty,
                    "skills": ["Python", "FastAPI", "PostgreSQL"][: i % 3 + 1],
                    "tags": ["backend", "api", "database"][: i % 3 + 1],
                },
            })

        return results

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Return mock similarity score between 0.65 and 0.9."""
        return random.uniform(0.65, 0.9)

    async def get_embedding(
        self,
        text: str,
    ) -> list[float]:
        """Generate mock embedding vector.

        Args:
            text: Text to embed

        Returns:
            Mock 1536-dimensional vector (OpenAI embedding size)
        """
        # Return a mock embedding with realistic dimensions
        # OpenAI embeddings are 1536-dimensional
        return [random.uniform(-1.0, 1.0) for _ in range(1536)]

    async def delete_embeddings(
        self,
        ids: list[UUID],
    ) -> None:
        """Mock deleting embeddings (no-op)."""
        # In mock mode, we just acknowledge the deletion
        pass
</file>

<file path="src/adapters/persistence/answer_repository.py">
"""PostgreSQL implementation of AnswerRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.answer import Answer
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from .mappers import AnswerMapper
from .models import AnswerModel


class PostgreSQLAnswerRepository(AnswerRepositoryPort):
    """PostgreSQL implementation of answer repository.

    This adapter implements the AnswerRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, answer: Answer) -> Answer:
        """Save a new answer to the database."""
        db_model = AnswerMapper.to_db_model(answer)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return AnswerMapper.to_domain(db_model)

    async def get_by_id(self, answer_id: UUID) -> Answer | None:
        """Retrieve an answer by ID."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer_id)
        )
        db_model = result.scalar_one_or_none()
        return AnswerMapper.to_domain(db_model) if db_model else None

    async def get_by_ids(self, answer_ids: list[UUID]) -> list[Answer]:
        """Retrieve multiple answers by IDs."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id.in_(answer_ids))
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        """Retrieve all answers for an interview."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.interview_id == interview_id)
            .order_by(AnswerModel.created_at.asc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_question_id(self, question_id: UUID) -> list[Answer]:
        """Retrieve all answers for a question."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.question_id == question_id)
            .order_by(AnswerModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_candidate_id(self, candidate_id: UUID) -> list[Answer]:
        """Retrieve all answers by a candidate."""
        result = await self.session.execute(
            select(AnswerModel)
            .where(AnswerModel.candidate_id == candidate_id)
            .order_by(AnswerModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [AnswerMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, answer: Answer) -> Answer:
        """Update an existing answer."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Answer with id {answer.id} not found")

        AnswerMapper.update_db_model(db_model, answer)
        await self.session.commit()
        await self.session.refresh(db_model)
        return AnswerMapper.to_domain(db_model)

    async def delete(self, answer_id: UUID) -> bool:
        """Delete an answer by ID."""
        result = await self.session.execute(
            select(AnswerModel).where(AnswerModel.id == answer_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True
</file>

<file path="src/adapters/persistence/candidate_repository.py">
"""PostgreSQL implementation of CandidateRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.candidate import Candidate
from ...domain.ports.candidate_repository_port import CandidateRepositoryPort
from .mappers import CandidateMapper
from .models import CandidateModel


class PostgreSQLCandidateRepository(CandidateRepositoryPort):
    """PostgreSQL implementation of candidate repository.

    This adapter implements the CandidateRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, candidate: Candidate) -> Candidate:
        """Save a new candidate to the database."""
        db_model = CandidateMapper.to_db_model(candidate)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CandidateMapper.to_domain(db_model)

    async def get_by_id(self, candidate_id: UUID) -> Candidate | None:
        """Retrieve a candidate by ID."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate_id)
        )
        db_model = result.scalar_one_or_none()
        return CandidateMapper.to_domain(db_model) if db_model else None

    async def get_by_email(self, email: str) -> Candidate | None:
        """Retrieve a candidate by email address."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.email == email)
        )
        db_model = result.scalar_one_or_none()
        return CandidateMapper.to_domain(db_model) if db_model else None

    async def update(self, candidate: Candidate) -> Candidate:
        """Update an existing candidate."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Candidate with id {candidate.id} not found")

        CandidateMapper.update_db_model(db_model, candidate)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CandidateMapper.to_domain(db_model)

    async def delete(self, candidate_id: UUID) -> bool:
        """Delete a candidate by ID."""
        result = await self.session.execute(
            select(CandidateModel).where(CandidateModel.id == candidate_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Candidate]:
        """List all candidates with pagination."""
        result = await self.session.execute(
            select(CandidateModel)
            .order_by(CandidateModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [CandidateMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/persistence/cv_analysis_repository.py">
"""PostgreSQL implementation of CVAnalysisRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.ports.cv_analysis_repository_port import CVAnalysisRepositoryPort
from .mappers import CVAnalysisMapper
from .models import CVAnalysisModel


class PostgreSQLCVAnalysisRepository(CVAnalysisRepositoryPort):
    """PostgreSQL implementation of CV analysis repository.

    This adapter implements the CVAnalysisRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Save a new CV analysis to the database."""
        db_model = CVAnalysisMapper.to_db_model(cv_analysis)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CVAnalysisMapper.to_domain(db_model)

    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        """Retrieve a CV analysis by ID."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis_id)
        )
        db_model = result.scalar_one_or_none()
        return CVAnalysisMapper.to_domain(db_model) if db_model else None

    async def get_by_candidate_id(self, candidate_id: UUID) -> list[CVAnalysis]:
        """Retrieve all CV analyses for a candidate."""
        result = await self.session.execute(
            select(CVAnalysisModel)
            .where(CVAnalysisModel.candidate_id == candidate_id)
            .order_by(CVAnalysisModel.created_at.desc())
        )
        db_models = result.scalars().all()
        return [CVAnalysisMapper.to_domain(db_model) for db_model in db_models]

    async def get_latest_by_candidate_id(
        self,
        candidate_id: UUID,
    ) -> CVAnalysis | None:
        """Retrieve the most recent CV analysis for a candidate."""
        result = await self.session.execute(
            select(CVAnalysisModel)
            .where(CVAnalysisModel.candidate_id == candidate_id)
            .order_by(CVAnalysisModel.created_at.desc())
            .limit(1)
        )
        db_model = result.scalar_one_or_none()
        return CVAnalysisMapper.to_domain(db_model) if db_model else None

    async def update(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Update an existing CV analysis."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"CV Analysis with id {cv_analysis.id} not found")

        CVAnalysisMapper.update_db_model(db_model, cv_analysis)
        await self.session.commit()
        await self.session.refresh(db_model)
        return CVAnalysisMapper.to_domain(db_model)

    async def delete(self, cv_analysis_id: UUID) -> bool:
        """Delete a CV analysis by ID."""
        result = await self.session.execute(
            select(CVAnalysisModel).where(CVAnalysisModel.id == cv_analysis_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True
</file>

<file path="src/adapters/persistence/interview_repository.py">
"""PostgreSQL implementation of InterviewRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.interview import Interview, InterviewStatus
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from .mappers import InterviewMapper
from .models import InterviewModel


class PostgreSQLInterviewRepository(InterviewRepositoryPort):
    """PostgreSQL implementation of interview repository.

    This adapter implements the InterviewRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, interview: Interview) -> Interview:
        """Save a new interview to the database."""
        db_model = InterviewMapper.to_db_model(interview)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return InterviewMapper.to_domain(db_model)

    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        """Retrieve an interview by ID."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview_id)
        )
        db_model = result.scalar_one_or_none()
        return InterviewMapper.to_domain(db_model) if db_model else None

    async def get_by_candidate_id(
        self,
        candidate_id: UUID,
        status: InterviewStatus | None = None,
    ) -> list[Interview]:
        """Retrieve interviews for a candidate with optional status filter."""
        query = select(InterviewModel).where(InterviewModel.candidate_id == candidate_id)

        if status:
            query = query.where(InterviewModel.status == status.value)

        query = query.order_by(InterviewModel.created_at.desc())

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]

    async def get_by_status(
        self,
        status: InterviewStatus,
        limit: int = 100,
    ) -> list[Interview]:
        """Retrieve interviews by status."""
        result = await self.session.execute(
            select(InterviewModel)
            .where(InterviewModel.status == status.value)
            .order_by(InterviewModel.created_at.desc())
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, interview: Interview) -> Interview:
        """Update an existing interview."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Interview with id {interview.id} not found")

        InterviewMapper.update_db_model(db_model, interview)
        await self.session.commit()
        await self.session.refresh(db_model)
        return InterviewMapper.to_domain(db_model)

    async def delete(self, interview_id: UUID) -> bool:
        """Delete an interview by ID."""
        result = await self.session.execute(
            select(InterviewModel).where(InterviewModel.id == interview_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Interview]:
        """List all interviews with pagination."""
        result = await self.session.execute(
            select(InterviewModel)
            .order_by(InterviewModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [InterviewMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/persistence/question_repository.py">
"""PostgreSQL implementation of QuestionRepositoryPort."""

from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from ...domain.models.question import DifficultyLevel, Question, QuestionType
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from .mappers import QuestionMapper
from .models import QuestionModel


class PostgreSQLQuestionRepository(QuestionRepositoryPort):
    """PostgreSQL implementation of question repository.

    This adapter implements the QuestionRepositoryPort interface
    using SQLAlchemy and PostgreSQL for persistence.
    """

    def __init__(self, session: AsyncSession):
        """Initialize repository with database session.

        Args:
            session: Async SQLAlchemy session
        """
        self.session = session

    async def save(self, question: Question) -> Question:
        """Save a new question to the database."""
        db_model = QuestionMapper.to_db_model(question)
        self.session.add(db_model)
        await self.session.commit()
        await self.session.refresh(db_model)
        return QuestionMapper.to_domain(db_model)

    async def get_by_id(self, question_id: UUID) -> Question | None:
        """Retrieve a question by ID."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()
        return QuestionMapper.to_domain(db_model) if db_model else None

    async def get_by_ids(self, question_ids: list[UUID]) -> list[Question]:
        """Retrieve multiple questions by IDs."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id.in_(question_ids))
        )
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_skill(
        self,
        skill: str,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by skill with optional difficulty filter."""
        query = select(QuestionModel).where(
            QuestionModel.skills.contains([skill])  # PostgreSQL array contains
        )

        if difficulty:
            query = query.where(QuestionModel.difficulty == difficulty.value)

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_type(
        self,
        question_type: QuestionType,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by type with optional difficulty filter."""
        query = select(QuestionModel).where(
            QuestionModel.question_type == question_type.value
        )

        if difficulty:
            query = query.where(QuestionModel.difficulty == difficulty.value)

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def find_by_tags(
        self,
        tags: list[str],
        match_all: bool = False,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by tags.

        Args:
            tags: List of tags to search for
            match_all: If True, match all tags; if False, match any tag
            limit: Maximum number of results
        """
        if match_all:
            # Match all tags (array contains all elements)
            query = select(QuestionModel).where(
                QuestionModel.tags.contains(tags)  # PostgreSQL @> operator
            )
        else:
            # Match any tag (array overlap)
            query = select(QuestionModel).where(
                QuestionModel.tags.overlap(tags)  # PostgreSQL && operator
            )

        query = query.limit(limit)

        result = await self.session.execute(query)
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]

    async def update(self, question: Question) -> Question:
        """Update an existing question."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question.id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            raise ValueError(f"Question with id {question.id} not found")

        QuestionMapper.update_db_model(db_model, question)
        await self.session.commit()
        await self.session.refresh(db_model)
        return QuestionMapper.to_domain(db_model)

    async def delete(self, question_id: UUID) -> bool:
        """Delete a question by ID."""
        result = await self.session.execute(
            select(QuestionModel).where(QuestionModel.id == question_id)
        )
        db_model = result.scalar_one_or_none()

        if not db_model:
            return False

        await self.session.delete(db_model)
        await self.session.commit()
        return True

    async def list_all(self, skip: int = 0, limit: int = 100) -> list[Question]:
        """List all questions with pagination."""
        result = await self.session.execute(
            select(QuestionModel)
            .order_by(QuestionModel.created_at.desc())
            .offset(skip)
            .limit(limit)
        )
        db_models = result.scalars().all()
        return [QuestionMapper.to_domain(db_model) for db_model in db_models]
</file>

<file path="src/adapters/speech/__init__.py">
"""Speech adapters package."""

from .azure_stt_adapter import AzureSpeechToTextAdapter
from .azure_tts_adapter import AzureTextToSpeechAdapter

__all__ = [
    "AzureSpeechToTextAdapter",
    "AzureTextToSpeechAdapter",
]
</file>

<file path="src/adapters/vector_db/pinecone_adapter.py">
"""Pinecone vector database adapter implementation."""

from typing import Any
from uuid import UUID

from openai import AsyncOpenAI
from pinecone import Pinecone, ServerlessSpec

from ...domain.ports.vector_search_port import VectorSearchPort


class PineconeAdapter(VectorSearchPort):
    """Pinecone implementation of vector search port.

    This adapter encapsulates all Pinecone-specific logic. Switching to
    another vector database (Weaviate, ChromaDB) only requires implementing
    the VectorSearchPort interface.
    """

    def __init__(
        self,
        api_key: str,
        environment: str,
        index_name: str,
        openai_api_key: str,
        embedding_model: str = "text-embedding-3-small",
    ):
        """Initialize Pinecone adapter.

        Args:
            api_key: Pinecone API key
            environment: Pinecone environment
            index_name: Name of the Pinecone index to use
            openai_api_key: OpenAI API key for embeddings
            embedding_model: OpenAI embedding model to use
        """
        self.pc = Pinecone(api_key=api_key)
        self.index_name = index_name
        self.index = None
        self.openai_client = AsyncOpenAI(api_key=openai_api_key)
        self.embedding_model = embedding_model
        self.environment = environment

        # Initialize index if it doesn't exist
        self._ensure_index_exists()

    def _ensure_index_exists(self) -> None:
        """Ensure the Pinecone index exists."""
        existing_indexes = [idx.name for idx in self.pc.list_indexes()]

        if self.index_name not in existing_indexes:
            # Create index with 1536 dimensions (OpenAI text-embedding-3-small)
            self.pc.create_index(
                name=self.index_name,
                dimension=1536,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region=self.environment),
            )

        self.index = self.pc.Index(self.index_name)

    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a question's vector embedding in Pinecone.

        Args:
            question_id: Unique question identifier
            embedding: Vector embedding
            metadata: Additional metadata
        """
        self.index.upsert(
            vectors=[
                {
                    "id": f"question_{str(question_id)}",
                    "values": embedding,
                    "metadata": {
                        **metadata,
                        "type": "question",
                        "question_id": str(question_id),
                    },
                }
            ]
        )

    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a CV analysis vector embedding in Pinecone.

        Args:
            cv_analysis_id: Unique CV analysis identifier
            embedding: Vector embedding
            metadata: Additional metadata
        """
        self.index.upsert(
            vectors=[
                {
                    "id": f"cv_{str(cv_analysis_id)}",
                    "values": embedding,
                    "metadata": {
                        **metadata,
                        "type": "cv",
                        "cv_analysis_id": str(cv_analysis_id),
                    },
                }
            ]
        )

    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Find similar questions using semantic search.

        Args:
            query_embedding: Query vector
            top_k: Number of results to return
            filters: Optional filters

        Returns:
            List of similar questions with similarity scores
        """
        # Build Pinecone filter
        pinecone_filter = {"type": "question"}
        if filters:
            pinecone_filter.update(filters)

        # Query Pinecone
        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            filter=pinecone_filter,
            include_metadata=True,
        )

        # Format results
        similar_questions = []
        for match in results.matches:
            similar_questions.append(
                {
                    "question_id": match.metadata.get("question_id"),
                    "score": match.score,
                    "metadata": match.metadata,
                }
            )

        return similar_questions

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Calculate similarity between answer and reference answers.

        Args:
            answer_embedding: Candidate's answer embedding
            reference_embeddings: Reference answer embeddings

        Returns:
            Similarity score (0-1)
        """
        # For simplicity, calculate cosine similarity with the first reference
        # In production, you might want to average or take max similarity
        if not reference_embeddings:
            return 0.0

        # Store temporary reference embedding
        temp_id = "temp_reference"
        self.index.upsert(
            vectors=[
                {
                    "id": temp_id,
                    "values": reference_embeddings[0],
                    "metadata": {"type": "temp"},
                }
            ]
        )

        # Query for similarity
        results = self.index.query(
            vector=answer_embedding,
            top_k=1,
            filter={"type": "temp"},
        )

        # Clean up temp vector
        self.index.delete(ids=[temp_id])

        return results.matches[0].score if results.matches else 0.0

    async def get_embedding(self, text: str) -> list[float]:
        """Generate embedding for text using OpenAI.

        Args:
            text: Text to embed

        Returns:
            Vector embedding
        """
        response = await self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=text,
        )

        return response.data[0].embedding

    async def delete_embeddings(self, ids: list[UUID]) -> None:
        """Delete embeddings by IDs.

        Args:
            ids: List of IDs to delete
        """
        # Delete both question and CV embeddings
        pinecone_ids = []
        for id_ in ids:
            pinecone_ids.extend([f"question_{str(id_)}", f"cv_{str(id_)}"])

        self.index.delete(ids=pinecone_ids)
</file>

<file path="src/application/use_cases/follow_up_decision.py">
"""Follow-up decision logic use case."""

import logging
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)

logger = logging.getLogger(__name__)


class FollowUpDecisionUseCase:
    """Decide if follow-up question should be generated based on answer gaps.

    This use case implements the decision logic for adaptive follow-ups:
    1. Count existing follow-ups for parent question
    2. Check break conditions (max 3, similarity >= 0.8, no gaps)
    3. Accumulate gaps from all previous follow-up answers
    4. Return decision with context for follow-up generation
    """

    def __init__(
        self,
        answer_repository: AnswerRepositoryPort,
        follow_up_question_repository: FollowUpQuestionRepositoryPort,
    ):
        """Initialize use case with required ports.

        Args:
            answer_repository: Answer storage
            follow_up_question_repository: Follow-up question storage
        """
        self.answer_repo = answer_repository
        self.follow_up_repo = follow_up_question_repository

    async def execute(
        self,
        interview_id: UUID,
        parent_question_id: UUID,
        latest_answer: Answer,
    ) -> dict[str, Any]:
        """Decide if follow-up needed and return decision details.

        Args:
            interview_id: Interview UUID
            parent_question_id: Parent question UUID (main question)
            latest_answer: Most recent answer (could be to follow-up)

        Returns:
            Decision dict with keys:
                - needs_followup: bool - Whether follow-up should be generated
                - reason: str - Explanation of decision
                - follow_up_count: int - Current count of follow-ups
                - cumulative_gaps: list[str] - All gaps across follow-up cycle
        """
        logger.info(
            f"Evaluating follow-up decision for question {parent_question_id}"
        )

        # Step 1: Count existing follow-ups for parent question
        follow_ups = await self.follow_up_repo.get_by_parent_question_id(
            parent_question_id
        )
        follow_up_count = len(follow_ups)
        logger.info(f"Found {follow_up_count} existing follow-ups")

        # Step 2: Check break condition - Max follow-ups reached
        # TODO: use domain instead
        if follow_up_count >= 3:
            logger.info("Break condition: Max follow-ups (3) reached")
            return {
                "needs_followup": False,
                "reason": "Max follow-ups (3) reached",
                "follow_up_count": follow_up_count,
                "cumulative_gaps": [],
            }

        # Step 3: Check break condition - Answer meets completion criteria
        if latest_answer.is_adaptive_complete():
            reason = "Answer meets completion criteria"
            if latest_answer.similarity_score and latest_answer.similarity_score >= 0.8:
                reason = f"Similarity score {latest_answer.similarity_score:.2f} >= 0.8"
            elif not latest_answer.has_gaps():
                reason = "No concept gaps detected"

            logger.info(f"Break condition: {reason}")
            return {
                "needs_followup": False,
                "reason": reason,
                "follow_up_count": follow_up_count,
                "cumulative_gaps": [],
            }

        # Step 4: Accumulate gaps from all follow-up answers
        cumulative_gaps = await self._accumulate_gaps(follow_ups, latest_answer)
        logger.info(f"Accumulated {len(cumulative_gaps)} unique concept gaps")

        # Step 5: Check if gaps exist - If no gaps, no follow-up needed
        if len(cumulative_gaps) == 0:
            logger.info("Break condition: No cumulative gaps detected")
            return {
                "needs_followup": False,
                "reason": "No cumulative gaps detected",
                "follow_up_count": follow_up_count,
                "cumulative_gaps": [],
            }

        # Step 6: Follow-up needed
        logger.info(f"Follow-up needed: {len(cumulative_gaps)} gaps to address")
        return {
            "needs_followup": True,
            "reason": f"Detected {len(cumulative_gaps)} missing concepts",
            "follow_up_count": follow_up_count,
            "cumulative_gaps": cumulative_gaps,
        }

    async def _accumulate_gaps(
        self,
        follow_ups: list[Any],
        latest_answer: Answer,
    ) -> list[str]:
        """Accumulate concept gaps from all follow-up answers.

        Args:
            follow_ups: List of follow-up questions for parent
            latest_answer: Most recent answer

        Returns:
            List of unique concept gaps across all answers
        """
        all_gaps = set()

        # Add gaps from latest answer
        if latest_answer.gaps and latest_answer.gaps.get("confirmed"):
            concepts = latest_answer.gaps.get("concepts", [])
            all_gaps.update(concepts)
            logger.debug(f"Latest answer contributes {len(concepts)} gaps")

        # Add gaps from previous follow-up answers
        for follow_up in follow_ups:
            # Fetch answer for this follow-up question
            prev_answer = await self.answer_repo.get_by_question_id(follow_up.id)
            if prev_answer and prev_answer.gaps and prev_answer.gaps.get("confirmed"):
                concepts = prev_answer.gaps.get("concepts", [])
                all_gaps.update(concepts)
                logger.debug(f"Follow-up #{follow_up.order_in_sequence} contributes {len(concepts)} gaps")

        return list(all_gaps)
</file>

<file path="src/domain/models/candidate.py">
"""Candidate domain model."""

from datetime import datetime
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class Candidate(BaseModel):
    """Represents a candidate participating in an interview.

    This is a rich domain model that encapsulates candidate-related business logic.
    """

    id: UUID = Field(default_factory=uuid4)
    name: str
    email: str
    cv_file_path: str | None = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""
        frozen = False  # Allow updates

    def update_cv(self, cv_file_path: str) -> None:
        """Update candidate's CV file path.

        Args:
            cv_file_path: Path to the CV file
        """
        self.cv_file_path = cv_file_path
        self.updated_at = datetime.utcnow()

    def has_cv(self) -> bool:
        """Check if candidate has uploaded a CV.

        Returns:
            True if CV exists, False otherwise
        """
        return self.cv_file_path is not None
</file>

<file path="src/domain/ports/analytics_port.py">
"""Analytics port interface."""

from abc import ABC, abstractmethod
from typing import Any
from uuid import UUID

from ..models.answer import Answer
from ..models.question import Question


class AnalyticsPort(ABC):
    """Interface for analytics and reporting operations.

    This port abstracts analytics storage and report generation.
    """

    @abstractmethod
    async def record_answer_evaluation(
        self,
        interview_id: UUID,
        answer: Answer,
    ) -> None:
        """Record answer evaluation for analytics.

        Args:
            interview_id: Interview identifier
            answer: Answer with evaluation data
        """
        pass

    @abstractmethod
    async def get_interview_statistics(
        self,
        interview_id: UUID,
    ) -> dict[str, Any]:
        """Get statistics for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            Dictionary with statistics (avg score, completion rate, etc.)
        """
        pass

    @abstractmethod
    async def get_candidate_performance_history(
        self,
        candidate_id: UUID,
    ) -> list[dict[str, Any]]:
        """Get candidate's performance across all interviews.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of interview performance data
        """
        pass

    @abstractmethod
    async def generate_improvement_recommendations(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[Answer],
    ) -> list[str]:
        """Generate improvement recommendations based on performance.

        Args:
            interview_id: Interview identifier
            questions: Questions asked
            answers: Answers with evaluations

        Returns:
            List of improvement recommendations
        """
        pass

    @abstractmethod
    async def calculate_skill_scores(
        self,
        answers: list[Answer],
        questions: list[Question],
    ) -> dict[str, float]:
        """Calculate scores per skill based on answers.

        Args:
            answers: List of evaluated answers
            questions: Corresponding questions

        Returns:
            Dictionary mapping skill names to scores
        """
        pass
</file>

<file path="src/domain/ports/answer_repository_port.py">
"""Answer repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.answer import Answer


class AnswerRepositoryPort(ABC):
    """Interface for answer persistence operations.

    This port abstracts database operations for answers,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, answer: Answer) -> Answer:
        """Save an answer.

        Args:
            answer: Answer to save

        Returns:
            Saved answer with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, answer_id: UUID) -> Answer | None:
        """Retrieve an answer by ID.

        Args:
            answer_id: Answer identifier

        Returns:
            Answer if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_ids(self, answer_ids: list[UUID]) -> list[Answer]:
        """Retrieve multiple answers by IDs.

        Args:
            answer_ids: List of answer identifiers

        Returns:
            List of answers found
        """
        pass

    @abstractmethod
    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        """Retrieve all answers for an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def get_by_question_id(self, question_id: UUID) -> list[Answer]:
        """Retrieve all answers for a question.

        Args:
            question_id: Question identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(self, candidate_id: UUID) -> list[Answer]:
        """Retrieve all answers by a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of answers
        """
        pass

    @abstractmethod
    async def update(self, answer: Answer) -> Answer:
        """Update an existing answer.

        Args:
            answer: Answer with updated data

        Returns:
            Updated answer
        """
        pass

    @abstractmethod
    async def delete(self, answer_id: UUID) -> bool:
        """Delete an answer.

        Args:
            answer_id: Answer identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/ports/candidate_repository_port.py">
"""Candidate repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.candidate import Candidate


class CandidateRepositoryPort(ABC):
    """Interface for candidate persistence operations.

    This port abstracts database operations for candidates,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, candidate: Candidate) -> Candidate:
        """Save a candidate.

        Args:
            candidate: Candidate to save

        Returns:
            Saved candidate with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, candidate_id: UUID) -> Candidate | None:
        """Retrieve a candidate by ID.

        Args:
            candidate_id: Candidate identifier

        Returns:
            Candidate if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_email(self, email: str) -> Candidate | None:
        """Retrieve a candidate by email.

        Args:
            email: Candidate email address

        Returns:
            Candidate if found, None otherwise
        """
        pass

    @abstractmethod
    async def update(self, candidate: Candidate) -> Candidate:
        """Update an existing candidate.

        Args:
            candidate: Candidate with updated data

        Returns:
            Updated candidate
        """
        pass

    @abstractmethod
    async def delete(self, candidate_id: UUID) -> bool:
        """Delete a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Candidate]:
        """List all candidates with pagination.

        Args:
            skip: Number of candidates to skip
            limit: Maximum number of results

        Returns:
            List of candidates
        """
        pass
</file>

<file path="src/domain/ports/cv_analysis_repository_port.py">
"""CV Analysis repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.cv_analysis import CVAnalysis


class CVAnalysisRepositoryPort(ABC):
    """Interface for CV analysis persistence operations.

    This port abstracts database operations for CV analyses,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Save a CV analysis.

        Args:
            cv_analysis: CV analysis to save

        Returns:
            Saved CV analysis with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        """Retrieve a CV analysis by ID.

        Args:
            cv_analysis_id: CV analysis identifier

        Returns:
            CV analysis if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(self, candidate_id: UUID) -> list[CVAnalysis]:
        """Retrieve all CV analyses for a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            List of CV analyses
        """
        pass

    @abstractmethod
    async def get_latest_by_candidate_id(
        self,
        candidate_id: UUID,
    ) -> CVAnalysis | None:
        """Retrieve the most recent CV analysis for a candidate.

        Args:
            candidate_id: Candidate identifier

        Returns:
            Latest CV analysis if found, None otherwise
        """
        pass

    @abstractmethod
    async def update(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        """Update an existing CV analysis.

        Args:
            cv_analysis: CV analysis with updated data

        Returns:
            Updated CV analysis
        """
        pass

    @abstractmethod
    async def delete(self, cv_analysis_id: UUID) -> bool:
        """Delete a CV analysis.

        Args:
            cv_analysis_id: CV analysis identifier

        Returns:
            True if deleted, False if not found
        """
        pass
</file>

<file path="src/domain/ports/interview_repository_port.py">
"""Interview repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.interview import Interview, InterviewStatus


class InterviewRepositoryPort(ABC):
    """Interface for interview persistence operations.

    This port abstracts database operations for interviews,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, interview: Interview) -> Interview:
        """Save an interview.

        Args:
            interview: Interview to save

        Returns:
            Saved interview with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        """Retrieve an interview by ID.

        Args:
            interview_id: Interview identifier

        Returns:
            Interview if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_candidate_id(
        self,
        candidate_id: UUID,
        status: InterviewStatus | None = None,
    ) -> list[Interview]:
        """Retrieve interviews for a candidate.

        Args:
            candidate_id: Candidate identifier
            status: Optional status filter

        Returns:
            List of interviews
        """
        pass

    @abstractmethod
    async def get_by_status(
        self,
        status: InterviewStatus,
        limit: int = 100,
    ) -> list[Interview]:
        """Retrieve interviews by status.

        Args:
            status: Interview status
            limit: Maximum number of results

        Returns:
            List of interviews
        """
        pass

    @abstractmethod
    async def update(self, interview: Interview) -> Interview:
        """Update an existing interview.

        Args:
            interview: Interview with updated data

        Returns:
            Updated interview
        """
        pass

    @abstractmethod
    async def delete(self, interview_id: UUID) -> bool:
        """Delete an interview.

        Args:
            interview_id: Interview identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Interview]:
        """List all interviews with pagination.

        Args:
            skip: Number of interviews to skip
            limit: Maximum number of results

        Returns:
            List of interviews
        """
        pass
</file>

<file path="src/domain/ports/question_repository_port.py">
"""Question repository port interface."""

from abc import ABC, abstractmethod
from uuid import UUID

from ..models.question import DifficultyLevel, Question, QuestionType


class QuestionRepositoryPort(ABC):
    """Interface for question persistence operations.

    This port abstracts database operations for questions,
    allowing easy switching between databases or storage mechanisms.
    """

    @abstractmethod
    async def save(self, question: Question) -> Question:
        """Save a question.

        Args:
            question: Question to save

        Returns:
            Saved question with updated metadata
        """
        pass

    @abstractmethod
    async def get_by_id(self, question_id: UUID) -> Question | None:
        """Retrieve a question by ID.

        Args:
            question_id: Question identifier

        Returns:
            Question if found, None otherwise
        """
        pass

    @abstractmethod
    async def get_by_ids(self, question_ids: list[UUID]) -> list[Question]:
        """Retrieve multiple questions by IDs.

        Args:
            question_ids: List of question identifiers

        Returns:
            List of questions found
        """
        pass

    @abstractmethod
    async def find_by_skill(
        self,
        skill: str,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by skill.

        Args:
            skill: Skill to filter by
            difficulty: Optional difficulty filter
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def find_by_type(
        self,
        question_type: QuestionType,
        difficulty: DifficultyLevel | None = None,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by type.

        Args:
            question_type: Type of questions to find
            difficulty: Optional difficulty filter
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def find_by_tags(
        self,
        tags: list[str],
        match_all: bool = False,
        limit: int = 10,
    ) -> list[Question]:
        """Find questions by tags.

        Args:
            tags: Tags to search for
            match_all: If True, match all tags; if False, match any tag
            limit: Maximum number of results

        Returns:
            List of matching questions
        """
        pass

    @abstractmethod
    async def update(self, question: Question) -> Question:
        """Update an existing question.

        Args:
            question: Question with updated data

        Returns:
            Updated question
        """
        pass

    @abstractmethod
    async def delete(self, question_id: UUID) -> bool:
        """Delete a question.

        Args:
            question_id: Question identifier

        Returns:
            True if deleted, False if not found
        """
        pass

    @abstractmethod
    async def list_all(
        self,
        skip: int = 0,
        limit: int = 100,
    ) -> list[Question]:
        """List all questions with pagination.

        Args:
            skip: Number of questions to skip
            limit: Maximum number of results

        Returns:
            List of questions
        """
        pass
</file>

<file path="src/infrastructure/database/__init__.py">
"""Database infrastructure package."""

from .base import Base
from .session import AsyncSessionLocal, close_db, get_async_session, get_engine, init_db

__all__ = [
    "get_async_session",
    "init_db",
    "close_db",
    "AsyncSessionLocal",
    "get_engine",
    "Base",
]
</file>

<file path="src/infrastructure/database/session.py">
"""Database session management with async support."""

from collections.abc import AsyncGenerator

from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)
from sqlalchemy.pool import NullPool, QueuePool

from ..config.settings import get_settings

# Global engine instance
_async_engine: AsyncEngine | None = None
AsyncSessionLocal: async_sessionmaker[AsyncSession] | None = None


def create_engine() -> AsyncEngine:
    """Create and configure the async database engine.

    Returns:
        Configured async SQLAlchemy engine

    Notes:
        - Uses connection pooling in production for better performance
        - Disables pooling in testing to avoid connection leaks
        - Enables echo in development for SQL debugging
    """
    settings = get_settings()

    # Configure pool based on environment
    is_prod = settings.is_production()
    poolclass = QueuePool if is_prod else NullPool

    # Base engine configuration
    engine_config = {
        "url": settings.async_database_url,
        "echo": settings.debug,  # Log SQL in debug mode
        "poolclass": poolclass,
    }

    # Add pool-specific parameters only when using QueuePool
    if is_prod:
        engine_config.update({
            "pool_size": 10,
            "max_overflow": 20,
            "pool_pre_ping": True,  # Verify connections before using
            "pool_recycle": 3600,  # Recycle connections after 1 hour
        })

    engine = create_async_engine(**engine_config)

    return engine


async def init_db() -> None:
    """Initialize database engine and session factory.

    This should be called once during application startup.
    Creates the async engine and configures the session factory.
    """
    global _async_engine, AsyncSessionLocal

    if _async_engine is None:
        _async_engine = create_engine()
        AsyncSessionLocal = async_sessionmaker(
            bind=_async_engine,
            class_=AsyncSession,
            expire_on_commit=False,  # Don't expire objects after commit
            autocommit=False,
            autoflush=False,
        )


async def close_db() -> None:
    """Close database connections and cleanup resources.

    This should be called during application shutdown.
    Disposes the engine and closes all connections.
    """
    global _async_engine, AsyncSessionLocal

    if _async_engine is not None:
        await _async_engine.dispose()
        _async_engine = None
        AsyncSessionLocal = None


async def get_async_session() -> AsyncGenerator[AsyncSession, None]:
    """Get an async database session.

    This is a dependency injection function for FastAPI.
    It yields a session and ensures proper cleanup.

    Yields:
        AsyncSession: Database session

    Example:
        ```python
        @app.get("/users")
        async def get_users(db: AsyncSession = Depends(get_async_session)):
            result = await db.execute(select(User))
            return result.scalars().all()
        ```
    """
    if AsyncSessionLocal is None:
        raise RuntimeError(
            "Database not initialized. Call init_db() during application startup."
        )

    async with AsyncSessionLocal() as session:
        try:
            yield session
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()


def get_engine() -> AsyncEngine:
    """Get the current async database engine.

    Returns:
        AsyncEngine: The global async engine instance

    Raises:
        RuntimeError: If database not initialized
    """
    if _async_engine is None:
        raise RuntimeError(
            "Database not initialized. Call init_db() during application startup."
        )
    return _async_engine
</file>

<file path="tests/integration/api/test_planning_endpoints.py">
"""Integration tests for Phase 05: Planning API endpoints."""

from uuid import uuid4

import pytest
from fastapi.testclient import TestClient

# Note: These are integration test templates
# Actual implementation would require FastAPI app fixture


class TestPlanningEndpoints:
    """Test REST API planning endpoints."""

    @pytest.mark.integration
    def test_plan_interview_endpoint_success(self):
        """Test POST /interviews/plan success case."""
        # Template for integration test
        # Requires: FastAPI app, test database, mocked LLM

        # Expected request
        request_data = {
            "cv_analysis_id": str(uuid4()),
            "candidate_id": str(uuid4()),
        }

        # Expected response (202 Accepted)
        expected_status = 202
        expected_fields = [
            "interview_id",
            "status",
            "planned_question_count",
            "plan_metadata",
            "message",
        ]

        # Test would verify:
        # 1. POST /interviews/plan returns 202
        # 2. Response contains all expected fields
        # 3. status = "READY"
        # 4. planned_question_count matches skill diversity
        # 5. plan_metadata contains strategy, n, generated_at

        # Example assertion structure:
        # response = client.post("/interviews/plan", json=request_data)
        # assert response.status_code == expected_status
        # data = response.json()
        # for field in expected_fields:
        #     assert field in data
        # assert data["status"] == "READY"

        pass  # Template only

    @pytest.mark.integration
    def test_plan_interview_cv_not_found(self):
        """Test POST /interviews/plan with non-existent CV."""
        # Template: Verify 404 when CV analysis not found

        request_data = {
            "cv_analysis_id": str(uuid4()),  # Non-existent
            "candidate_id": str(uuid4()),
        }

        # Expected: 404 Not Found
        # Error detail: "CV analysis {id} not found"

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_ready(self):
        """Test GET /interviews/{id}/plan when READY."""
        # Template: Verify planning status check

        # Steps:
        # 1. Create planned interview
        # 2. GET /interviews/{id}/plan
        # 3. Verify status="READY", message contains question count

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_preparing(self):
        """Test GET /interviews/{id}/plan when PREPARING."""
        # Template: Verify status during planning

        # Expected message: "Interview planning in progress..."

        pass  # Template only

    @pytest.mark.integration
    def test_get_planning_status_interview_not_found(self):
        """Test GET /interviews/{id}/plan with non-existent interview."""
        # Template: Verify 404 error

        # Expected: 404 Not Found

        pass  # Template only


class TestAdaptiveInterviewFlow:
    """Test complete adaptive interview flow via API."""

    @pytest.mark.integration
    def test_complete_adaptive_flow(self):
        """Test end-to-end adaptive interview flow."""
        # Template for full flow test

        # Steps:
        # 1. Create CV analysis
        # 2. POST /interviews/plan
        # 3. Verify READY status
        # 4. PUT /interviews/{id}/start
        # 5. Connect WebSocket
        # 6. Receive first question
        # 7. Submit low-similarity answer
        # 8. Receive evaluation with similarity_score
        # 9. Receive follow-up question
        # 10. Submit follow-up answer
        # 11. Receive next main question
        # 12. Complete all questions
        # 13. Verify interview COMPLETED

        pass  # Template only

    @pytest.mark.integration
    def test_adaptive_vs_legacy_mode(self):
        """Test adaptive mode is triggered by plan_metadata."""
        # Template: Verify mode detection

        # Test A: Interview with plan_metadata -> adaptive mode
        # - Receives similarity_score and gaps in evaluation
        # - May receive follow-up questions

        # Test B: Interview without plan_metadata -> legacy mode
        # - No similarity_score or gaps
        # - No follow-up questions

        pass  # Template only


class TestFollowUpQuestionDelivery:
    """Test follow-up question delivery via WebSocket."""

    @pytest.mark.integration
    def test_followup_message_structure(self):
        """Test follow-up WebSocket message structure."""
        # Template: Verify message format

        expected_message_fields = {
            "type": "follow_up_question",
            "question_id": "uuid",
            "parent_question_id": "uuid",
            "text": "string",
            "generated_reason": "string",
            "order_in_sequence": "int",
            "audio_data": "base64_string",
        }

        # Test would verify all fields present and correct types

        pass  # Template only

    @pytest.mark.integration
    def test_max_3_followups_enforced(self):
        """Test max 3 follow-ups per question enforced."""
        # Template: Verify limit

        # Steps:
        # 1. Submit answer -> follow-up 1
        # 2. Submit answer -> follow-up 2
        # 3. Submit answer -> follow-up 3
        # 4. Submit answer -> next main question (no 4th follow-up)

        pass  # Template only

    @pytest.mark.integration
    def test_followup_stops_at_high_similarity(self):
        """Test no follow-up when answer meets threshold."""
        # Template: Verify threshold logic

        # Submit answer with high similarity (>= 80%)
        # -> No follow-up
        # -> Next main question

        pass  # Template only


class TestEvaluationEnhancement:
    """Test evaluation response enhancements for adaptive mode."""

    @pytest.mark.integration
    def test_evaluation_includes_similarity_score(self):
        """Test evaluation message includes similarity_score."""
        # Template: Verify adaptive evaluation fields

        expected_fields = [
            "type",  # "evaluation"
            "answer_id",
            "score",
            "feedback",
            "strengths",
            "weaknesses",
            "similarity_score",  # NEW for adaptive
            "gaps",  # NEW for adaptive
        ]

        pass  # Template only

    @pytest.mark.integration
    def test_gaps_structure(self):
        """Test gaps field structure in evaluation."""
        # Template: Verify gaps format

        expected_gaps_structure = {
            "concepts": ["list", "of", "missing"],
            "keywords": ["list", "of", "keywords"],
            "confirmed": True,  # or False
            "severity": "minor",  # or "moderate", "major"
        }

        pass  # Template only


class TestEndpointIntegration:
    """Test API endpoint integration with adaptive interviews."""

    @pytest.mark.integration
    def test_existing_endpoints_work_with_adaptive(self):
        """Test existing endpoints work with adaptive interviews."""
        # Template: Verify endpoints work with adaptive flow

        endpoints_to_test = [
            "POST /interviews/plan",  # Plan interview
            "GET /interviews/{id}",  # Get interview
            "PUT /interviews/{id}/start",  # Start interview
            "GET /interviews/{id}/questions/current",  # Get question
        ]

        # Verify all work with adaptive request/response structure

        pass  # Template only


# Test execution notes:
# These are integration test templates that demonstrate what should be tested.
# Actual implementation requires:
# 1. FastAPI test client fixture
# 2. Test database setup/teardown
# 3. Mocked external services (LLM, Vector DB, TTS)
# 4. WebSocket test client
# 5. Async test support (pytest-asyncio)
#
# Run integration tests with:
# pytest tests/integration -m integration --asyncio-mode=auto
</file>

<file path="tests/unit/adapters/api/websocket/test_session_orchestrator.py">
"""Unit tests for InterviewSessionOrchestrator (Phase 5)."""

import base64
import pytest
from datetime import datetime
from unittest.mock import AsyncMock, MagicMock, patch
from uuid import UUID, uuid4

from src.adapters.api.websocket.session_orchestrator import (
    InterviewSessionOrchestrator,
    SessionState,
)
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


# ==================== Fixtures ====================


@pytest.fixture
def interview_id():
    """Sample interview ID."""
    return uuid4()


@pytest.fixture
def question_id():
    """Sample question ID."""
    return uuid4()


@pytest.fixture
def mock_websocket():
    """Mock WebSocket connection."""
    ws = MagicMock()
    ws.accept = AsyncMock()
    ws.send_json = AsyncMock()
    ws.receive_json = AsyncMock()
    return ws


@pytest.fixture
def mock_container():
    """Mock dependency injection container."""
    container = MagicMock()

    # Mock repositories
    container.interview_repository_port = MagicMock()
    container.question_repository_port = MagicMock()
    container.answer_repository_port = MagicMock()
    container.follow_up_question_repository = MagicMock()

    # Mock services
    container.llm_port = MagicMock()
    container.vector_search_port = MagicMock()
    container.text_to_speech_port = MagicMock()

    return container


@pytest.fixture
def sample_question():
    """Sample question entity."""
    return Question(
        text="Explain recursion in programming",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Python", "Algorithms"],
        ideal_answer="Recursion is when a function calls itself...",
    )


@pytest.fixture
def sample_interview(interview_id, question_id):
    """Sample interview entity."""
    interview = Interview(
        candidate_id=uuid4(),
        status=InterviewStatus.QUESTIONING,
        cv_analysis_id=uuid4(),
    )
    interview.id = interview_id
    interview.question_ids = [question_id, uuid4(), uuid4()]
    interview.current_question_index = 0
    return interview


@pytest.fixture
def sample_answer(interview_id, question_id):
    """Sample answer with evaluation."""
    answer = Answer(
        interview_id=interview_id,
        question_id=question_id,
        candidate_id=uuid4(),
        text="Recursion is when a function calls itself to solve problems.",
        is_voice=False,
        similarity_score=0.75,
        gaps={
            "concepts": ["base case", "call stack"],
            "keywords": ["base", "stack"],
            "confirmed": True,
            "severity": "moderate",
        },
    )
    answer.evaluate(
        AnswerEvaluation(
            score=75.0,
            semantic_similarity=0.75,
            completeness=0.7,
            relevance=0.9,
            sentiment="uncertain",
            reasoning="Answer needs more detail on base case and call stack",
            strengths=["Correct definition"],
            weaknesses=["Missing base case", "No call stack explanation"],
            improvement_suggestions=["Explain base case", "Describe call stack"],
        )
    )
    return answer


@pytest.fixture
def orchestrator(interview_id, mock_websocket, mock_container):
    """Create orchestrator instance."""
    return InterviewSessionOrchestrator(
        interview_id=interview_id,
        websocket=mock_websocket,
        container=mock_container,
    )


# ==================== State Transition Tests ====================


class TestStateTransitions:
    """Test state machine transition logic."""

    def test_initial_state_is_idle(self, orchestrator):
        """Test orchestrator starts in IDLE state."""
        assert orchestrator.state == SessionState.IDLE

    def test_valid_transition_idle_to_questioning(self, orchestrator):
        """Test valid transition from IDLE to QUESTIONING."""
        orchestrator._transition(SessionState.QUESTIONING)
        assert orchestrator.state == SessionState.QUESTIONING

    def test_invalid_transition_idle_to_evaluating(self, orchestrator):
        """Test invalid transition from IDLE to EVALUATING raises ValueError."""
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.EVALUATING)

    def test_invalid_transition_idle_to_follow_up(self, orchestrator):
        """Test invalid transition from IDLE to FOLLOW_UP raises ValueError."""
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.FOLLOW_UP)

    def test_invalid_transition_idle_to_complete(self, orchestrator):
        """Test invalid transition from IDLE to COMPLETE raises ValueError."""
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.COMPLETE)

    def test_valid_transition_questioning_to_evaluating(self, orchestrator):
        """Test valid transition from QUESTIONING to EVALUATING."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        assert orchestrator.state == SessionState.EVALUATING

    def test_invalid_transition_questioning_to_follow_up(self, orchestrator):
        """Test invalid direct transition from QUESTIONING to FOLLOW_UP."""
        orchestrator._transition(SessionState.QUESTIONING)
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.FOLLOW_UP)

    def test_valid_transition_evaluating_to_follow_up(self, orchestrator):
        """Test valid transition from EVALUATING to FOLLOW_UP."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.FOLLOW_UP)
        assert orchestrator.state == SessionState.FOLLOW_UP

    def test_valid_transition_evaluating_to_questioning(self, orchestrator):
        """Test valid transition from EVALUATING to QUESTIONING (next question)."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.QUESTIONING)
        assert orchestrator.state == SessionState.QUESTIONING

    def test_valid_transition_evaluating_to_complete(self, orchestrator):
        """Test valid transition from EVALUATING to COMPLETE."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.COMPLETE)
        assert orchestrator.state == SessionState.COMPLETE

    def test_valid_transition_follow_up_to_evaluating(self, orchestrator):
        """Test valid transition from FOLLOW_UP to EVALUATING."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.FOLLOW_UP)
        orchestrator._transition(SessionState.EVALUATING)
        assert orchestrator.state == SessionState.EVALUATING

    def test_invalid_transition_follow_up_to_questioning(self, orchestrator):
        """Test invalid direct transition from FOLLOW_UP to QUESTIONING."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.FOLLOW_UP)
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.QUESTIONING)

    def test_terminal_state_complete_rejects_all_transitions(self, orchestrator):
        """Test COMPLETE state rejects all transitions (terminal state)."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.COMPLETE)

        # Try all possible transitions from COMPLETE
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.IDLE)

        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.QUESTIONING)

        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.EVALUATING)

        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.FOLLOW_UP)

    def test_transition_updates_last_activity(self, orchestrator):
        """Test transition updates last_activity timestamp."""
        old_time = orchestrator.last_activity
        import time
        time.sleep(0.01)  # Small delay
        orchestrator._transition(SessionState.QUESTIONING)
        assert orchestrator.last_activity > old_time

    def test_error_message_shows_allowed_transitions(self, orchestrator):
        """Test error message includes allowed transitions for clarity."""
        with pytest.raises(ValueError) as exc_info:
            orchestrator._transition(SessionState.EVALUATING)

        error_msg = str(exc_info.value)
        assert "IDLE" in error_msg
        assert "EVALUATING" in error_msg
        assert "Allowed transitions" in error_msg


# ==================== Session Lifecycle Tests ====================


class TestSessionLifecycle:
    """Test session lifecycle methods."""

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_start_session_sends_first_question(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        sample_interview,
        mock_container,
    ):
        """Test start_session transitions to QUESTIONING and sends first question."""
        # Setup mocks
        mock_session = AsyncMock()
        mock_get_session.return_value.__aenter__ = AsyncMock(return_value=mock_session)
        mock_get_session.return_value.__aexit__ = AsyncMock()

        # Mock async generators to return properly
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_question_repo.get_by_id = AsyncMock(return_value=sample_question)
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_tts = AsyncMock()
        mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio")
        mock_container.text_to_speech_port.return_value = mock_tts

        # Mock GetNextQuestionUseCase
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=sample_question)
            mock_use_case.return_value = mock_instance

            # Mock connection manager
            with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                mock_manager.send_message = AsyncMock()

                # Execute
                await orchestrator.start_session()

                # Verify state transition
                assert orchestrator.state == SessionState.QUESTIONING

                # Verify current_question_id set
                assert orchestrator.current_question_id == sample_question.id
                assert orchestrator.parent_question_id == sample_question.id

                # Verify message sent
                mock_manager.send_message.assert_called_once()
                call_args = mock_manager.send_message.call_args[0]
                message = call_args[1]
                assert message["type"] == "question"
                assert message["question_id"] == str(sample_question.id)
                assert message["text"] == sample_question.text
                assert "audio_data" in message

    @pytest.mark.asyncio
    async def test_start_session_already_started_raises_error(self, orchestrator):
        """Test start_session raises ValueError if session already started."""
        orchestrator._transition(SessionState.QUESTIONING)

        with pytest.raises(ValueError, match="Session already started"):
            await orchestrator.start_session()

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_start_session_no_questions_available(
        self,
        mock_get_session,
        orchestrator,
        sample_interview,
        mock_container,
    ):
        """Test start_session handles no questions available (raises ValueError).

        FIXED: Now validates questions exist BEFORE transitioning to QUESTIONING state.
        This prevents invalid state transitions and keeps state machine clean.
        """
        # Setup mocks
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_tts = AsyncMock()
        mock_container.text_to_speech_port.return_value = mock_tts

        # Mock GetNextQuestionUseCase to return None
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=None)
            mock_use_case.return_value = mock_instance

            # Mock connection manager
            with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                mock_manager.send_message = AsyncMock()

                # Execute - should raise ValueError with clear message
                with pytest.raises(ValueError, match="No questions available"):
                    await orchestrator.start_session()

                # State should remain IDLE (never transitioned)
                assert orchestrator.state == SessionState.IDLE

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_start_session_interview_not_found(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        mock_container,
    ):
        """Test start_session handles interview not found (raises ValueError).

        FIXED: Now validates interview exists BEFORE transitioning to QUESTIONING state.
        """
        # Setup mocks
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=None)  # Not found
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_tts = AsyncMock()
        mock_container.text_to_speech_port.return_value = mock_tts

        # Mock GetNextQuestionUseCase
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=sample_question)
            mock_use_case.return_value = mock_instance

            # Mock connection manager
            with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                mock_manager.send_message = AsyncMock()

                # Execute - should raise ValueError with clear message
                with pytest.raises(ValueError, match="Interview.*not found"):
                    await orchestrator.start_session()

                # State should remain IDLE (never transitioned)
                assert orchestrator.state == SessionState.IDLE

    @pytest.mark.asyncio
    async def test_handle_answer_in_questioning_state(self, orchestrator):
        """Test handle_answer calls _handle_main_question_answer in QUESTIONING state."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator.current_question_id = uuid4()

        with patch.object(orchestrator, "_handle_main_question_answer", new=AsyncMock()) as mock_handler:
            await orchestrator.handle_answer("Test answer")
            mock_handler.assert_called_once_with("Test answer")

    @pytest.mark.asyncio
    async def test_handle_answer_in_follow_up_state(self, orchestrator):
        """Test handle_answer calls _handle_followup_answer in FOLLOW_UP state."""
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.FOLLOW_UP)
        orchestrator.current_question_id = uuid4()

        with patch.object(orchestrator, "_handle_followup_answer", new=AsyncMock()) as mock_handler:
            await orchestrator.handle_answer("Test answer")
            mock_handler.assert_called_once_with("Test answer")

    @pytest.mark.asyncio
    async def test_handle_answer_in_invalid_state_raises_error(self, orchestrator):
        """Test handle_answer raises ValueError in invalid states."""
        # IDLE state
        with pytest.raises(ValueError, match="Cannot handle answer"):
            await orchestrator.handle_answer("Test answer")

        # EVALUATING state
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator._transition(SessionState.EVALUATING)
        with pytest.raises(ValueError, match="Cannot handle answer"):
            await orchestrator.handle_answer("Test answer")

        # COMPLETE state
        orchestrator._transition(SessionState.COMPLETE)
        with pytest.raises(ValueError, match="Cannot handle answer"):
            await orchestrator.handle_answer("Test answer")


# ==================== Follow-up Logic Tests ====================


class TestFollowUpLogic:
    """Test follow-up question generation and tracking."""

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_follow_up_generated_when_gaps_detected(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        sample_interview,
        sample_answer,
        mock_container,
    ):
        """Test follow-up question generated when gaps detected."""
        # Setup
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator.current_question_id = sample_question.id
        orchestrator.parent_question_id = sample_question.id

        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        # Mock repositories
        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_interview_repo.update = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_question_repo.get_by_id = AsyncMock(return_value=sample_question)
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_answer_repo = AsyncMock()
        mock_container.answer_repository_port.return_value = mock_answer_repo

        mock_follow_up_repo = AsyncMock()
        mock_follow_up_repo.save = AsyncMock(side_effect=lambda x: x)
        mock_container.follow_up_question_repository.return_value = mock_follow_up_repo

        # Mock LLM
        mock_llm = AsyncMock()
        mock_llm.generate_followup_question = AsyncMock(
            return_value="Can you explain what a base case is?"
        )
        mock_container.llm_port.return_value = mock_llm

        # Mock vector search
        mock_vector_search = AsyncMock()
        mock_container.vector_search_port.return_value = mock_vector_search

        # Mock TTS
        mock_tts = AsyncMock()
        mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio")
        mock_container.text_to_speech_port.return_value = mock_tts

        # Mock ProcessAnswerAdaptiveUseCase to return answer with gaps
        with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=(sample_answer, True))
            mock_use_case.return_value = mock_instance

            # Mock FollowUpDecisionUseCase to say follow-up needed
            with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision_use_case:
                mock_decision_instance = AsyncMock()
                mock_decision_instance.execute = AsyncMock(return_value={
                    "needs_followup": True,
                    "reason": "2 missing concepts: base case, call stack",
                    "follow_up_count": 0,
                    "cumulative_gaps": ["base case", "call stack"],
                })
                mock_decision_use_case.return_value = mock_decision_instance

                # Mock connection manager
                with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                    mock_manager.send_message = AsyncMock()

                    # Execute
                    await orchestrator.handle_answer("Brief answer")

                    # Verify state transitioned to FOLLOW_UP
                    assert orchestrator.state == SessionState.FOLLOW_UP

                    # Verify follow-up count incremented
                    assert orchestrator.follow_up_count == 1

                    # Verify follow-up question saved
                    mock_follow_up_repo.save.assert_called_once()

                    # Verify follow-up message sent
                    follow_up_calls = [c for c in mock_manager.send_message.call_args_list
                                      if c[0][1].get("type") == "follow_up_question"]
                    assert len(follow_up_calls) == 1
                    follow_up_msg = follow_up_calls[0][0][1]
                    assert follow_up_msg["parent_question_id"] == str(sample_question.id)
                    assert follow_up_msg["order_in_sequence"] == 1

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_max_3_follow_ups_enforced_by_decision_use_case(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        sample_interview,
        sample_answer,
        mock_container,
    ):
        """Test that FollowUpDecisionUseCase enforces max 3 follow-ups."""
        # Setup - already at 3 follow-ups
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator.current_question_id = sample_question.id
        orchestrator.parent_question_id = sample_question.id
        orchestrator.follow_up_count = 3  # Already at max

        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        # Mock repositories
        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_question_repo.get_by_id = AsyncMock(return_value=sample_question)
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_answer_repo = AsyncMock()
        mock_container.answer_repository_port.return_value = mock_answer_repo

        mock_follow_up_repo = AsyncMock()
        mock_container.follow_up_question_repository.return_value = mock_follow_up_repo

        mock_llm = AsyncMock()
        mock_container.llm_port.return_value = mock_llm

        mock_vector_search = AsyncMock()
        mock_container.vector_search_port.return_value = mock_vector_search

        # Mock ProcessAnswerAdaptiveUseCase
        with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=(sample_answer, True))
            mock_use_case.return_value = mock_instance

            # Mock FollowUpDecisionUseCase to say NO follow-up (max reached)
            with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision_use_case:
                mock_decision_instance = AsyncMock()
                mock_decision_instance.execute = AsyncMock(return_value={
                    "needs_followup": False,
                    "reason": "Max follow-ups (3) reached",
                    "follow_up_count": 3,
                    "cumulative_gaps": ["base case", "call stack"],
                })
                mock_decision_use_case.return_value = mock_decision_instance

                # Mock GetNextQuestionUseCase for next main question
                with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
                    next_question = Question(
                        text="Next question",
                        question_type=QuestionType.TECHNICAL,
                        difficulty=DifficultyLevel.MEDIUM,
                        skills=["Python"],
                    )
                    mock_next_instance = AsyncMock()
                    mock_next_instance.execute = AsyncMock(return_value=next_question)
                    mock_next_q.return_value = mock_next_instance

                    # Mock TTS
                    mock_tts = AsyncMock()
                    mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio")
                    mock_container.text_to_speech_port.return_value = mock_tts

                    # Mock connection manager
                    with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                        mock_manager.send_message = AsyncMock()

                        # Execute
                        await orchestrator.handle_answer("Another brief answer")

                        # Verify no follow-up generated (should move to next main question)
                        assert orchestrator.state == SessionState.QUESTIONING

                        # Verify follow_up_count reset to 0 for next question
                        assert orchestrator.follow_up_count == 0

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_follow_up_count_tracking(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        sample_interview,
        sample_answer,
        mock_container,
    ):
        """Test follow-up count increments correctly."""
        # Setup
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator.current_question_id = sample_question.id
        orchestrator.parent_question_id = sample_question.id

        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        # Need to return new generator each time
        mock_get_session.return_value = async_gen()

        # Mock all dependencies
        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_interview_repo.update = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_question_repo.get_by_id = AsyncMock(return_value=sample_question)
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_answer_repo = AsyncMock()
        mock_container.answer_repository_port.return_value = mock_answer_repo

        mock_follow_up_repo = AsyncMock()
        mock_follow_up_repo.save = AsyncMock(side_effect=lambda x: x)
        mock_container.follow_up_question_repository.return_value = mock_follow_up_repo

        mock_llm = AsyncMock()
        mock_llm.generate_followup_question = AsyncMock(return_value="Follow-up question")
        mock_container.llm_port.return_value = mock_llm

        mock_vector_search = AsyncMock()
        mock_container.vector_search_port.return_value = mock_vector_search

        mock_tts = AsyncMock()
        mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio")
        mock_container.text_to_speech_port.return_value = mock_tts

        # Mock use cases
        with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=(sample_answer, True))
            mock_use_case.return_value = mock_instance

            with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                # First follow-up
                mock_decision_instance = AsyncMock()
                mock_decision_instance.execute = AsyncMock(return_value={
                    "needs_followup": True,
                    "reason": "Gaps detected",
                    "follow_up_count": 0,
                    "cumulative_gaps": ["concept1"],
                })
                mock_decision.return_value = mock_decision_instance

                with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                    mock_manager.send_message = AsyncMock()

                    # Reset mock_get_session for first call
                    mock_get_session.side_effect = [async_gen(), async_gen()]

                    # Generate first follow-up
                    await orchestrator.handle_answer("Answer 1")
                    assert orchestrator.follow_up_count == 1
                    assert orchestrator.state == SessionState.FOLLOW_UP

                    # Update decision for second follow-up
                    mock_decision_instance.execute.return_value = {
                        "needs_followup": True,
                        "reason": "More gaps",
                        "follow_up_count": 1,
                        "cumulative_gaps": ["concept1", "concept2"],
                    }

                    # Answer the follow-up question, which should generate another follow-up
                    await orchestrator.handle_answer("Answer 2")
                    assert orchestrator.follow_up_count == 2

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_parent_question_id_tracking(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        sample_interview,
        sample_answer,
        mock_container,
    ):
        """Test parent_question_id remains consistent across follow-ups."""
        # Setup
        orchestrator._transition(SessionState.QUESTIONING)
        orchestrator.current_question_id = sample_question.id
        orchestrator.parent_question_id = sample_question.id
        original_parent_id = sample_question.id

        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        # Mock dependencies
        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_interview_repo.update = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_question_repo.get_by_id = AsyncMock(return_value=sample_question)
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_answer_repo = AsyncMock()
        mock_container.answer_repository_port.return_value = mock_answer_repo

        mock_follow_up_repo = AsyncMock()
        mock_follow_up_repo.save = AsyncMock(side_effect=lambda x: x)
        mock_container.follow_up_question_repository.return_value = mock_follow_up_repo

        mock_llm = AsyncMock()
        mock_llm.generate_followup_question = AsyncMock(return_value="Follow-up question")
        mock_container.llm_port.return_value = mock_llm

        mock_vector_search = AsyncMock()
        mock_container.vector_search_port.return_value = mock_vector_search

        mock_tts = AsyncMock()
        mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio")
        mock_container.text_to_speech_port.return_value = mock_tts

        with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=(sample_answer, True))
            mock_use_case.return_value = mock_instance

            with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                mock_decision_instance = AsyncMock()
                mock_decision_instance.execute = AsyncMock(return_value={
                    "needs_followup": True,
                    "reason": "Gaps",
                    "follow_up_count": 0,
                    "cumulative_gaps": ["concept1"],
                })
                mock_decision.return_value = mock_decision_instance

                with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                    mock_manager.send_message = AsyncMock()

                    # Generate follow-up
                    await orchestrator.handle_answer("Answer")

                    # Verify parent_question_id unchanged
                    assert orchestrator.parent_question_id == original_parent_id

                    # Verify current_question_id changed to follow-up ID
                    assert orchestrator.current_question_id != original_parent_id


# ==================== Progress Tracking Tests ====================


class TestProgressTracking:
    """Test progress tracking functionality."""

    def test_current_question_id_updates_correctly(self, orchestrator):
        """Test current_question_id tracks current question."""
        assert orchestrator.current_question_id is None

        q_id = uuid4()
        orchestrator.current_question_id = q_id
        assert orchestrator.current_question_id == q_id

    def test_follow_up_count_increments(self, orchestrator):
        """Test follow_up_count increments."""
        assert orchestrator.follow_up_count == 0

        orchestrator.follow_up_count += 1
        assert orchestrator.follow_up_count == 1

        orchestrator.follow_up_count += 1
        assert orchestrator.follow_up_count == 2

    def test_follow_up_count_resets_on_next_main_question(self, orchestrator):
        """Test follow_up_count resets when moving to next main question."""
        orchestrator.follow_up_count = 3
        orchestrator.follow_up_count = 0  # Reset
        assert orchestrator.follow_up_count == 0

    def test_get_state_returns_complete_session_data(self, orchestrator, interview_id):
        """Test get_state returns all session data."""
        # Setup state
        orchestrator._transition(SessionState.QUESTIONING)
        q_id = uuid4()
        orchestrator.current_question_id = q_id
        orchestrator.parent_question_id = q_id
        orchestrator.follow_up_count = 2

        # Get state
        state = orchestrator.get_state()

        # Verify all fields present
        assert state["interview_id"] == str(interview_id)
        assert state["state"] == SessionState.QUESTIONING
        assert state["current_question_id"] == str(q_id)
        assert state["parent_question_id"] == str(q_id)
        assert state["follow_up_count"] == 2
        assert "created_at" in state
        assert "last_activity" in state

    def test_get_state_handles_none_values(self, orchestrator, interview_id):
        """Test get_state handles None for question IDs."""
        state = orchestrator.get_state()

        assert state["interview_id"] == str(interview_id)
        assert state["state"] == SessionState.IDLE
        assert state["current_question_id"] is None
        assert state["parent_question_id"] is None
        assert state["follow_up_count"] == 0

    def test_created_at_set_on_init(self, orchestrator):
        """Test created_at timestamp set on initialization."""
        assert isinstance(orchestrator.created_at, datetime)
        assert orchestrator.created_at <= datetime.utcnow()

    def test_last_activity_updates_on_transition(self, orchestrator):
        """Test last_activity updates on state transitions."""
        old_activity = orchestrator.last_activity
        import time
        time.sleep(0.01)

        orchestrator._transition(SessionState.QUESTIONING)
        assert orchestrator.last_activity > old_activity


# ==================== Error Handling Tests ====================


class TestErrorHandling:
    """Test error handling scenarios."""

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_interview_not_found_during_start(
        self,
        mock_get_session,
        orchestrator,
        sample_question,
        mock_container,
    ):
        """Test handling when interview not found during start_session (raises ValueError).

        FIXED: Validates interview exists BEFORE transitioning to QUESTIONING.
        """
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=None)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_tts = AsyncMock()
        mock_container.text_to_speech_port.return_value = mock_tts

        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=sample_question)
            mock_use_case.return_value = mock_instance

            with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                mock_manager.send_message = AsyncMock()

                # Should raise ValueError with clear message
                with pytest.raises(ValueError, match="Interview.*not found"):
                    await orchestrator.start_session()

                # State should remain IDLE (never transitioned)
                assert orchestrator.state == SessionState.IDLE

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    async def test_no_questions_available_during_start(
        self,
        mock_get_session,
        orchestrator,
        sample_interview,
        mock_container,
    ):
        """Test handling when no questions available (raises ValueError).

        FIXED: Validates questions exist BEFORE transitioning to QUESTIONING.
        """
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_interview_repo = AsyncMock()
        mock_interview_repo.get_by_id = AsyncMock(return_value=sample_interview)
        mock_container.interview_repository_port.return_value = mock_interview_repo

        mock_question_repo = AsyncMock()
        mock_container.question_repository_port.return_value = mock_question_repo

        mock_tts = AsyncMock()
        mock_container.text_to_speech_port.return_value = mock_tts

        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_use_case:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=None)  # No questions
            mock_use_case.return_value = mock_instance

            with patch("src.adapters.api.websocket.connection_manager.manager") as mock_manager:
                mock_manager.send_message = AsyncMock()

                # Should raise ValueError with clear message
                with pytest.raises(ValueError, match="No questions available"):
                    await orchestrator.start_session()

                # State should remain IDLE (never transitioned)
                assert orchestrator.state == SessionState.IDLE

    def test_state_transition_validation_errors(self, orchestrator):
        """Test state transition validation raises clear errors."""
        # Invalid from IDLE
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.EVALUATING)

        # Invalid from QUESTIONING
        orchestrator._transition(SessionState.QUESTIONING)
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.FOLLOW_UP)

        # Terminal state COMPLETE
        orchestrator._transition(SessionState.EVALUATING)
        orchestrator._transition(SessionState.COMPLETE)
        with pytest.raises(ValueError, match="Invalid state transition"):
            orchestrator._transition(SessionState.IDLE)
</file>

<file path="tests/unit/domain/test_adaptive_models.py">
"""Tests for Phase 01: Domain model updates for adaptive interviews."""

from datetime import datetime
from uuid import uuid4

import pytest

from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


class TestQuestionAdaptiveFields:
    """Test Question model adaptive fields (ideal_answer, rationale)."""

    def test_question_with_ideal_answer(self):
        """Test question with ideal answer and rationale."""
        question = Question(
            text="Explain recursion",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python"],
            ideal_answer="Recursion is a function calling itself with a base case...",
            rationale="This answer demonstrates mastery by covering base case and examples",
        )

        assert question.has_ideal_answer() is True
        assert question.is_planned is True
        assert question.ideal_answer is not None
        assert question.rationale is not None

    def test_question_without_ideal_answer(self):
        """Test legacy question without ideal answer."""
        question = Question(
            text="Tell me about yourself",
            question_type=QuestionType.BEHAVIORAL,
            difficulty=DifficultyLevel.EASY,
            skills=["Communication"],
        )

        assert question.has_ideal_answer() is False
        assert question.is_planned is False
        assert question.ideal_answer is None
        assert question.rationale is None

    def test_has_ideal_answer_empty_string(self):
        """Test has_ideal_answer with empty or short string."""
        question = Question(
            text="Test",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.EASY,
            skills=["Test"],
            ideal_answer="   ",  # Only whitespace
        )

        assert question.has_ideal_answer() is False

        question.ideal_answer = "Short"  # < 10 chars
        assert question.has_ideal_answer() is False


class TestInterviewAdaptiveFields:
    """Test Interview model adaptive fields (plan_metadata, adaptive_follow_ups)."""

    def test_interview_with_planning_metadata(self):
        """Test interview with plan_metadata."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.IDLE,
        )
        interview.plan_metadata = {
            "n": 4,
            "generated_at": datetime.utcnow().isoformat(),
            "strategy": "adaptive_planning_v1",
        }

        assert interview.planned_question_count == 4
        assert "strategy" in interview.plan_metadata

    def test_interview_without_planning_metadata(self):
        """Test legacy interview without plan_metadata."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.IDLE,
        )

        assert interview.plan_metadata == {}
        assert interview.planned_question_count == 0

    def test_add_adaptive_followup(self):
        """Test adding adaptive follow-up questions."""
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.EVALUATING,
        )

        follow_up_id = uuid4()
        interview.add_adaptive_followup(follow_up_id)

        assert len(interview.adaptive_follow_ups) == 1
        assert follow_up_id in interview.adaptive_follow_ups

    def test_mark_ready_with_cv_analysis(self):
        """Test marking interview as READY after planning."""
        cv_analysis_id = uuid4()
        interview = Interview(
            candidate_id=uuid4(),
            status=InterviewStatus.IDLE,
        )

        interview.mark_ready(cv_analysis_id)

        assert interview.status == InterviewStatus.IDLE
        assert interview.cv_analysis_id == cv_analysis_id


class TestAnswerAdaptiveFields:
    """Test Answer model adaptive fields (similarity_score, gaps)."""

    def test_answer_with_similarity_score(self):
        """Test answer with similarity score."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Recursion calls itself with base case",
            is_voice=False,
            similarity_score=0.85,
        )

        assert answer.has_similarity_score() is True
        assert answer.similarity_score == 0.85
        assert answer.meets_threshold(0.8) is True
        assert answer.meets_threshold(0.9) is False

    def test_answer_without_similarity_score(self):
        """Test legacy answer without similarity score."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Some answer",
            is_voice=False,
        )

        assert answer.has_similarity_score() is False
        assert answer.similarity_score is None
        assert answer.meets_threshold(0.8) is False

    def test_answer_with_gaps(self):
        """Test answer with detected gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Brief answer",
            is_voice=False,
            gaps={
                "concepts": ["base case", "recursive case"],
                "keywords": ["base", "recursive"],
                "confirmed": True,
                "severity": "moderate",
            },
        )

        assert answer.has_gaps() is True
        assert len(answer.gaps["concepts"]) == 2  # type: ignore
        assert answer.gaps["confirmed"] is True  # type: ignore

    def test_answer_without_gaps(self):
        """Test answer without gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Complete answer",
            is_voice=False,
            gaps={"concepts": [], "confirmed": False},
        )

        assert answer.has_gaps() is False

    def test_is_adaptive_complete_high_similarity(self):
        """Test adaptive completion with high similarity (>= 80%)."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Good answer",
            is_voice=False,
            similarity_score=0.85,
            gaps={"concepts": ["minor"], "confirmed": True},
        )

        assert answer.is_adaptive_complete() is True

    def test_is_adaptive_complete_no_gaps(self):
        """Test adaptive completion with no confirmed gaps."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Good answer",
            is_voice=False,
            similarity_score=0.75,
            gaps={"concepts": [], "confirmed": False},
        )

        assert answer.is_adaptive_complete() is True

    def test_is_adaptive_incomplete(self):
        """Test answer that needs follow-up."""
        answer = Answer(
            interview_id=uuid4(),
            question_id=uuid4(),
            candidate_id=uuid4(),
            text="Brief answer",
            is_voice=False,
            similarity_score=0.60,
            gaps={"concepts": ["base case"], "confirmed": True},
        )

        assert answer.is_adaptive_complete() is False


class TestFollowUpQuestion:
    """Test FollowUpQuestion model."""

    def test_follow_up_question_creation(self):
        """Test creating a follow-up question."""
        parent_id = uuid4()
        interview_id = uuid4()

        follow_up = FollowUpQuestion(
            parent_question_id=parent_id,
            interview_id=interview_id,
            text="Can you explain the base case in recursion?",
            generated_reason="Missing concepts: base case",
            order_in_sequence=1,
        )

        assert follow_up.parent_question_id == parent_id
        assert follow_up.interview_id == interview_id
        assert follow_up.order_in_sequence == 1
        assert follow_up.is_last_allowed() is False

    def test_is_last_allowed(self):
        """Test detection of 3rd follow-up (max allowed)."""
        follow_up_third = FollowUpQuestion(
            parent_question_id=uuid4(),
            interview_id=uuid4(),
            text="Third follow-up",
            generated_reason="Still missing concepts",
            order_in_sequence=3,
        )

        assert follow_up_third.is_last_allowed() is True

    def test_follow_up_has_created_at(self):
        """Test follow-up question has timestamp."""
        follow_up = FollowUpQuestion(
            parent_question_id=uuid4(),
            interview_id=uuid4(),
            text="Follow-up",
            generated_reason="Gaps detected",
            order_in_sequence=1,
        )

        assert follow_up.created_at is not None
        assert isinstance(follow_up.created_at, datetime)


class TestSimilarityScoreValidation:
    """Test similarity_score field validation (0-1 range)."""

    def test_valid_similarity_scores(self):
        """Test valid similarity scores (0.0 to 1.0)."""
        for score in [0.0, 0.5, 0.8, 1.0]:
            answer = Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=score,
            )
            assert answer.similarity_score == score

    def test_invalid_similarity_score_too_high(self):
        """Test invalid similarity score > 1.0."""
        with pytest.raises(Exception):  # Pydantic validation error
            Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=1.5,
            )

    def test_invalid_similarity_score_negative(self):
        """Test invalid negative similarity score."""
        with pytest.raises(Exception):  # Pydantic validation error
            Answer(
                interview_id=uuid4(),
                question_id=uuid4(),
                candidate_id=uuid4(),
                text="Test",
                is_voice=False,
                similarity_score=-0.1,
            )
</file>

<file path="tests/unit/use_cases/test_complete_interview.py">
"""Tests for Phase 06: CompleteInterviewUseCase."""

from uuid import uuid4

import pytest

from src.application.use_cases.complete_interview import CompleteInterviewUseCase
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.interview import InterviewStatus


class TestCompleteInterviewUseCase:
    """Test interview completion with summary generation."""

    @pytest.mark.asyncio
    async def test_complete_interview_with_summary_generation(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test complete interview with summary generation enabled."""
        # Setup interview
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create question and answer
        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Good answer",
            is_voice=False,
            similarity_score=0.85,
            gaps={"concepts": [], "keywords": [], "confirmed": False},
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=85.0,
                semantic_similarity=0.85,
                completeness=0.9,
                relevance=0.95,
                sentiment="confident",
                reasoning="Strong",
                strengths=["Good"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        # Execute use case with all dependencies
        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=True,
        )

        # Verify interview completed
        assert interview.status == InterviewStatus.COMPLETE

        # Verify summary generated
        assert summary is not None
        assert summary["interview_id"] == str(sample_interview_adaptive.id)
        assert "overall_score" in summary
        assert "strengths" in summary

        # Verify summary stored in metadata
        assert interview.plan_metadata is not None
        assert "completion_summary" in interview.plan_metadata
        assert interview.plan_metadata["completion_summary"] == summary

    @pytest.mark.asyncio
    async def test_complete_interview_without_summary_generation(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
    ):
        """Test complete interview with summary generation disabled."""
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=False,
        )

        # Verify interview completed
        assert interview.status == InterviewStatus.COMPLETE

        # Verify no summary generated
        assert summary is None

    @pytest.mark.asyncio
    async def test_complete_interview_missing_dependencies(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
        mock_answer_repo,
    ):
        """Test complete interview with missing dependencies -> no summary."""
        await mock_interview_repo.save(sample_interview_adaptive)

        # Missing question_repo, follow_up_repo, llm
        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=None,
            follow_up_question_repository=None,
            llm=None,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=True,  # Requested but will be skipped
        )

        # Verify interview completed
        assert interview.status == InterviewStatus.COMPLETE

        # Verify no summary generated (missing dependencies)
        assert summary is None

    @pytest.mark.asyncio
    async def test_complete_interview_not_found(
        self,
        mock_interview_repo,
    ):
        """Test error when interview not found."""
        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
        )

        with pytest.raises(ValueError, match="Interview .* not found"):
            await use_case.execute(interview_id=uuid4())

    @pytest.mark.asyncio
    async def test_complete_interview_invalid_status(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
    ):
        """Test error when interview not IN_PROGRESS."""
        # Set status to COMPLETED
        sample_interview_adaptive.status = InterviewStatus.COMPLETE
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
        )

        with pytest.raises(ValueError, match="Cannot complete interview with status"):
            await use_case.execute(interview_id=sample_interview_adaptive.id)

    @pytest.mark.asyncio
    async def test_complete_interview_ready_status_invalid(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
    ):
        """Test error when interview status is READY (not started)."""
        # Set status to READY
        sample_interview_adaptive.status = InterviewStatus.IDLE
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
        )

        with pytest.raises(ValueError, match="Cannot complete interview with status"):
            await use_case.execute(interview_id=sample_interview_adaptive.id)

    @pytest.mark.asyncio
    async def test_complete_interview_initializes_metadata(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test plan_metadata initialized if None before storing summary."""
        # Setup interview with None metadata
        sample_interview_adaptive.plan_metadata = None
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create minimal data for summary
        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Answer",
            is_voice=False,
            similarity_score=0.8,
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=80.0,
                semantic_similarity=0.8,
                completeness=0.85,
                relevance=0.9,
                sentiment="confident",
                reasoning="Good",
                strengths=["Clear"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=True,
        )

        # Verify metadata initialized
        assert interview.plan_metadata is not None
        assert "completion_summary" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_complete_interview_preserves_existing_metadata(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test existing plan_metadata preserved when adding summary."""
        # Setup interview with existing metadata
        sample_interview_adaptive.plan_metadata = {
            "n": 3,
            "strategy": "adaptive_planning_v1",
            "custom_field": "preserved",
        }
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create minimal data
        q1_id = sample_interview_adaptive.question_ids[0]
        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        answer1 = Answer(
            interview_id=sample_interview_adaptive.id,
            question_id=q1_id,
            candidate_id=sample_interview_adaptive.candidate_id,
            text="Answer",
            is_voice=False,
            similarity_score=0.8,
        )
        answer1.evaluate(
            AnswerEvaluation(
                score=80.0,
                semantic_similarity=0.8,
                completeness=0.85,
                relevance=0.9,
                sentiment="confident",
                reasoning="Good",
                strengths=["Clear"],
                weaknesses=[],
                improvement_suggestions=[],
            )
        )
        await mock_answer_repo.save(answer1)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=True,
        )

        # Verify existing metadata preserved
        assert interview.plan_metadata["n"] == 3
        assert interview.plan_metadata["strategy"] == "adaptive_planning_v1"
        assert interview.plan_metadata["custom_field"] == "preserved"
        assert "completion_summary" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_complete_interview_returns_tuple(
        self,
        sample_interview_adaptive,
        mock_interview_repo,
    ):
        """Test return value is tuple (Interview, dict | None)."""
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
        )

        result = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=False,
        )

        # Verify tuple structure
        assert isinstance(result, tuple)
        assert len(result) == 2
        interview, summary = result
        assert interview is not None
        assert summary is None


class TestCompleteInterviewIntegration:
    """Integration tests for complete interview with summary."""

    @pytest.mark.asyncio
    async def test_complete_flow_with_multiple_answers(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_interview_repo,
        mock_answer_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
    ):
        """Test complete flow with multiple evaluated answers."""
        await mock_interview_repo.save(sample_interview_adaptive)

        # Create 3 questions
        q1_id = sample_interview_adaptive.question_ids[0]
        q2_id = sample_interview_adaptive.question_ids[1]
        q3_id = sample_interview_adaptive.question_ids[2]

        q1 = sample_question_with_ideal_answer
        q1.id = q1_id
        await mock_question_repo.save(q1)

        q2 = sample_question_with_ideal_answer
        q2.id = q2_id
        await mock_question_repo.save(q2)

        q3 = sample_question_with_ideal_answer
        q3.id = q3_id
        await mock_question_repo.save(q3)

        # Create 3 evaluated answers
        for q_id in [q1_id, q2_id, q3_id]:
            answer = Answer(
                interview_id=sample_interview_adaptive.id,
                question_id=q_id,
                candidate_id=sample_interview_adaptive.candidate_id,
                text="Good answer",
                is_voice=False,
                similarity_score=0.85,
            )
            answer.evaluate(
                AnswerEvaluation(
                    score=85.0,
                    semantic_similarity=0.85,
                    completeness=0.9,
                    relevance=0.95,
                    sentiment="confident",
                    reasoning="Strong",
                    strengths=["Good"],
                    weaknesses=[],
                    improvement_suggestions=[],
                )
            )
            await mock_answer_repo.save(answer)

        use_case = CompleteInterviewUseCase(
            interview_repository=mock_interview_repo,
            answer_repository=mock_answer_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
        )

        interview, summary = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            generate_summary=True,
        )

        # Verify complete flow
        assert interview.status == InterviewStatus.COMPLETE
        assert summary is not None
        assert summary["total_questions"] == 3
        assert summary["overall_score"] > 0.0
        assert len(summary["question_summaries"]) == 3
</file>

<file path="alembic/env.py">
"""Alembic environment configuration for async SQLAlchemy."""

import asyncio
import re
from logging.config import fileConfig

from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context
from alembic.script import ScriptDirectory

# Import Base and all models for autogenerate support
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.infrastructure.config.settings import get_settings
from src.infrastructure.database.base import Base

# Import all models to ensure they're registered with Base.metadata
from src.adapters.persistence import models  # noqa: F401

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Get database URL from settings
settings = get_settings()
config.set_main_option("sqlalchemy.url", settings.async_database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata


def get_next_sequential_revision():
    """Get the next sequential revision number by scanning existing migrations.

    Scans all migration files in the versions directory and finds the highest
    numeric revision ID, then returns the next one formatted as 4-digit zero-padded.

    Returns:
        str: Next sequential revision ID (e.g., "0001", "0002", etc.)
    """
    script_dir = ScriptDirectory.from_config(config)
    versions_dir = Path(script_dir.versions)

    max_revision = 0

    # Scan all Python files in versions directory
    for file in versions_dir.glob("*.py"):
        if file.name.startswith("__"):
            continue
        try:
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
                # Look for revision = "..." or revision: str = "..."
                # Match both quoted strings and numeric-only IDs (4+ digits for sequential)
                # Pattern matches: revision = "0001" or revision: str = '0001'
                match = re.search(
                    r'revision\s*[=:]\s*["\'](\d{4,})["\']',
                    content
                )
                if match:
                    rev_id = match.group(1)
                    # Only consider pure numeric IDs (sequential ones)
                    if rev_id.isdigit():
                        rev_num = int(rev_id)
                        max_revision = max(max_revision, rev_num)
        except Exception:
            # Skip files that can't be read or parsed
            pass

    return f"{max_revision + 1:04d}"  # Returns "0001", "0002", etc.


def process_revision_directives(context, revision, directives):
    """Customize revision ID generation to use sequential numbers.

    This hook is called by Alembic when generating new revision scripts.
    If sequential revisions are enabled in config, it replaces the auto-generated
    revision ID with a sequential numeric one.

    Args:
        context: Alembic migration context
        revision: Tuple of (revision_id, branch_labels) or None
        directives: List of revision directive objects
    """
    use_sequential = config.get_main_option(
        "use_sequential_revisions", "true"
    ).lower() == "true"

    if use_sequential and directives:
        # Check if we should override the revision ID
        # Override if:
        # 1. No explicit --rev-id was provided (revision is None or has auto-generated hash)
        # 2. The current rev_id doesn't look like a sequential number (4+ digits)
        current_rev_id = directives[0].rev_id if directives else None

        if current_rev_id:
            # If it's already a sequential number (4+ digits), don't override
            if current_rev_id.isdigit() and len(current_rev_id) >= 4:
                return
            # If it looks like a hash (alphanumeric, not all digits), override it
            if not current_rev_id.isdigit():
                new_rev_id = get_next_sequential_revision()
                directives[0].rev_id = new_rev_id
        else:
            # No rev_id set yet, generate sequential one
            new_rev_id = get_next_sequential_revision()
            directives[0].rev_id = new_rev_id


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,  # Detect column type changes
        compare_server_default=True,  # Detect default value changes
        process_revision_directives=process_revision_directives,
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    """Run migrations with the given connection.

    Args:
        connection: SQLAlchemy connection
    """
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,  # Detect column type changes
        compare_server_default=True,  # Detect default value changes
        process_revision_directives=process_revision_directives,
    )

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """Run migrations in async mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.
    """
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode using async engine."""
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="pyproject.toml">
[project]
name = "elios-ai-service"
version = "0.1.0"
description = "AI-powered mock interview platform with CV analysis and real-time feedback"
readme = "README.md"
requires-python = ">=3.11"
license = {text = "MIT"}
authors = [
    {name = "Elios Team"},
]
keywords = ["ai", "interview", "nlp", "fastapi"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    # Core Framework
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.1.0",

    # LLM Providers
    "openai>=1.3.0",
    "anthropic>=0.7.0",

    # Vector Databases
    "pinecone-client>=3.0.0",

    # Database
    "sqlalchemy[asyncio]>=2.0.0",
    "asyncpg>=0.29.0",
    "alembic>=1.13.0",

    # NLP & Document Processing
    "spacy>=3.7.0",
    "langchain>=0.1.0",
    "PyPDF2>=3.0.0",
    "python-docx>=1.1.0",

    # Speech Services
    "azure-cognitiveservices-speech>=1.31.0",

    # Utilities
    "python-multipart>=0.0.6",
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
    "httpx>=0.25.0",
]

[project.optional-dependencies]
dev = [
    # Testing
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",

    # Code Quality
    "ruff>=0.1.6",
    "black>=23.11.0",
    "mypy>=1.7.0",

    # Development Tools
    "ipython>=8.18.0",
    "python-dotenv>=1.0.0",
]

[project.urls]
Homepage = "https://github.com/elios/elios-ai-service"
Repository = "https://github.com/elios/elios-ai-service"
Documentation = "https://github.com/elios/elios-ai-service#readme"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
addopts = "-v --cov=src --cov-report=term-missing --cov-report=html"
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"

[tool.coverage.run]
source = ["src"]
omit = ["tests/*", "**/__pycache__/*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "@abstractmethod",
]

[tool.ruff]
line-length = 100
target-version = "py311"
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
    "UP",  # pyupgrade
]
ignore = [
    "E501",  # line too long (handled by black)
    "B008",  # do not perform function calls in argument defaults
]
exclude = [
    ".git",
    ".venv",
    "venv",
    "__pycache__",
    "build",
    "dist",
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]  # Unused imports in __init__.py

[tool.black]
line-length = 100
target-version = ["py311"]
include = '\.pyi?$'
exclude = '''
/(
    \.git
  | \.venv
  | venv
  | __pycache__
  | build
  | dist
)/
'''

[tool.mypy]
 python_version = "3.12"
  # Enable stricter checks
  disallow_untyped_defs = true
  disallow_incomplete_defs = true
  strict = true
  warn_return_any = true
  warn_unused_configs = true
  check_untyped_defs = true
  no_implicit_optional = true
  warn_redundant_casts = true
  warn_unused_ignores = true
  warn_no_return = true
  strict_equality = true
  ignore_missing_imports = false

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
</file>

<file path="src/application/dto/websocket_dto.py">
"""WebSocket message DTOs for real-time interview communication."""

from typing import Any, Literal
from uuid import UUID

from pydantic import BaseModel, Field, field_validator


# Base message
class WebSocketMessage(BaseModel):
    """Base WebSocket message format."""

    type: str
    payload: dict[str, Any]


# ============================================================================
# CLIENT â†’ SERVER MESSAGES
# ============================================================================


class TextAnswerMessage(BaseModel):
    """Client sends text answer for a question."""

    type: Literal["text_answer"] = "text_answer"
    question_id: UUID = Field(..., description="ID of question being answered")
    answer_text: str = Field(..., min_length=1, description="Answer text content")


class AudioChunkMessage(BaseModel):
    """Client sends audio chunk for voice answer.

    Audio is sent in chunks to support streaming and large files.
    """

    type: Literal["audio_chunk"] = "audio_chunk"
    audio_data: str = Field(..., description="Base64-encoded audio bytes")
    chunk_index: int = Field(..., ge=0, description="Sequential chunk number")
    is_final: bool = Field(..., description="Whether this is the last chunk")
    format: str = Field(default="webm", description="Audio format (webm, wav, mp3)")
    question_id: UUID = Field(..., description="ID of question being answered")

    @field_validator("format")
    @classmethod
    def validate_format(cls, v: str) -> str:
        """Validate audio format is supported."""
        supported_formats = {"webm", "wav", "mp3", "ogg"}
        if v.lower() not in supported_formats:
            raise ValueError(f"Unsupported audio format: {v}. Must be one of {supported_formats}")
        return v.lower()


class GetNextQuestionMessage(BaseModel):
    """Client requests next question in interview."""

    type: Literal["get_next_question"] = "get_next_question"


class RequestRetryMessage(BaseModel):
    """Client requests retry of failed operation."""

    type: Literal["request_retry"] = "request_retry"
    failed_message_type: str = Field(..., description="Type of message that failed")
    error_code: str = Field(..., description="Error code from failure")


# ============================================================================
# SERVER â†’ CLIENT MESSAGES
# ============================================================================


class QuestionMessage(BaseModel):
    """Server sends question to client with optional TTS audio."""

    type: Literal["question"] = "question"
    question_id: UUID = Field(..., description="Unique question ID")
    text: str = Field(..., description="Question text")
    question_type: str = Field(..., description="Type (TECHNICAL, BEHAVIORAL, SITUATIONAL)")
    difficulty: str = Field(..., description="Difficulty (EASY, MEDIUM, HARD)")
    index: int = Field(..., ge=1, description="Current question number (1-based)")
    total: int = Field(..., ge=1, description="Total questions in interview")
    audio_data: str | None = Field(None, description="Base64-encoded TTS audio (WAV)")
    audio_format: str = Field(default="wav", description="Audio format")


class FollowUpQuestionMessage(BaseModel):
    """Server sends follow-up question for deeper probing."""

    type: Literal["follow_up_question"] = "follow_up_question"
    question_id: UUID = Field(..., description="Unique follow-up question ID")
    parent_question_id: UUID = Field(..., description="ID of original question")
    text: str = Field(..., description="Follow-up question text")
    generated_reason: str = Field(..., description="Why this follow-up was generated")
    order_in_sequence: int = Field(..., ge=1, description="Order in follow-up chain")
    audio_data: str | None = Field(None, description="Base64-encoded TTS audio")
    audio_format: str = Field(default="wav", description="Audio format")


class EvaluationMessage(BaseModel):
    """Server sends answer evaluation with optional voice metrics (Phase 3 enhanced)."""

    type: Literal["evaluation"] = "evaluation"
    answer_id: UUID = Field(..., description="Unique answer ID")
    score: float = Field(..., ge=0.0, le=100.0, description="Overall combined score (0-100)")
    feedback: str = Field(..., description="Overall feedback")
    strengths: list[str] = Field(default_factory=list, description="Answer strengths")
    weaknesses: list[str] = Field(default_factory=list, description="Areas for improvement")

    # Detailed scoring breakdown (Phase 3)
    theoretical_score: float | None = Field(
        None, ge=0.0, le=100.0, description="Semantic/content score (0-100)"
    )
    speaking_score: float | None = Field(
        None, ge=0.0, le=100.0, description="Voice delivery score (0-100)"
    )

    # Additional metrics
    similarity_score: float | None = Field(
        None, ge=0.0, le=1.0, description="Similarity to ideal answer (0-1)"
    )
    gaps: dict[str, Any] | None = Field(None, description="Knowledge gaps detected")
    voice_metrics: dict[str, float] | None = Field(
        None, description="Voice quality metrics (if audio answer)"
    )


class VoiceMetricsMessage(BaseModel):
    """Server sends real-time voice analysis during answer."""

    type: Literal["voice_metrics"] = "voice_metrics"
    intonation_score: float = Field(..., ge=0.0, le=1.0, description="Pitch variance (0-1)")
    fluency_score: float = Field(..., ge=0.0, le=1.0, description="Speaking fluency (0-1)")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Recognition confidence (0-1)")
    speaking_rate_wpm: int = Field(..., ge=0, description="Words per minute")
    real_time: bool = Field(default=True, description="Real-time or final metrics")


class TranscriptionMessage(BaseModel):
    """Server sends transcription of audio (intermediate or final)."""

    type: Literal["transcription"] = "transcription"
    text: str = Field(..., description="Transcribed text")
    is_final: bool = Field(..., description="Whether transcription is final")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Transcription confidence")


class InterviewCompleteMessage(BaseModel):
    """Server notifies interview completion with final results."""

    type: Literal["interview_complete"] = "interview_complete"
    interview_id: UUID = Field(..., description="Completed interview ID")
    overall_score: float = Field(..., ge=0.0, le=100.0, description="Overall score (0-100)")
    total_questions: int = Field(..., ge=1, description="Total questions answered")
    feedback_url: str = Field(..., description="URL to detailed feedback report")


class ErrorMessage(BaseModel):
    """Server sends structured error with recovery options."""

    type: Literal["error"] = "error"
    code: str = Field(..., description="Error code (see WebSocketErrorCode enum)")
    message: str = Field(..., description="Human-readable error message")
    recoverable: bool = Field(default=False, description="Whether error is recoverable")
    retry_available: bool = Field(default=False, description="Whether retry is possible")
    fallback_option: str | None = Field(
        None, description="Suggested fallback (e.g., 'text_mode')"
    )
</file>

<file path="src/application/use_cases/analyze_cv.py">
"""Analyze CV use case."""

from uuid import UUID

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.ports.cv_analyzer_port import CVAnalyzerPort
from ...domain.ports.vector_search_port import VectorSearchPort


class AnalyzeCVUseCase:
    """Use case for analyzing a candidate's CV.

    This orchestrates the CV analysis process:
    1. Extract text from CV file
    2. Analyze and extract structured information
    3. Generate embeddings for semantic search
    4. Store embeddings in vector database
    """

    def __init__(
        self,
        cv_analyzer: CVAnalyzerPort,
        vector_search: VectorSearchPort,
    ):
        """Initialize use case with required ports.

        Args:
            cv_analyzer: CV analysis service
            vector_search: Vector database service
        """
        self.cv_analyzer = cv_analyzer
        self.vector_search = vector_search

    async def execute(
        self,
        cv_file_path: str,
        candidate_id: UUID,
    ) -> CVAnalysis:
        """Execute CV analysis.

        Args:
            cv_file_path: Path to CV file
            candidate_id: ID of the candidate

        Returns:
            CVAnalysis with extracted information

        Raises:
            ValueError: If CV file is invalid or cannot be processed
        """
        # Step 1: Analyze CV using the CV analyzer port
        cv_analysis = await self.cv_analyzer.analyze_cv(
            cv_file_path=cv_file_path,
            candidate_id=str(candidate_id),
        )

        # Step 2: Generate embedding for the CV
        # Combine key information for embedding
        cv_text_for_embedding = f"""
        Summary: {cv_analysis.summary}
        Skills: {', '.join([skill.name for skill in cv_analysis.skills])}
        Experience: {cv_analysis.work_experience_years} years
        Education: {cv_analysis.education_level}
        Difficulty: {cv_analysis.suggested_difficulty}
        """.strip()

        embedding = await self.vector_search.get_embedding(cv_text_for_embedding)
        cv_analysis.embedding = embedding
        metadata = cv_analysis.metadata

        # Step 3: Store embedding in vector database for future question matching
        await self.vector_search.store_cv_embedding(
            cv_analysis_id=cv_analysis.id,
            embedding=embedding,
            # metadata={
            #     "candidate_id": str(candidate_id),
            #     "skills": [skill.name for skill in cv_analysis.skills],
            #     "experience_years": cv_analysis.work_experience_years,
            #     "education": cv_analysis.education_level,
            #     "suggested_difficulty": cv_analysis.suggested_difficulty,
            # },
            metadata = metadata
        )

        return cv_analysis
</file>

<file path="src/application/use_cases/complete_interview.py">
"""Complete interview use case."""

from typing import Any
from uuid import UUID

from ...domain.models.interview import Interview, InterviewStatus
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from .generate_summary import GenerateSummaryUseCase


class CompleteInterviewUseCase:
    """Complete interview and generate comprehensive summary."""

    def __init__(
        self,
        interview_repository: InterviewRepositoryPort,
        answer_repository: AnswerRepositoryPort | None = None,
        question_repository: QuestionRepositoryPort | None = None,
        follow_up_question_repository: FollowUpQuestionRepositoryPort | None = None,
        llm: LLMPort | None = None,
    ):
        self.interview_repo = interview_repository
        self.answer_repo = answer_repository
        self.question_repo = question_repository
        self.follow_up_repo = follow_up_question_repository
        self.llm = llm

    async def execute(
        self, interview_id: UUID, generate_summary: bool = True
    ) -> tuple[Interview, dict[str, Any] | None]:
        """Mark interview as completed and optionally generate summary.

        Args:
            interview_id: The interview UUID
            generate_summary: Whether to generate comprehensive summary (default: True)

        Returns:
            Tuple of (completed interview, summary dict or None)

        Raises:
            ValueError: If interview not found or invalid state
        """
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        if interview.status != InterviewStatus.EVALUATING:
            raise ValueError(f"Cannot complete interview with status: {interview.status}")

        # Generate summary if requested and dependencies available
        summary = None
        if (
            generate_summary
            and self.answer_repo
            and self.question_repo
            and self.follow_up_repo
            and self.llm
        ):
            summary_use_case = GenerateSummaryUseCase(
                interview_repository=self.interview_repo,
                answer_repository=self.answer_repo,
                question_repository=self.question_repo,
                follow_up_question_repository=self.follow_up_repo,
                llm=self.llm,
            )
            summary = await summary_use_case.execute(interview_id)

            # Store summary in interview metadata
            if interview.plan_metadata is None:
                interview.plan_metadata = {}
            interview.plan_metadata["completion_summary"] = summary

        # Mark interview as complete
        interview.complete()
        updated_interview = await self.interview_repo.update(interview)

        return updated_interview, summary
</file>

<file path="src/application/use_cases/plan_interview.py">
"""Plan interview use case."""

import logging
from datetime import datetime
from typing import Any
from uuid import UUID

from ...domain.models.cv_analysis import CVAnalysis
from ...domain.models.interview import Interview, InterviewStatus
from ...domain.models.question import DifficultyLevel, Question, QuestionType
from ...domain.ports.cv_analysis_repository_port import CVAnalysisRepositoryPort
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from ...domain.ports.vector_search_port import VectorSearchPort

logger = logging.getLogger(__name__)


class PlanInterviewUseCase:
    """Use case for planning interview questions with ideal answers.

    This orchestrates the pre-planning phase:
    1. Load CV analysis
    2. Calculate n based on skill diversity (max 5)
    3. Generate n questions with ideal answers + rationale (using vector exemplars)
    4. Store questions with embeddings and mark interview as READY
    """

    def __init__(
        self,
        llm: LLMPort,
        vector_search: VectorSearchPort,
        cv_analysis_repo: CVAnalysisRepositoryPort,
        interview_repo: InterviewRepositoryPort,
        question_repo: QuestionRepositoryPort,
    ):
        """Initialize use case with required ports.

        Args:
            llm: LLM service for question generation
            vector_search: Vector search service for exemplar retrieval and embedding storage
            cv_analysis_repo: CV analysis storage
            interview_repo: Interview storage
            question_repo: Question storage
        """
        self.llm = llm
        self.vector_search = vector_search
        self.cv_analysis_repo = cv_analysis_repo
        self.interview_repo = interview_repo
        self.question_repo = question_repo

    async def execute(
        self,
        cv_analysis_id: UUID,
        candidate_id: UUID,
    ) -> Interview:
        """Plan interview by generating n questions with ideal answers.

        Args:
            cv_analysis_id: CV analysis to base questions on
            candidate_id: Candidate being interviewed

        Returns:
            Interview entity with status=READY

        Raises:
            ValueError: If CV analysis not found
            Exception: If question generation fails
        """
        logger.info(
            "Starting interview planning",
            extra={
                "cv_analysis_id": str(cv_analysis_id),
                "candidate_id": str(candidate_id),
            },
        )

        # Step 1: Load CV analysis
        cv_analysis = await self.cv_analysis_repo.get_by_id(cv_analysis_id)
        if not cv_analysis:
            raise ValueError(f"CV analysis {cv_analysis_id} not found")

        # Step 2: Calculate n based on skill diversity
        n = self._calculate_question_count(cv_analysis)
        logger.info(f"Calculated n={n} questions based on CV complexity")

        # Step 3: Create interview
        interview = Interview(
            candidate_id=candidate_id,
            status=InterviewStatus.IDLE,
            cv_analysis_id=cv_analysis_id,
        )
        await self.interview_repo.save(interview)

        # Step 4: Generate n questions with embeddings (sequential for MVP)
        question_ids = []
        try:
            for i in range(n):
                question = await self._generate_question_with_ideal_answer(cv_analysis, i, n)
                await self.question_repo.save(question)
                question_ids.append(question.id)
                logger.info(f"Generated question {i + 1}/{n}: {question.id}")

                # Store question embedding in vector DB (non-blocking)
                await self._store_question_embedding(question)

        except Exception as e:
            logger.error(f"Failed to generate questions: {e}")
            # Rollback: Delete partially created questions
            for qid in question_ids:
                try:
                    await self.question_repo.delete(qid)
                except Exception:
                    pass  # Best effort cleanup
            raise

        # Step 5: Update interview
        interview.question_ids = question_ids
        interview.plan_metadata = {
            "n": n,
            "generated_at": datetime.utcnow().isoformat(),
            "strategy": "adaptive_planning_v1",
            "cv_summary": cv_analysis.summary or "No summary",
        }
        interview.mark_ready(cv_analysis_id)
        await self.interview_repo.update(interview)

        logger.info(
            "Interview planning complete",
            extra={
                "interview_id": str(interview.id),
                "question_count": n,
            },
        )

        return interview

    def _calculate_question_count(self, cv_analysis: CVAnalysis) -> int:
        """Calculate question count based on skill diversity only.

        Args:
            cv_analysis: Analyzed CV data

        Returns:
            Question count (2-5)
        """
        skill_count = len(cv_analysis.skills)

        # Skill-only calculation (ignore experience years)
        if skill_count <= 2:
            n = 2
        elif skill_count <= 4:
            n = 3
        elif skill_count <= 7:
            n = 4
        else:
            n = 5  # Max 5 questions

        return n

    def _build_search_query(
        self,
        skill: str,
        cv_analysis: CVAnalysis,
        difficulty: DifficultyLevel,
    ) -> str:
        """Build search query for exemplar retrieval.

        Args:
            skill: Target skill being tested
            cv_analysis: CV analysis with experience data
            difficulty: Question difficulty level

        Returns:
            Search query string for vector DB
        """
        experience = cv_analysis.work_experience_years or 0
        exp_level = "junior" if experience < 3 else "mid" if experience < 7 else "senior"

        return f"{skill} {difficulty.value.lower()} interview question for {exp_level} developer"

    async def _find_exemplar_questions(
        self,
        skill: str,
        question_type: QuestionType,
        difficulty: DifficultyLevel,
        cv_analysis: CVAnalysis,
    ) -> list[dict[str, Any]]:
        """Find similar questions as exemplars from vector DB.

        Args:
            skill: Target skill
            question_type: Question type (TECHNICAL/BEHAVIORAL/SITUATIONAL)
            difficulty: Question difficulty
            cv_analysis: CV analysis for context

        Returns:
            List of exemplar questions (max 3), empty list on failure
        """
        try:
            # Build search query
            query_text = self._build_search_query(skill, cv_analysis, difficulty)

            # Get query embedding
            query_embedding = await self.vector_search.get_embedding(query_text)

            # Search with filters
            similar_questions = await self.vector_search.find_similar_questions(
                query_embedding=query_embedding,
                top_k=5,  # Request 5, use top 3
                filters={
                    "question_type": question_type.value,
                    "difficulty": difficulty.value,
                }
            )

            # Format exemplars
            exemplars = [
                {
                    "text": q.get("text", ""),
                    "skills": q.get("metadata", {}).get("skills", []),
                    "difficulty": q.get("metadata", {}).get("difficulty", ""),
                    "similarity_score": q.get("score", 0.0),
                }
                for q in similar_questions[:3]  # Use top 3
                if q.get("score", 0) > 0.5  # Similarity threshold
            ]

            logger.info(f"Found {len(exemplars)} exemplar questions for {skill}")
            return exemplars

        except Exception as e:
            logger.warning(f"Vector search failed: {e}. Falling back to no exemplars.")
            return []  # Fallback: empty exemplars

    async def _store_question_embedding(
        self,
        question: Question,
    ) -> None:
        """Store question embedding in vector DB.

        Args:
            question: Question entity to store embedding for

        Note:
            Non-critical operation - continues even if storage fails
        """
        try:
            # Generate embedding
            embedding = await self.vector_search.get_embedding(question.text)

            # Store with metadata
            await self.vector_search.store_question_embedding(
                question_id=question.id,
                embedding=embedding,
                metadata={
                    "text": question.text,
                    "skills": question.skills,
                    "difficulty": question.difficulty.value,
                    "question_type": question.question_type.value,
                    "tags": question.tags or [],
                }
            )

            logger.info(f"Stored embedding for question {question.id}")

        except Exception as e:
            logger.error(f"Failed to store embedding for {question.id}: {e}")
            # Non-critical: Continue even if embedding storage fails

    async def _generate_question_with_ideal_answer(
        self,
        cv_analysis: CVAnalysis,
        index: int,
        total: int,
    ) -> Question:
        """Generate single question with ideal answer + rationale using vector exemplars.

        Args:
            cv_analysis: CV data for context
            index: Current question index (0-based)
            total: Total questions to generate

        Returns:
            Question entity with ideal_answer + rationale populated
        """
        # Determine question type/difficulty based on index
        question_type, difficulty = self._get_question_distribution(index, total)

        # Select skill to test
        skills = cv_analysis.get_top_skills(limit=5)
        skill = skills[index % len(skills)].name if skills else "general knowledge"

        # Find exemplar questions from vector DB
        exemplars = await self._find_exemplar_questions(
            skill=skill,
            question_type=question_type,
            difficulty=difficulty,
            cv_analysis=cv_analysis,
        )

        # Generate question with exemplars
        context = {
            "summary": cv_analysis.summary or "No summary",
            "skills": [s.name for s in skills],
            "experience": cv_analysis.work_experience_years or 0,
        }

        # Generate question using exemplars (if found)
        question_text = await self.llm.generate_question(
            context=context,
            skill=skill,
            difficulty=difficulty.value,
            exemplars=exemplars if exemplars else None,
        )

        # Generate ideal answer
        ideal_answer = await self.llm.generate_ideal_answer(
            question_text=question_text,
            context=context,
        )

        # Generate rationale
        rationale = await self.llm.generate_rationale(
            question_text=question_text,
            ideal_answer=ideal_answer,
        )

        # Create Question entity
        question = Question(
            text=question_text,
            question_type=question_type,
            difficulty=difficulty,
            skills=[skill],
            ideal_answer=ideal_answer,
            rationale=rationale,
        )

        return question

    def _get_question_distribution(
        self, index: int, total: int
    ) -> tuple[QuestionType, DifficultyLevel]:
        """Determine question type and difficulty based on index.

        Distribution:
        - 60% technical, 30% behavioral, 10% situational
        - 50% easy, 30% medium, 20% hard

        Args:
            index: Current question index
            total: Total questions

        Returns:
            (QuestionType, DifficultyLevel)
        """
        # Question type distribution
        technical_count = int(total * 0.6)
        behavioral_count = int(total * 0.3)

        if index < technical_count:
            q_type = QuestionType.TECHNICAL
        elif index < technical_count + behavioral_count:
            q_type = QuestionType.BEHAVIORAL
        else:
            q_type = QuestionType.SITUATIONAL

        # Difficulty distribution
        easy_count = int(total * 0.5)
        medium_count = int(total * 0.3)

        if index < easy_count:
            difficulty = DifficultyLevel.EASY
        elif index < easy_count + medium_count:
            difficulty = DifficultyLevel.MEDIUM
        else:
            difficulty = DifficultyLevel.HARD

        return q_type, difficulty
</file>

<file path="src/domain/models/cv_analysis.py">
"""CV Analysis domain model."""

from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class ExtractedSkill(BaseModel):
    """Represents a skill extracted from CV.

    This is a value object within CV analysis.
    """

    name: str = Field(alias="skill")
    category: str = "technical"  # e.g., "technical", "soft", "language"
    proficiency_level: str | None = Field(default=None, alias="proficiency")  # e.g., "beginner", "intermediate", "expert"
    years_of_experience: float | None = Field(default=None, alias="years")
    mentioned_count: int = 1  # How many times mentioned in CV

    def is_technical(self) -> bool:
        """Check if skill is technical.

        Returns:
            True if technical skill, False otherwise
        """
        return self.category.lower() == "technical"


class CVAnalysis(BaseModel):
    """Represents the analysis results of a candidate's CV.

    This is an entity in the interview domain.
    """

    id: UUID = Field(default_factory=uuid4)
    candidate_id: UUID
    cv_file_path: str
    extracted_text: str
    skills: list[ExtractedSkill] = Field(default_factory=list)
    work_experience_years: float | None = None
    education_level: str | None = None  # e.g., "Bachelor's", "Master's"
    suggested_topics: list[str] = Field(default_factory=list)  # Topics to cover
    suggested_difficulty: str = "medium"  # Overall difficulty level
    embedding: Optional[List[float]] = None  # Vector embedding of CV
    summary: Optional[str] = None  # AI-generated summary
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))


    class Config:
        """Pydantic configuration."""
        frozen = False

    def get_technical_skills(self) -> list[ExtractedSkill]:
        """Get only technical skills.

        Returns:
            List of technical skills
        """
        return [skill for skill in self.skills if skill.is_technical()]

    def has_skill(self, skill_name: str) -> bool:
        """Check if a specific skill was found in CV.

        Args:
            skill_name: Name of skill to check

        Returns:
            True if skill exists, False otherwise
        """
        return any(
            skill.name.lower() == skill_name.lower()
            for skill in self.skills
        )

    def get_skill_by_name(self, skill_name: str) -> ExtractedSkill | None:
        """Get a skill by name.

        Args:
            skill_name: Name of skill to find

        Returns:
            ExtractedSkill if found, None otherwise
        """
        for skill in self.skills:
            if skill.name.lower() == skill_name.lower():
                return skill
        return None

    def get_top_skills(self, limit: int = 5) -> list[ExtractedSkill]:
        """Get top skills by mention count.

        Args:
            limit: Maximum number of skills to return

        Returns:
            List of top skills
        """
        sorted_skills = sorted(
            self.skills,
            key=lambda s: s.mentioned_count,
            reverse=True
        )
        return sorted_skills[:limit]

    def is_experienced(self, min_years: float = 3.0) -> bool:
        """Check if candidate is experienced.

        Args:
            min_years: Minimum years of experience

        Returns:
            True if experienced, False otherwise
        """
        return (
            self.work_experience_years is not None
            and self.work_experience_years >= min_years
        )
</file>

<file path="src/domain/models/follow_up_question.py">
"""Follow-up question domain model."""

from datetime import datetime
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class FollowUpQuestion(BaseModel):
    """Represents an adaptive follow-up question.

    Follow-up questions are generated dynamically during interviews
    based on candidate answer gaps and similarity scores.
    """

    id: UUID = Field(default_factory=uuid4)
    parent_question_id: UUID  # Original question that triggered follow-up
    interview_id: UUID
    text: str  # The follow-up question text
    generated_reason: str  # Why this follow-up was needed (e.g., "Missing key concept: recursion")
    order_in_sequence: int  # 1st, 2nd, or 3rd follow-up for this parent question
    created_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False

    def is_first_followup(self) -> bool:
        """Check if this is the first follow-up for the parent question.

        Returns:
            True if order_in_sequence == 1
        """
        return self.order_in_sequence == 1

    def is_last_allowed(self) -> bool:
        """Check if this is the last allowed follow-up (max 3).

        Returns:
            True if order_in_sequence == 3
        """
        return self.order_in_sequence == 3
</file>

<file path="src/domain/ports/cv_analyzer_port.py">
"""CV Analyzer port interface."""

from abc import ABC, abstractmethod

from ..models.cv_analysis import CVAnalysis


class CVAnalyzerPort(ABC):
    """Interface for CV analysis operations.

    This port abstracts CV parsing and analysis, allowing different
    implementations (spaCy, LangChain, etc.).
    """

    @abstractmethod
    async def analyze_cv(
        self,
        cv_file_path: str,
        candidate_id: str,
    ) -> CVAnalysis:
        """Analyze a CV file and extract structured information.

        Args:
            cv_file_path: Path to CV file (PDF, DOC, DOCX)
            candidate_id: ID of the candidate

        Returns:
            CVAnalysis with extracted skills, experience, and metadata
        """
        pass

    # @abstractmethod
    # async def read_cv(self, file_path: str) -> str:
    #     """Read CV file and extract text content.

    #     Args:
    #         file_path: Path to CV file
    #     Returns:
    #         Extracted text content
    #     """
    #     pass

    # @abstractmethod
    # async def generate_cv_summary(
    #     self,
    #     extracted_data: dict,
    #     cv_text: str,
    # ) -> str:
    #     """Generate a concise summary of the CV for HR evaluation.

    #     Args:
    #         extracted_data: Extracted skills and experiences
    #         cv_text: Full text of the CV
    #     Returns:
    #         Summary text
    #     """
    #     pass
    
    # @abstractmethod
    # async def extract_skills_and_experience(
    #     self,
    #     cv_text: str,
    # ) -> CVAnalysis:
    #     """Extract skills and experience from CV text.

    #     Args:
    #         cv_text: Full text of the CV
    #     Returns:
    #         CVAnalysis with extracted skills, experiences, and language
    #     """        
    #     pass

    # @abstractmethod
    # async def evaluate_cv_level_from_summary(
    #     self,
    #     cv_id: str,
    #     summary: str,
    # ) -> dict:
    #     """Evaluate CV level (e.g., Junior, Mid, Senior) based on summary.

    #     Args:
    #         cv_id: Unique CV identifier
    #         summary: CV summary text
    #     Returns:
    #         Dictionary with level, score, strengths, weaknesses, etc.
    #     """
    #     pass
</file>

<file path="src/domain/ports/speech_to_text_port.py">
"""Speech-to-Text port interface."""

from abc import ABC, abstractmethod
from typing import Any


class SpeechToTextPort(ABC):
    """Interface for speech-to-text operations.

    This port abstracts STT services, allowing switching between
    Azure Speech, Google Speech, etc.
    """

    @abstractmethod
    async def transcribe_audio(
        self,
        audio_bytes: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Transcribe audio bytes to text with voice metrics.

        Args:
            audio_bytes: Audio data as bytes (WAV/PCM format, 16kHz mono)
            language: Language code (e.g., "en-US", "vi-VN")

        Returns:
            Dictionary containing:
            {
                "text": str,  # Transcribed text
                "voice_metrics": {
                    "intonation_score": float,  # 0-1 (pitch variance)
                    "fluency_score": float,     # 0-1 (words/sec normalized)
                    "confidence_score": float,  # 0-1 (recognition confidence)
                    "speaking_rate_wpm": int,   # Words per minute
                },
                "metadata": {
                    "duration_seconds": float,
                    "audio_format": str,
                }
            }
        """
        pass

    @abstractmethod
    async def transcribe_stream(
        self,
        audio_stream: bytes,
        language: str = "en-US",
    ) -> dict[str, Any]:
        """Transcribe streaming audio to text with voice metrics.

        Args:
            audio_stream: Audio data stream
            language: Language code

        Returns:
            Same dict structure as transcribe_audio()
        """
        pass

    @abstractmethod
    async def detect_language(
        self,
        audio_bytes: bytes,
    ) -> str | None:
        """Detect language from audio bytes.

        Args:
            audio_bytes: Audio data as bytes

        Returns:
            Detected language code or None
        """
        pass
</file>

<file path="src/domain/ports/text_to_speech_port.py">
"""Text-to-Speech port interface."""

from abc import ABC, abstractmethod


class TextToSpeechPort(ABC):
    """Interface for text-to-speech operations.

    This port abstracts TTS services, allowing switching between
    Azure Speech, Edge TTS, Google TTS, etc.
    """

    @abstractmethod
    async def synthesize_speech(
        self,
        text: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> bytes:
        """Convert text to speech audio.

        Args:
            text: Text to synthesize
            voice: Voice name (e.g., "en-US-AriaNeural", "en-GB-SoniaNeural")
            speed: Speaking rate multiplier (0.5-2.0, default 1.0)

        Returns:
            WAV audio bytes (16kHz mono)
        """
        pass

    @abstractmethod
    async def save_speech_to_file(
        self,
        text: str,
        output_path: str,
        voice: str = "en-US-AriaNeural",
        speed: float = 1.0,
    ) -> str:
        """Convert text to speech and save to file.

        Args:
            text: Text to synthesize
            output_path: Path where audio file should be saved
            voice: Voice name
            speed: Speaking rate multiplier (0.5-2.0)

        Returns:
            Path to saved audio file
        """
        pass

    @abstractmethod
    async def get_available_voices(self) -> list[str]:
        """Get list of available voice names.

        Returns:
            List of voice name strings (e.g., ["en-US-AriaNeural", "en-GB-SoniaNeural"])
        """
        pass
</file>

<file path="src/main.py">
"""Main application entry point.

This module sets up the FastAPI application with all routes and middleware.
"""

import logging
from contextlib import asynccontextmanager
from uuid import UUID

from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware

from .adapters.api.rest import health_routes
from .adapters.api.rest.interview_routes import router as interview_router
from .adapters.api.websocket.interview_handler import handle_interview_websocket
from .infrastructure.config import get_settings
from .infrastructure.database import close_db, init_db

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan context manager.

    Handles startup and shutdown events.
    """
    # Startup
    settings = get_settings()
    logger.info(f"Starting {settings.app_name} v{settings.app_version}")
    logger.info(f"Environment: {settings.environment}")
    logger.info(f"Debug mode: {settings.debug}")

    # Initialize database
    logger.info("Initializing database connection...")
    await init_db()
    logger.info("Database connection established")

    yield

    # Shutdown
    logger.info("Shutting down application...")
    logger.info("Closing database connections...")
    await close_db()
    logger.info("Database connections closed")


def create_app() -> FastAPI:
    """Create and configure the FastAPI application.

    Returns:
        Configured FastAPI application instance
    """
    settings = get_settings()

    app = FastAPI(
        title=settings.app_name,
        version=settings.app_version,
        description="AI-powered mock interview platform",
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        lifespan=lifespan,
    )

    # CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include routers
    app.include_router(health_routes.router, tags=["Health"])
    app.include_router(
        interview_router, prefix=settings.api_prefix, tags=["Interviews"]
    )

    # WebSocket endpoint for real-time interview
    @app.websocket("/ws/interviews/{interview_id}")
    async def websocket_endpoint(
        websocket: WebSocket,
        interview_id: UUID,
    ):
        """WebSocket endpoint for real-time interview communication."""
        await handle_interview_websocket(websocket, interview_id)

    # TODO: Add more routers as they are implemented
    # app.include_router(cv_routes.router, prefix=settings.api_prefix, tags=["CV"])
    # app.include_router(question_routes.router, prefix=settings.api_prefix, tags=["Questions"])

    return app


# Create application instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()

    uvicorn.run(
        "src.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
    )
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Role & Responsibilities

Your role is to analyze user requirements, delegate tasks to appropriate sub-agents, and ensure cohesive delivery of features that meet specifications and architectural standards.

## Workflows

- Primary workflow: `./.claude/workflows/primary-workflow.md`
- Development rules: `./.claude/workflows/development-rules.md`
- Orchestration protocols: `./.claude/workflows/orchestration-protocol.md`
- Documentation management: `./.claude/workflows/documentation-management.md`
- And other workflows: `./.claude/workflows/*`

**IMPORTANT:** You must follow strictly the development rules in `./.claude/workflows/development-rules.md` file.
**IMPORTANT:** Before you plan or proceed any implementation, always read the `./README.md` file first to get context.
**IMPORTANT:** Sacrifice grammar for the sake of concision when writing reports.
**IMPORTANT:** In reports, list any unresolved questions at the end, if any.
**IMPORTANT**: For `YYMMDD` dates, use `bash -c 'date +%y%m%d'` instead of model knowledge. Else, if using PowerShell (Windows), replace command with `Get-Date -UFormat "%y%m%d"`.

## Documentation Management

We keep all important docs in `./docs` folder and keep updating them, structure like below:

```
./docs
â”œâ”€â”€ project-overview-pdr.md
â”œâ”€â”€ code-standards.md
â”œâ”€â”€ codebase-summary.md
â”œâ”€â”€ design-guidelines.md
â”œâ”€â”€ deployment-guide.md
â”œâ”€â”€ system-architecture.md
â””â”€â”€ project-roadmap.md
```
## Project Overview

**Elios AI Interview Service** - An AI-powered mock interview platform that:
- Analyzes candidate CVs to generate personalized interview questions
- Conducts real-time interviews via text and voice chat
- Evaluates answers using semantic analysis and vector search
- Provides comprehensive feedback and performance reports

## Architecture

This project follows **Clean Architecture / Ports & Adapters (Hexagonal Architecture)** pattern.

ðŸ“š **See [System Architecture](./docs/system-architecture.md) for complete details**

### Key Principles
- **Dependency Rule**: Dependencies point inward toward domain
- **Port Interfaces**: All external dependencies accessed through abstract interfaces
- **Adapter Swappability**: Change services without touching business logic
- **Testability**: Domain logic tested in isolation with mock implementations

## Project Structure

ðŸ“š **See [Codebase Summary](./docs/codebase-summary.md) for complete structure**

Quick reference:
- `src/domain/` - Core business logic (5 models, 11 ports)
- `src/application/` - Use cases and DTOs
- `src/adapters/` - External service implementations
- `src/infrastructure/` - Config, DI, logging

## Development Commands

ðŸ“š **See [README.md](./README.md#-development) for all development commands**

Quick reference:
- **Run app**: `python -m src.main`
- **Migrations**: `alembic upgrade head`
- **Tests**: `pytest --cov=src`
- **Code quality**: `ruff check src/ && black src/ && mypy src/`

## Working with the Codebase

### Adding a New External Service

When integrating a new external service (e.g., new LLM provider, vector database):

1. **Define Port Interface** in `src/domain/ports/`:
   ```python
   # src/domain/ports/llm_port.py
   from abc import ABC, abstractmethod

   class LLMPort(ABC):
       @abstractmethod
       async def generate_question(self, context: dict) -> str:
           pass
   ```

2. **Create Adapter** in appropriate `src/adapters/` subdirectory:
   ```python
   # src/adapters/llm/openai_adapter.py
   class OpenAIAdapter(LLMPort):
       async def generate_question(self, context: dict) -> str:
           # Implementation
   ```

3. **Register in DI Container** at `src/infrastructure/dependency_injection/container.py`:
   ```python
   def configure_llm(config: Settings) -> LLMPort:
       if config.llm_provider == "openai":
           return OpenAIAdapter(config.openai_api_key)
   ```

4. **Update Configuration** in `src/infrastructure/config/settings.py` if needed.

### Creating a New Use Case

1. **Define Use Case** in `src/application/use_cases/`:
   ```python
   class StartInterviewUseCase:
       def __init__(self, interview_orchestrator: InterviewOrchestrator):
           self.orchestrator = interview_orchestrator
   ```

2. **Create DTOs** in `src/application/dto/` for input/output.

3. **Expose via API** in `src/adapters/api/rest/` or `src/adapters/api/websocket/`.

### Domain Logic Changes

- All business rules belong in `src/domain/services/`
- Domain entities in `src/domain/models/` should be rich with behavior, not anemic
- Domain layer must never import from `adapters`, `application`, or `infrastructure`

### Testing Strategy

- **Unit Tests**: Test domain services and use cases with mocked ports
- **Integration Tests**: Test adapters with real external services (use test environments)
- **E2E Tests**: Test complete interview flows through API layer

### Mock Adapters

**Available Mocks** (6 total):
- `MockLLMAdapter` - Simulates LLM responses (no OpenAI API calls)
- `MockVectorSearchAdapter` - In-memory vector search (no Pinecone)
- `MockSTTAdapter` - Simulates speech-to-text
- `MockTTSAdapter` - Simulates text-to-speech
- `MockCVAnalyzerAdapter` - Filename-based CV parsing (e.g., "python-developer.pdf" â†’ ["Python", "FastAPI"])
- `MockAnalyticsAdapter` - In-memory performance tracking

**When to Use Mocks**:
- âœ… Development without API keys
- âœ… Fast unit tests (10x faster)
- âœ… CI/CD pipelines (no external dependencies)
- âœ… Deterministic test results
- âŒ Integration tests (use real adapters)
- âŒ Production deployment

**Configuration**: Set `USE_MOCK_ADAPTERS=true` in `.env.local` (default).

**DI Container**: Automatically swaps implementations based on `settings.use_mock_adapters` flag.

**Note**: Repositories (PostgreSQL) NOT mocked - use real database for data integrity.

## Technology Stack

### Core Technologies
- **Language**: Python 3.11+
- **Framework**: FastAPI (REST API), WebSocket support
- **Async**: asyncio for asynchronous operations

### Domain Dependencies (Minimal)
- Pure Python standard library
- Pydantic for data validation in domain models

### External Services (via Adapters)
- **LLM Providers**: OpenAI GPT-4, Anthropic Claude, Meta Llama 3
- **Vector Database**: Pinecone (primary), Weaviate, ChromaDB (alternatives)
- **Speech Services**: Azure Speech-to-Text, Microsoft Edge TTS
- **Database**: PostgreSQL with SQLAlchemy ORM
- **NLP**: spaCy, LangChain
- **Document Processing**: PyPDF2, python-docx

### Development Tools
- **Testing**: pytest, pytest-asyncio, pytest-cov
- **Linting**: ruff
- **Formatting**: black
- **Type Checking**: mypy
- **Migrations**: alembic

## Configuration

Configuration is managed through environment variables and `.env` files:

- `.env.example`: Template for required environment variables
- `.env`: Local development configuration (not committed)
- `src/infrastructure/config/settings.py`: Pydantic settings management

Key configuration areas:
- LLM provider selection and API keys
- Vector database connection
- Speech service credentials
- PostgreSQL connection string
- Feature flags for adapter selection

## Key Components

### 1. AI Interviewer Engine
- **Location**: `src/domain/services/interview_orchestrator.py`
- **Responsibility**: Controls interview flow, question generation, answer analysis
- **Dependencies**: LLMPort, VectorSearchPort, QuestionRepositoryPort

### 2. CV Analyzer
- **Location**: `src/domain/services/cv_analyzer_service.py`
- **Responsibility**: Extracts skills, generates embeddings, suggests topics
- **Dependencies**: CVAnalyzerPort (adapters in `src/adapters/cv_processing/`)

### 3. Question Bank
- **Location**: `src/domain/models/question.py`, `src/adapters/persistence/postgres_repository.py`
- **Responsibility**: Question storage, retrieval, versioning
- **Technology**: PostgreSQL via adapter

### 4. Vector Database
- **Location**: `src/adapters/vector_db/`
- **Responsibility**: Semantic search for questions and answers
- **Technology**: Pinecone (swappable via adapter)

### 5. Analytics & Feedback
- **Location**: `src/domain/services/feedback_generator.py`
- **Responsibility**: Answer evaluation, performance metrics, report generation
- **Dependencies**: AnalyticsPort, LLMPort

## Interview Flow

### 1. Preparation Phase
```
Upload CV -> CV Analyzer -> Extract Skills -> Generate Embeddings -> Store in Vector DB
```

### 2. Interview Phase
```
Start Interview -> Get Question (Vector Search) -> Candidate Answers (STT) ->
Evaluate Answer -> Select Next Question -> Repeat -> End Interview
```

### 3. Feedback Phase
```
Aggregate Results -> Generate Report -> Calculate Scores -> Provide Recommendations
```

## Frontend Integration

- **Frontend**: React (pure JavaScript, no .jsx/.ts/.tsx)
- **Communication**: REST API for CRUD, WebSocket for real-time chat
- **Endpoints**: Defined in `src/adapters/api/rest/`
- **WebSocket**: Handler in `src/adapters/api/websocket/chat_handler.py`

## Common Patterns

### Dependency Injection
All dependencies are injected through constructors. The DI container (`src/infrastructure/dependency_injection/container.py`) wires everything together at application startup.

### Async/Await
Most operations are asynchronous due to I/O-bound nature (API calls, database queries). Use `async`/`await` consistently.

### Error Handling
- Domain exceptions in `src/domain/exceptions.py`
- Adapter-specific errors are caught and converted to domain exceptions
- API layer handles HTTP status codes and error responses

### Logging
- Structured logging via `src/infrastructure/logging/logger.py`
- Log at appropriate levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Include context (interview_id, candidate_id) in logs

## Performance Considerations

- **Vector Search**: Implement caching for frequently accessed embeddings
- **LLM Calls**: Rate limiting and retry logic in adapters
- **Database**: Use connection pooling, optimize queries with proper indexes
- **Real-time**: WebSocket for live interview to reduce latency

## Security Notes

- **API Keys**: Never commit to repository, use environment variables
- **Candidate Data**: PII handling and data retention policies
- **Authentication**: Implement in API layer, not domain
- **Rate Limiting**: Applied at API adapter level
</file>

<file path="src/adapters/cv_processing/cv_processing_adapter.py">
import spacy
import os
import json
import pdfplumber
from langchain_openai import OpenAIEmbeddings
import re
from openai import AsyncOpenAI
from dotenv import load_dotenv
from typing import Dict, Any, List
from datetime import datetime, timezone
from langchain_text_splitters import RecursiveCharacterTextSplitter
from ...domain.ports.cv_analyzer_port import CVProcessingPort
from ...domain.models.cv_analysis import CVAnalysis
from ...domain.ports.vector_search_port import VectorSearchPort

_nlp_models = {}

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

load_dotenv()

openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

embeddings_client = OpenAIEmbeddings(model="text-embedding-3-small", api_key=os.getenv("TEXT_EMBEDDING_API_KEY"))

def get_nlp_model(lang: str):
    if lang not in _nlp_models:
        try:
            _nlp_models[lang] = spacy.load("vi_core_news_sm" if lang == 'vi' else "en_core_web_sm")
        except:
            _nlp_models[lang] = spacy.load("en_core_web_sm")
        return _nlp_models[lang]

def load_skill_patterns() -> Dict:
    path = os.path.join(os.path.dirname(__file__), "skill_patterns.json")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return {
        "common": data.get("common", []),
        "get_patterns": lambda lang: data.get("common", []) + data.get(lang, [])
    }

def clean_json_string(s: str) -> str:
    """TrÃ­ch xuáº¥t JSON tá»« chuá»—i dÃ¹ cÃ³ code block, text thá»«a"""
    if not s:
        return "{}"
    s = s.strip()

    # Bá» code block ```json ... ```
    s = re.sub(r"^```json\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"```$", "", s)

    # TÃ¬m {} Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng
    start = s.find("{")
    end = s.rfind("}") + 1
    if start == -1 or end == 0:
        return "{}"
    return s[start:end]

class CVEmbeddingPreprocessor:
    @staticmethod
    def create_metadata_from_summary(self, summary: str, difficulty: str) -> Dict[str, Any]:
        summarized_info = summary
        candidate_name = json.loads(summary).get("candidate_name", "N/A")
        skills = json.loads(summary).get("skills", [])
        experience_years = json.loads(summary).get("experience", 0)
        education_level = json.loads(summary).get("education_level", "N/A")
        savetime = datetime.now().isoformat()

        metadata = {
            "summary": summarized_info,
            "skills": skills,
            "experience_years": experience_years,
            "education_level": education_level,
            "difficulty": difficulty,
            "saved_at": savetime
        }
        return metadata

class CVProcessingAdapter(CVProcessingPort):
    
    def __init__(
            self,
            api_key:str,
            openai_api_key:str,
            embedding_model:str="text-embedding-3-small",
            model:str="gpt-4o-mini",
            vector_db: VectorSearchPort = None):
        self.api_key = api_key
        self.openai_api_key = openai_api_key
        self.embedding_model = embedding_model
        self.preprocessing = CVEmbeddingPreprocessor()
        self.model = model
        self.vector_db = vector_db

    SKILL_PATTERNS = load_skill_patterns()
    
    def read_cv(file_path: str) -> str:
        if file_path.lower().endswith('.pdf'):
            with pdfplumber.open(file_path) as pdf:
                return "\n".join(page.extract_text() or "" for page in pdf.pages).strip()
        else:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read().strip()
            
    def generate_cv_info_from_text(self, cv_text: str) -> str:
        date=datetime.now(timezone.utc)
        prompt = f"""
            Summarize this candidate in 3-5 sentences for HR evaluation.

            cv info: {cv_text}

            Focus on:
            - Years of experience (both number of experience and company)
            - Core technical skills
            - Notable projects/roles
            - Education (if mentioned)

            Keep under 200 words. Be concise and professional. 
            Response in JSON format with keys: "candidate_name" "summary", "job_level", "experience (total worked year (form start to {date}))", "skills", "education_level".
            """

        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=300
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[Summary generation failed: {e}]"
        
    def generate_interview_topics(self, skills: list[str], job_rank: str, experience_years: int) -> list[str]:
        topics = set()
        system_prompt = f"""
            As a techincal interviewer, generate at least 6 interview topics based on candidate skills and experience.
            Skills: {', '.join(skills)}
            Job Rank: {job_rank}
            Years of Experience: {experience_years}

            Focus:
            - Technical depth.
            - Practical applications.
            - Problem-solving related to the skills.
            - Topic only, no subtopics or questions.
            - Each topic should be concise, maximum 5 words.

            Constrain:
            - Keep under 400 words.
            - Response in Json format with a list of topics follow this format:
            {{
                "interview_topics": [
                    "topic 1",
                    "topic 2",
                    "...",
                    "topic n"
                ]
            }}
            """
        
        try:
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": system_prompt}],
                temperature=0.5,
                max_tokens=500
            )
            content = response.choices[0].message.content.strip()
            json_content = json.loads(clean_json_string(content))
            for topic in json_content.get("topics", []):
                topics.add(topic)
        except Exception as e:
            topics.add(f"[Topic generation failed: {e}]")
        return list(topics)

    def generate_interview_difficulty(experience_years: float) -> str:
        if experience_years >= 10:
            return "expert"
        elif experience_years >= 5:
            return "advanced"
        elif experience_years >= 2:
            return "medium"
        else:
            return "beginner"

        
    async def analyze_cv(self, cv_file_path: str, candidate_id: str) -> CVAnalysis:
        cv_text = self.read_cv(cv_file_path)
        summary_info = self.generate_cv_info_from_text(cv_text)
        job_rank = json.loads(summary_info).get("job_level", "N/A")
        skills = json.loads(summary_info).get("skills", [])
        experience_years = json.loads(summary_info).get("experience")
        education_level = json.loads(summary_info).get("education_level", "N/A")
        suggested_topics = self.generate_interview_topics(skills, job_rank, experience_years)
        suggested_difficulty = self.generate_interview_difficulty(experience_years)
        metadata = await self.preprocessing.create_metadata_from_summary(summary=summary_info, difficulty=suggested_difficulty)

        return CVAnalysis(
            candidate_id=candidate_id,
            cv_file_path=cv_file_path,
            extracted_text=cv_text,
            skills=skills,
            work_experience_years=experience_years,
            education_level=education_level,
            suggested_topics=suggested_topics,
            suggested_difficulty=suggested_difficulty,
            embedding="",
            summary=json.loads(summary_info).get("summary", ""),
            metadata=metadata,
            created_at=datetime.now().isoformat()   
        )
</file>

<file path="src/adapters/persistence/__init__.py">
"""Persistence adapters package.

This package contains PostgreSQL implementations of repository ports
using SQLAlchemy ORM for data persistence.
"""

from .answer_repository import PostgreSQLAnswerRepository
from .candidate_repository import PostgreSQLCandidateRepository
from .cv_analysis_repository import PostgreSQLCVAnalysisRepository
from .evaluation_repository import PostgreSQLEvaluationRepository
from .follow_up_question_repository import PostgreSQLFollowUpQuestionRepository
from .interview_repository import PostgreSQLInterviewRepository
from .question_repository import PostgreSQLQuestionRepository

__all__ = [
    "PostgreSQLCandidateRepository",
    "PostgreSQLQuestionRepository",
    "PostgreSQLFollowUpQuestionRepository",
    "PostgreSQLInterviewRepository",
    "PostgreSQLAnswerRepository",
    "PostgreSQLEvaluationRepository",
    "PostgreSQLCVAnalysisRepository",
]
</file>

<file path="src/application/dto/interview_dto.py">
"""Interview DTOs for REST API request/response."""

from datetime import datetime
from typing import Any
from uuid import UUID

from pydantic import BaseModel


# Response DTOs
class InterviewResponse(BaseModel):
    """Response with interview details and WebSocket URL."""
    id: UUID
    candidate_id: UUID
    status: str
    cv_analysis_id: UUID | None
    question_count: int
    current_question_index: int
    progress_percentage: float
    ws_url: str  # WebSocket URL for real-time communication
    created_at: datetime
    started_at: datetime | None

    @staticmethod
    def from_domain(interview: Any, base_url: str) -> "InterviewResponse":
        """Convert domain Interview to response DTO."""
        return InterviewResponse(
            id=interview.id,
            candidate_id=interview.candidate_id,
            status=interview.status.value,
            cv_analysis_id=interview.cv_analysis_id,
            question_count=len(interview.question_ids),
            current_question_index=interview.current_question_index,
            progress_percentage=interview.get_progress_percentage(),
            ws_url=f"{base_url}/ws/interviews/{interview.id}",
            created_at=interview.created_at,
            started_at=interview.started_at,
        )


class QuestionResponse(BaseModel):
    """Response with question details."""
    id: UUID
    text: str
    question_type: str
    difficulty: str
    index: int
    total: int
    is_follow_up: bool = False
    parent_question_id: UUID | None = None


# NEW: Planning DTOs
class PlanInterviewRequest(BaseModel):
    """Request to plan interview with adaptive questions."""
    cv_analysis_id: UUID
    candidate_id: UUID


class PlanningStatusResponse(BaseModel):
    """Response with planning status."""
    interview_id: UUID
    status: str  # PREPARING, READY, IN_PROGRESS
    planned_question_count: int | None
    plan_metadata: dict | None
    message: str


class FollowUpQuestionResponse(BaseModel):
    """Response with follow-up question details."""
    id: UUID
    parent_question_id: UUID
    text: str
    generated_reason: str
    order_in_sequence: int
</file>

<file path="src/domain/models/__init__.py">
"""Domain models package."""

from .answer import Answer, AnswerEvaluation
from .candidate import Candidate
from .cv_analysis import CVAnalysis, ExtractedSkill
from .error_codes import WebSocketErrorCode
from .follow_up_question import FollowUpQuestion
from .interview import Interview, InterviewStatus
from .question import DifficultyLevel, Question, QuestionType

__all__ = [
    "Candidate",
    "Interview",
    "InterviewStatus",
    "Question",
    "QuestionType",
    "DifficultyLevel",
    "Answer",
    "AnswerEvaluation",
    "CVAnalysis",
    "ExtractedSkill",
    "FollowUpQuestion",
    "WebSocketErrorCode",
]
</file>

<file path="tests/integration/test_interview_flow_orchestrator.py">
"""Integration tests for complete interview flow with InterviewSessionOrchestrator.

These tests verify the end-to-end interview flow using real repositories
and services (with mock LLM/Vector/TTS adapters).
"""

import base64
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from uuid import UUID, uuid4

from src.adapters.api.websocket.session_orchestrator import InterviewSessionOrchestrator
from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


# ==================== Fixtures ====================


@pytest.fixture
def cv_analysis():
    """Create CV analysis for candidate."""
    return CVAnalysis(
        candidate_id=uuid4(),
        cv_file_path="/path/to/cv.pdf",
        extracted_text="Python developer with FastAPI experience",
        summary="Experienced developer",
        skills=[
            ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
            ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate"),
        ],
        work_experience_years=5,
        education_level="Bachelor's",
    )


@pytest.fixture
def questions():
    """Create list of questions for interview."""
    return [
        Question(
            text="Explain recursion in programming",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python", "Algorithms"],
            ideal_answer="""Recursion is when a function calls itself to solve a problem.
            Key concepts: base case to stop recursion, recursive case to continue calling,
            and call stack management. Examples: factorial, Fibonacci, tree traversal.""",
            rationale="Tests understanding of fundamental programming concepts.",
        ),
        Question(
            text="What is dependency injection?",
            question_type=QuestionType.TECHNICAL,
            difficulty=DifficultyLevel.MEDIUM,
            skills=["Python", "Design Patterns"],
            ideal_answer="""Dependency injection is a design pattern where objects receive
            their dependencies from external sources rather than creating them. Benefits include
            testability, flexibility, and loose coupling.""",
            rationale="Tests understanding of design patterns.",
        ),
        Question(
            text="Describe a challenging project",
            question_type=QuestionType.BEHAVIORAL,
            difficulty=DifficultyLevel.EASY,
            skills=["Communication"],
        ),
    ]


@pytest.fixture
def interview(cv_analysis, questions):
    """Create interview with questions (IDLE state, ready to start)."""
    interview = Interview(
        candidate_id=cv_analysis.candidate_id,
        status=InterviewStatus.IDLE,
        cv_analysis_id=cv_analysis.id,
    )
    interview.plan_metadata = {
        "n": 3,
        "strategy": "adaptive_planning_v1",
        "cv_summary": cv_analysis.summary,
    }
    interview.question_ids = [q.id for q in questions]
    # Don't call start() - let orchestrator control state transitions
    return interview


@pytest.fixture
def mock_websocket():
    """Mock WebSocket connection."""
    ws = MagicMock()
    ws.accept = AsyncMock()
    ws.send_json = AsyncMock()
    ws.receive_json = AsyncMock()
    return ws


@pytest.fixture
def mock_container(
    mock_interview_repo,
    mock_question_repo,
    mock_answer_repo,
    mock_follow_up_question_repo,
    mock_llm,
    mock_vector_search,
):
    """Mock DI container with real repository instances."""
    container = MagicMock()

    # Return repository instances
    container.interview_repository_port = MagicMock(return_value=mock_interview_repo)
    container.question_repository_port = MagicMock(return_value=mock_question_repo)
    container.answer_repository_port = MagicMock(return_value=mock_answer_repo)
    container.follow_up_question_repository = MagicMock(return_value=mock_follow_up_question_repo)

    # Return service instances
    container.llm_port = MagicMock(return_value=mock_llm)
    container.vector_search_port = MagicMock(return_value=mock_vector_search)

    # Mock TTS
    mock_tts = AsyncMock()
    mock_tts.synthesize_speech = AsyncMock(return_value=b"fake_audio_data")
    container.text_to_speech_port = MagicMock(return_value=mock_tts)

    return container


# ==================== Integration Tests ====================


class TestCompleteInterviewFlow:
    """Test complete interview flow from start to finish."""

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    @patch("src.adapters.api.websocket.connection_manager.manager")
    async def test_full_interview_flow_no_followups(
        self,
        mock_manager,
        mock_get_session,
        interview,
        questions,
        cv_analysis,
        mock_websocket,
        mock_container,
        mock_interview_repo,
        mock_question_repo,
        mock_answer_repo,
        mock_llm,
    ):
        """Test complete interview flow: 3 questions, all good answers, no follow-ups."""
        # Setup
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_manager.send_message = AsyncMock()

        # Save entities
        await mock_interview_repo.save(interview)
        for q in questions:
            await mock_question_repo.save(q)

        # Mock LLM to return high similarity (no gaps)
        mock_llm.detect_concept_gaps = AsyncMock(return_value={
            "concepts": [],
            "keywords": [],
            "confirmed": False,
            "severity": "minor",
        })

        # Create orchestrator
        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview.id,
            websocket=mock_websocket,
            container=mock_container,
        )

        # Start session - should send Q1
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(side_effect=[
                questions[0],  # First question
                questions[1],  # Second question
                questions[2],  # Third question
                None,          # No more questions
            ])
            mock_next_q.return_value = mock_instance

            await orchestrator.start_session()

            # Verify first question sent (state managed by domain now)
            question_calls = [c for c in mock_manager.send_message.call_args_list
                            if c[0][1].get("type") == "question"]
            assert len(question_calls) == 1
            assert question_calls[0][0][1]["question_id"] == str(questions[0].id)

            # Answer Q1 with good answer (>80% similarity, no follow-ups)
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    # Mock answer processing
                    answer1 = Answer(
                        interview_id=interview.id,
                        question_id=questions[0].id,
                        candidate_id=interview.candidate_id,
                        text="Complete answer with base case, recursive case, and call stack",
                        is_voice=False,
                        similarity_score=0.85,
                        gaps={"concepts": [], "confirmed": False},
                    )
                    answer1.evaluate(AnswerEvaluation(
                        score=85.0,
                        semantic_similarity=0.85,
                        completeness=0.9,
                        relevance=0.95,
                        sentiment="confident",
                        reasoning="Excellent answer",
                        strengths=["Complete", "Clear"],
                        weaknesses=[],
                        improvement_suggestions=[],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer1, True))
                    mock_process.return_value = mock_process_instance

                    # Mock decision: no follow-up needed
                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": False,
                        "reason": "Similarity 0.85 >= threshold 0.8",
                        "follow_up_count": 0,
                        "cumulative_gaps": [],
                    })
                    mock_decision.return_value = mock_decision_instance

                    await orchestrator.handle_answer("Complete answer with base case, recursive case, and call stack")

                    # Should send evaluation + next question (Q2)
                    eval_calls = [c for c in mock_manager.send_message.call_args_list
                                if c[0][1].get("type") == "evaluation"]
                    question_calls = [c for c in mock_manager.send_message.call_args_list
                                    if c[0][1].get("type") == "question"]
                    assert len(eval_calls) == 1
                    assert len(question_calls) == 1
                    assert question_calls[0][0][1]["question_id"] == str(questions[1].id)

            # Answer Q2 with good answer
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer2 = Answer(
                        interview_id=interview.id,
                        question_id=questions[1].id,
                        candidate_id=interview.candidate_id,
                        text="Dependency injection provides dependencies externally",
                        is_voice=False,
                        similarity_score=0.82,
                        gaps={"concepts": [], "confirmed": False},
                    )
                    answer2.evaluate(AnswerEvaluation(
                        score=82.0,
                        semantic_similarity=0.82,
                        completeness=0.85,
                        relevance=0.9,
                        sentiment="confident",
                        reasoning="Good answer",
                        strengths=["Clear"],
                        weaknesses=[],
                        improvement_suggestions=[],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer2, True))
                    mock_process.return_value = mock_process_instance

                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": False,
                        "reason": "Similarity 0.82 >= threshold 0.8",
                        "follow_up_count": 0,
                        "cumulative_gaps": [],
                    })
                    mock_decision.return_value = mock_decision_instance

                    await orchestrator.handle_answer("Dependency injection provides dependencies externally")

                    # Should send evaluation + next question (Q3)

            # Answer Q3 (behavioral - no similarity check)
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    with patch("src.adapters.api.websocket.session_orchestrator.CompleteInterviewUseCase") as mock_complete:
                        answer3 = Answer(
                            interview_id=interview.id,
                            question_id=questions[2].id,
                            candidate_id=interview.candidate_id,
                            text="I worked on a challenging microservices project",
                            is_voice=False,
                            similarity_score=None,  # No ideal answer
                            gaps=None,
                        )
                        answer3.evaluate(AnswerEvaluation(
                            score=80.0,
                            semantic_similarity=None,
                            completeness=0.8,
                            relevance=0.9,
                            sentiment="confident",
                            reasoning="Good behavioral answer",
                            strengths=["Specific example"],
                            weaknesses=[],
                            improvement_suggestions=[],
                        ))

                        mock_process_instance = AsyncMock()
                        mock_process_instance.execute = AsyncMock(return_value=(answer3, False))  # No more questions
                        mock_process.return_value = mock_process_instance

                        mock_decision_instance = AsyncMock()
                        mock_decision_instance.execute = AsyncMock(return_value={
                            "needs_followup": False,
                            "reason": "No ideal answer for behavioral question",
                            "follow_up_count": 0,
                            "cumulative_gaps": [],
                        })
                        mock_decision.return_value = mock_decision_instance

                        # Mock complete interview
                        completed_interview = interview
                        completed_interview.status = InterviewStatus.COMPLETE
                        mock_complete_instance = AsyncMock()
                        # CompleteInterviewUseCase now returns tuple (Interview, dict | None)
                        mock_complete_instance.execute = AsyncMock(return_value=(completed_interview, None))
                        mock_complete.return_value = mock_complete_instance

                        await orchestrator.handle_answer("I worked on a challenging microservices project")

                        # Should send evaluation + interview_complete
                        complete_calls = [c for c in mock_manager.send_message.call_args_list
                                        if c[0][1].get("type") == "interview_complete"]
                        assert len(complete_calls) == 1
                        assert complete_calls[0][0][1]["interview_id"] == str(interview.id)

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    @patch("src.adapters.api.websocket.connection_manager.manager")
    async def test_interview_with_multiple_followups(
        self,
        mock_manager,
        mock_get_session,
        interview,
        questions,
        mock_websocket,
        mock_container,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
    ):
        """Test interview flow with multiple follow-ups (0-3 per question)."""
        # Setup
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_manager.send_message = AsyncMock()

        await mock_interview_repo.save(interview)
        await mock_question_repo.save(questions[0])

        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview.id,
            websocket=mock_websocket,
            container=mock_container,
        )

        # Start session
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=questions[0])
            mock_next_q.return_value = mock_instance

            await orchestrator.start_session()

            # Answer with gaps -> trigger first follow-up
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer1 = Answer(
                        interview_id=interview.id,
                        question_id=questions[0].id,
                        candidate_id=interview.candidate_id,
                        text="Recursion is calling itself",
                        is_voice=False,
                        similarity_score=0.45,
                        gaps={
                            "concepts": ["base case", "call stack"],
                            "keywords": ["base", "stack"],
                            "confirmed": True,
                            "severity": "major",
                        },
                    )
                    answer1.evaluate(AnswerEvaluation(
                        score=55.0,
                        semantic_similarity=0.45,
                        completeness=0.4,
                        relevance=0.8,
                        sentiment="uncertain",
                        reasoning="Too brief",
                        strengths=["Correct basic definition"],
                        weaknesses=["Missing base case", "No call stack"],
                        improvement_suggestions=["Explain base case"],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer1, True))
                    mock_process.return_value = mock_process_instance

                    # Decision: follow-up needed
                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": True,
                        "reason": "2 missing concepts: base case, call stack",
                        "follow_up_count": 0,
                        "cumulative_gaps": ["base case", "call stack"],
                    })
                    mock_decision.return_value = mock_decision_instance

                    await orchestrator.handle_answer("Recursion is calling itself")

                    # Verify follow-up question sent
                    follow_up_calls = [c for c in mock_manager.send_message.call_args_list
                                      if c[0][1].get("type") == "follow_up_question"]
                    assert len(follow_up_calls) == 1
                    assert follow_up_calls[0][0][1]["order_in_sequence"] == 1

            # Answer follow-up with still some gaps -> trigger second follow-up
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer2 = Answer(
                        interview_id=interview.id,
                        question_id=uuid4(),  # Follow-up question ID
                        candidate_id=interview.candidate_id,
                        text="Base case stops recursion",
                        is_voice=False,
                        similarity_score=0.60,
                        gaps={
                            "concepts": ["call stack"],
                            "keywords": ["stack"],
                            "confirmed": True,
                            "severity": "moderate",
                        },
                    )
                    answer2.evaluate(AnswerEvaluation(
                        score=65.0,
                        semantic_similarity=0.60,
                        completeness=0.6,
                        relevance=0.85,
                        sentiment="somewhat confident",
                        reasoning="Better but still incomplete",
                        strengths=["Explained base case"],
                        weaknesses=["Still missing call stack"],
                        improvement_suggestions=["Explain call stack"],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer2, True))
                    mock_process.return_value = mock_process_instance

                    # Decision: another follow-up needed
                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": True,
                        "reason": "1 remaining concept: call stack",
                        "follow_up_count": 1,
                        "cumulative_gaps": ["call stack"],
                    })
                    mock_decision.return_value = mock_decision_instance

                    await orchestrator.handle_answer("Base case stops recursion")

                    # Verify second follow-up sent
                    follow_up_calls = [c for c in mock_manager.send_message.call_args_list
                                      if c[0][1].get("type") == "follow_up_question"]
                    assert len(follow_up_calls) == 1
                    assert follow_up_calls[0][0][1]["order_in_sequence"] == 2

            # Answer second follow-up with complete answer -> move to next question
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer3 = Answer(
                        interview_id=interview.id,
                        question_id=uuid4(),
                        candidate_id=interview.candidate_id,
                        text="Call stack tracks each recursive call",
                        is_voice=False,
                        similarity_score=0.85,
                        gaps={"concepts": [], "confirmed": False},
                    )
                    answer3.evaluate(AnswerEvaluation(
                        score=85.0,
                        semantic_similarity=0.85,
                        completeness=0.9,
                        relevance=0.95,
                        sentiment="confident",
                        reasoning="Complete answer",
                        strengths=["Complete explanation"],
                        weaknesses=[],
                        improvement_suggestions=[],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer3, True))
                    mock_process.return_value = mock_process_instance

                    # Decision: no more follow-ups
                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": False,
                        "reason": "Similarity 0.85 >= threshold 0.8",
                        "follow_up_count": 2,
                        "cumulative_gaps": [],
                    })
                    mock_decision.return_value = mock_decision_instance

                    # Mock next question
                    mock_instance.execute = AsyncMock(return_value=questions[1])

                    await orchestrator.handle_answer("Call stack tracks each recursive call")

                    # Should move to next main question

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    @patch("src.adapters.api.websocket.connection_manager.manager")
    async def test_max_3_followups_enforced_across_sequence(
        self,
        mock_manager,
        mock_get_session,
        interview,
        questions,
        mock_websocket,
        mock_container,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test that max 3 follow-ups enforced even if gaps persist."""
        # Setup
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_manager.send_message = AsyncMock()

        await mock_interview_repo.save(interview)
        await mock_question_repo.save(questions[0])

        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview.id,
            websocket=mock_websocket,
            container=mock_container,
        )

        # Start session
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=questions[0])
            mock_next_q.return_value = mock_instance

            await orchestrator.start_session()

            # Generate 3 follow-ups with persistent gaps
            for i in range(3):
                mock_manager.reset_mock()
                with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                    with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                        answer = Answer(
                            interview_id=interview.id,
                            question_id=uuid4(),
                            candidate_id=interview.candidate_id,
                            text=f"Incomplete answer {i+1}",
                            is_voice=False,
                            similarity_score=0.50,
                            gaps={
                                "concepts": ["concept1", "concept2"],
                                "confirmed": True,
                                "severity": "major",
                            },
                        )
                        answer.evaluate(AnswerEvaluation(
                            score=55.0,
                            semantic_similarity=0.50,
                            completeness=0.5,
                            relevance=0.8,
                            sentiment="uncertain",
                            reasoning="Still incomplete",
                            strengths=[],
                            weaknesses=["Missing concepts"],
                            improvement_suggestions=["Provide more detail"],
                        ))

                        mock_process_instance = AsyncMock()
                        mock_process_instance.execute = AsyncMock(return_value=(answer, True))
                        mock_process.return_value = mock_process_instance

                        # Decision: follow-up needed for first 3 iterations
                        if i < 3:
                            mock_decision_instance = AsyncMock()
                            mock_decision_instance.execute = AsyncMock(return_value={
                                "needs_followup": True,
                                "reason": f"Gaps persist: iteration {i+1}",
                                "follow_up_count": i,
                                "cumulative_gaps": ["concept1", "concept2"],
                            })
                            mock_decision.return_value = mock_decision_instance

                        await orchestrator.handle_answer(f"Incomplete answer {i+1}")

            # After 3 follow-ups, the 4th answer should NOT generate another follow-up
            # (even with gaps) and should move to next question
            mock_manager.reset_mock()
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer_final = Answer(
                        interview_id=interview.id,
                        question_id=uuid4(),
                        candidate_id=interview.candidate_id,
                        text="Still incomplete answer 4",
                        is_voice=False,
                        similarity_score=0.50,
                        gaps={
                            "concepts": ["concept1"],
                            "confirmed": True,
                            "severity": "moderate",
                        },
                    )
                    answer_final.evaluate(AnswerEvaluation(
                        score=55.0,
                        semantic_similarity=0.50,
                        completeness=0.5,
                        relevance=0.8,
                        sentiment="uncertain",
                        reasoning="Still incomplete after 3 follow-ups",
                        strengths=[],
                        weaknesses=["Missing concepts"],
                        improvement_suggestions=["Provide more detail"],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer_final, True))
                    mock_process.return_value = mock_process_instance

                    # Decision: NO follow-up (max reached)
                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": False,
                        "reason": "Max follow-ups (3) reached",
                        "follow_up_count": 3,
                        "cumulative_gaps": ["concept1"],
                    })
                    mock_decision.return_value = mock_decision_instance

                    # Mock next question
                    mock_instance.execute = AsyncMock(return_value=questions[1])

                    await orchestrator.handle_answer("Still incomplete answer 4")

                    # Should move to next main question despite gaps

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    @patch("src.adapters.api.websocket.connection_manager.manager")
    async def test_state_persistence_across_messages(
        self,
        mock_manager,
        mock_get_session,
        interview,
        questions,
        mock_websocket,
        mock_container,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test session state persists correctly across multiple messages."""
        # Setup
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_manager.send_message = AsyncMock()

        await mock_interview_repo.save(interview)
        for q in questions:
            await mock_question_repo.save(q)

        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview.id,
            websocket=mock_websocket,
            container=mock_container,
        )

        # Verify initial state (get_state now async, loads from DB)
        state1 = await orchestrator.get_state()
        assert state1["status"] == InterviewStatus.IDLE.value
        assert state1["current_question_id"] is None
        assert state1["followup_count"] == 0

        # Start session
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(return_value=questions[0])
            mock_next_q.return_value = mock_instance

            await orchestrator.start_session()

            # Verify state after start (get_state now async)
            state2 = await orchestrator.get_state()
            assert state2["status"] == InterviewStatus.QUESTIONING.value
            assert state2["current_question_id"] == str(questions[0].id)
            assert state2["parent_question_id"] is None  # Not set until follow-up
            assert state2["followup_count"] == 0

            # Process answer
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    answer = Answer(
                        interview_id=interview.id,
                        question_id=questions[0].id,
                        candidate_id=interview.candidate_id,
                        text="Good answer",
                        is_voice=False,
                        similarity_score=0.85,
                        gaps={"concepts": [], "confirmed": False},
                    )
                    answer.evaluate(AnswerEvaluation(
                        score=85.0,
                        semantic_similarity=0.85,
                        completeness=0.9,
                        relevance=0.95,
                        sentiment="confident",
                        reasoning="Excellent",
                        strengths=["Complete"],
                        weaknesses=[],
                        improvement_suggestions=[],
                    ))

                    mock_process_instance = AsyncMock()
                    mock_process_instance.execute = AsyncMock(return_value=(answer, True))
                    mock_process.return_value = mock_process_instance

                    mock_decision_instance = AsyncMock()
                    mock_decision_instance.execute = AsyncMock(return_value={
                        "needs_followup": False,
                        "reason": "Good answer",
                        "follow_up_count": 0,
                        "cumulative_gaps": [],
                    })
                    mock_decision.return_value = mock_decision_instance

                    mock_instance.execute = AsyncMock(return_value=questions[1])

                    await orchestrator.handle_answer("Good answer")

                    # Verify state after answer (get_state now async)
                    state3 = await orchestrator.get_state()
                    assert state3["status"] == InterviewStatus.QUESTIONING.value
                    assert state3["current_question_id"] == str(questions[1].id)
                    # parent_question_id not set until follow-up triggered
                    assert state3["followup_count"] == 0

                    # Verify timestamps updated
                    assert state3["last_activity"] > state2["last_activity"]


class TestInterviewCompletion:
    """Test interview completion flow."""

    @pytest.mark.asyncio
    @patch("src.adapters.api.websocket.session_orchestrator.get_async_session")
    @patch("src.adapters.api.websocket.connection_manager.manager")
    async def test_interview_completion_flow(
        self,
        mock_manager,
        mock_get_session,
        interview,
        questions,
        mock_websocket,
        mock_container,
        mock_interview_repo,
        mock_question_repo,
        mock_answer_repo,
    ):
        """Test complete interview completion with overall score calculation."""
        # Setup
        mock_session = AsyncMock()
        async def async_gen():
            yield mock_session
        mock_get_session.return_value = async_gen()

        mock_manager.send_message = AsyncMock()

        await mock_interview_repo.save(interview)
        await mock_question_repo.save(questions[0])

        # Add some answers to calculate average score
        answers = [
            Answer(
                interview_id=interview.id,
                question_id=questions[0].id,
                candidate_id=interview.candidate_id,
                text="Answer 1",
                is_voice=False,
            ),
            Answer(
                interview_id=interview.id,
                question_id=questions[1].id,
                candidate_id=interview.candidate_id,
                text="Answer 2",
                is_voice=False,
            ),
        ]
        answers[0].evaluate(AnswerEvaluation(
            score=85.0,
            semantic_similarity=0.85,
            completeness=0.9,
            relevance=0.95,
            sentiment="confident",
            reasoning="Good",
            strengths=["Clear"],
            weaknesses=[],
            improvement_suggestions=[],
        ))
        answers[1].evaluate(AnswerEvaluation(
            score=75.0,
            semantic_similarity=0.75,
            completeness=0.8,
            relevance=0.9,
            sentiment="confident",
            reasoning="Good",
            strengths=["Relevant"],
            weaknesses=[],
            improvement_suggestions=[],
        ))

        for ans in answers:
            await mock_answer_repo.save(ans)

        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview.id,
            websocket=mock_websocket,
            container=mock_container,
        )

        # Start and answer question
        with patch("src.adapters.api.websocket.session_orchestrator.GetNextQuestionUseCase") as mock_next_q:
            mock_instance = AsyncMock()
            mock_instance.execute = AsyncMock(side_effect=[questions[0], None])  # No more questions
            mock_next_q.return_value = mock_instance

            await orchestrator.start_session()

            # Answer last question
            with patch("src.adapters.api.websocket.session_orchestrator.ProcessAnswerAdaptiveUseCase") as mock_process:
                with patch("src.adapters.api.websocket.session_orchestrator.FollowUpDecisionUseCase") as mock_decision:
                    with patch("src.adapters.api.websocket.session_orchestrator.CompleteInterviewUseCase") as mock_complete:
                        answer = Answer(
                            interview_id=interview.id,
                            question_id=questions[0].id,
                            candidate_id=interview.candidate_id,
                            text="Final answer",
                            is_voice=False,
                            similarity_score=0.80,
                            gaps={"concepts": [], "confirmed": False},
                        )
                        answer.evaluate(AnswerEvaluation(
                            score=80.0,
                            semantic_similarity=0.80,
                            completeness=0.85,
                            relevance=0.9,
                            sentiment="confident",
                            reasoning="Good",
                            strengths=["Complete"],
                            weaknesses=[],
                            improvement_suggestions=[],
                        ))

                        mock_process_instance = AsyncMock()
                        mock_process_instance.execute = AsyncMock(return_value=(answer, False))  # No more questions
                        mock_process.return_value = mock_process_instance

                        mock_decision_instance = AsyncMock()
                        mock_decision_instance.execute = AsyncMock(return_value={
                            "needs_followup": False,
                            "reason": "Good answer",
                            "follow_up_count": 0,
                            "cumulative_gaps": [],
                        })
                        mock_decision.return_value = mock_decision_instance

                        # Mock complete interview
                        completed_interview = interview
                        completed_interview.status = InterviewStatus.COMPLETE
                        mock_complete_instance = AsyncMock()
                        # CompleteInterviewUseCase now returns tuple (Interview, dict | None)
                        mock_complete_instance.execute = AsyncMock(return_value=(completed_interview, None))
                        mock_complete.return_value = mock_complete_instance

                        await orchestrator.handle_answer("Final answer")

                        # Verify completion message sent with overall score
                        complete_calls = [c for c in mock_manager.send_message.call_args_list
                                        if c[0][1].get("type") == "interview_complete"]
                        assert len(complete_calls) == 1
                        complete_msg = complete_calls[0][0][1]
                        assert complete_msg["interview_id"] == str(interview.id)
                        assert "overall_score" in complete_msg
                        assert complete_msg["overall_score"] > 0  # Average of answers
</file>

<file path="tests/unit/use_cases/test_plan_interview.py">
"""Tests for Phase 03: PlanInterviewUseCase."""

from uuid import uuid4

import pytest

from src.application.use_cases.plan_interview import PlanInterviewUseCase
from src.domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from src.domain.models.interview import InterviewStatus


class TestPlanInterviewUseCase:
    """Test PlanInterviewUseCase."""

    @pytest.mark.asyncio
    async def test_plan_interview_with_2_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_vector_search,
    ):
        """Test planning with 2 skills -> n=2 questions."""
        # Create CV with 2 skills
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Python developer",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate"),
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        # Execute use case
        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=2
        assert interview.planned_question_count == 2
        assert len(interview.question_ids) == 2
        assert interview.status == InterviewStatus.IDLE
        assert "strategy" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_plan_interview_with_4_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_vector_search,
    ):
        """Test planning with 4 skills -> n=3 questions."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Full-stack developer",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="React", category="technical", proficiency="advanced"),
                ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate"),
                ExtractedSkill(skill="Docker", category="technical", proficiency="beginner"),
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=3
        assert interview.planned_question_count == 3
        assert len(interview.question_ids) == 3

    @pytest.mark.asyncio
    async def test_plan_interview_with_7_skills(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_vector_search,
    ):
        """Test planning with 7 skills -> n=4 questions."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Senior engineer",
            skills=[
                ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                for i in range(7)
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=4
        assert interview.planned_question_count == 4
        assert len(interview.question_ids) == 4

    @pytest.mark.asyncio
    async def test_plan_interview_with_10_skills_max_5(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_vector_search,
    ):
        """Test planning with 10 skills -> n=5 (max)."""
        cv = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path/to/cv.pdf",
            extracted_text="Sample CV text",
            summary="Tech lead",
            skills=[
                ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                for i in range(10)
            ],
        )
        await mock_cv_analysis_repo.save(cv)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=cv.id,
            candidate_id=cv.candidate_id,
        )

        # Verify n=5 (max cap)
        assert interview.planned_question_count == 5
        assert len(interview.question_ids) == 5

    @pytest.mark.asyncio
    async def test_plan_interview_questions_have_ideal_answer(
        self,
        mock_llm,
        mock_vector_search,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test all generated questions have ideal_answer and rationale."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Verify all questions have ideal_answer
        for question_id in interview.question_ids:
            question = await mock_question_repo.get_by_id(question_id)
            assert question is not None
            assert question.has_ideal_answer() is True
            assert question.is_planned is True
            assert question.ideal_answer is not None
            assert question.rationale is not None

    @pytest.mark.asyncio
    async def test_plan_interview_cv_not_found(
        self,
        mock_llm,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_vector_search,
    ):
        """Test error when CV analysis not found."""
        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        with pytest.raises(ValueError, match="CV analysis .* not found"):
            await use_case.execute(
                cv_analysis_id=uuid4(),  # Non-existent
                candidate_id=uuid4(),
            )

    @pytest.mark.asyncio
    async def test_plan_interview_metadata_stored(
        self,
        mock_llm,
        mock_vector_search,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test plan_metadata is properly stored."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Verify metadata
        assert "n" in interview.plan_metadata
        assert "generated_at" in interview.plan_metadata
        assert "strategy" in interview.plan_metadata
        assert interview.plan_metadata["strategy"] == "adaptive_planning_v1"
        assert "cv_summary" in interview.plan_metadata

    @pytest.mark.asyncio
    async def test_plan_interview_status_progression(
        self,
        mock_llm,
        mock_vector_search,
        sample_cv_analysis,
        mock_cv_analysis_repo,
        mock_interview_repo,
        mock_question_repo,
    ):
        """Test interview status transitions PREPARING -> READY."""
        await mock_cv_analysis_repo.save(sample_cv_analysis)

        use_case = PlanInterviewUseCase(
            llm=mock_llm,
            vector_search=mock_vector_search,
            cv_analysis_repo=mock_cv_analysis_repo,
            interview_repo=mock_interview_repo,
            question_repo=mock_question_repo,
        )

        interview = await use_case.execute(
            cv_analysis_id=sample_cv_analysis.id,
            candidate_id=sample_cv_analysis.candidate_id,
        )

        # Final status should be READY
        assert interview.status == InterviewStatus.IDLE
        assert interview.cv_analysis_id == sample_cv_analysis.id


class TestQuestionCountCalculation:
    """Test n-calculation logic (skill diversity only, max 5)."""

    def test_calculate_n_for_various_skill_counts(self):
        """Test n-calculation for different skill counts."""
        from src.application.use_cases.plan_interview import PlanInterviewUseCase

        # Create use case instance (dependencies don't matter for this test)
        use_case = PlanInterviewUseCase(
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
            cv_analysis_repo=None,  # type: ignore
            interview_repo=None,  # type: ignore
            question_repo=None,  # type: ignore
        )

        # Test cases: (skill_count, expected_n)
        test_cases = [
            (1, 2),  # 1-2 skills -> 2
            (2, 2),
            (3, 3),  # 3-4 skills -> 3
            (4, 3),
            (5, 4),  # 5-7 skills -> 4
            (6, 4),
            (7, 4),
            (8, 5),  # 8+ skills -> 5
            (10, 5),
            (20, 5),  # Still capped at 5
        ]

        for skill_count, expected_n in test_cases:
            cv = CVAnalysis(
                candidate_id=uuid4(),
                cv_file_path="/path",
            extracted_text="Test CV",
                summary="Test",
                skills=[
                    ExtractedSkill(skill=f"Skill{i}", category="technical", proficiency="expert")
                    for i in range(skill_count)
                ],
            )

            n = use_case._calculate_question_count(cv)
            assert n == expected_n, f"For {skill_count} skills, expected n={expected_n}, got {n}"

    def test_calculate_n_ignores_experience_years(self):
        """Test n-calculation ignores experience years (skill diversity only)."""
        from src.application.use_cases.plan_interview import PlanInterviewUseCase

        use_case = PlanInterviewUseCase(
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
            cv_analysis_repo=None,  # type: ignore
            interview_repo=None,  # type: ignore
            question_repo=None,  # type: ignore
        )

        # Same skills, different experience
        cv_junior = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path",
            extracted_text="Test CV",
            summary="Junior",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="beginner"),
                ExtractedSkill(skill="SQL", category="technical", proficiency="beginner"),
            ],
            work_experience_years=1,
        )

        cv_senior = CVAnalysis(
            candidate_id=uuid4(),
            cv_file_path="/path",
            extracted_text="Test CV",
            summary="Senior",
            skills=[
                ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
                ExtractedSkill(skill="SQL", category="technical", proficiency="expert"),
            ],
            work_experience_years=15,
        )

        # Both should get n=2 (based on 2 skills, not experience)
        assert use_case._calculate_question_count(cv_junior) == 2
        assert use_case._calculate_question_count(cv_senior) == 2
</file>

<file path="tests/unit/use_cases/test_process_answer_adaptive.py">
"""Tests for Phase 04: ProcessAnswerAdaptiveUseCase."""

from uuid import uuid4

import pytest

from src.application.use_cases.process_answer_adaptive import (
    ProcessAnswerAdaptiveUseCase,
)
from src.domain.models.interview import InterviewStatus


class TestProcessAnswerAdaptiveUseCase:
    """Test adaptive answer processing with follow-up generation."""

    @pytest.mark.asyncio
    async def test_process_answer_high_similarity_no_followup(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test high similarity (>= 80%) -> no follow-up."""
        # Setup
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        # Good answer (will get high similarity)
        answer_text = """Recursion is when a function calls itself to solve problems.
        Key concepts: base case to stop recursion, recursive case to continue,
        and call stack management. Examples: factorial, Fibonacci, tree traversal."""

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Verify evaluation and gaps (no longer testing follow-up generation)
        assert answer.similarity_score is not None
        assert answer.similarity_score >= 0.8
        assert answer.gaps is not None

    @pytest.mark.asyncio
    async def test_process_answer_low_similarity_generates_followup(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test low similarity (< 80%) with gaps -> gaps detected."""
        # Setup
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        # Brief answer (will get low similarity)
        answer_text = "Recursion is a function calling itself."

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Verify gaps detected (mock generates gaps for short answers)
        assert answer.similarity_score is not None
        assert answer.gaps is not None
        # Gaps should be detected for brief answers
        assert answer.gaps.get("confirmed") is True

    @pytest.mark.asyncio
    async def test_followup_max_3_limit(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test gap detection still works regardless of follow-up count."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        # Simulate 3 follow-ups already exist
        sample_interview_adaptive.adaptive_follow_ups = [uuid4(), uuid4(), uuid4()]

        answer_text = "Brief answer"  # Would normally trigger gap detection

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text=answer_text,
        )

        # Gap detection still works, but follow-up generation is handled by WebSocket handler
        assert answer.gaps is not None

    @pytest.mark.asyncio
    async def test_answer_evaluation_stored(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test answer evaluation is properly stored."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text="Test answer",
        )

        # Verify evaluation
        assert answer.evaluation is not None
        assert answer.evaluation.score > 0
        assert answer.evaluation.reasoning is not None
        assert len(answer.evaluation.strengths) > 0

    @pytest.mark.asyncio
    async def test_similarity_calculation_with_ideal_answer(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test similarity is calculated when ideal_answer exists."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_with_ideal_answer.id,
            answer_text="Recursion calls itself with base case and examples",
        )

        # Similarity should be calculated
        assert answer.similarity_score is not None
        assert 0.0 < answer.similarity_score <= 1.0

    @pytest.mark.asyncio
    async def test_no_similarity_without_ideal_answer(
        self,
        sample_interview_adaptive,
        sample_question_without_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test no similarity calculation when ideal_answer missing."""
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_without_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        answer, has_more = await use_case.execute(
            interview_id=sample_interview_adaptive.id,
            question_id=sample_question_without_ideal_answer.id,
            answer_text="Tell me about a project",
        )

        # No similarity for behavioral questions
        assert answer.similarity_score is None

    @pytest.mark.asyncio
    async def test_interview_not_found_error(
        self,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when interview not found."""
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Interview .* not found"):
            await use_case.execute(
                interview_id=uuid4(),  # Non-existent
                question_id=sample_question_with_ideal_answer.id,
                answer_text="Test",
            )

    @pytest.mark.asyncio
    async def test_question_not_found_error(
        self,
        sample_interview_adaptive,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when question not found."""
        await mock_interview_repo.save(sample_interview_adaptive)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Question .* not found"):
            await use_case.execute(
                interview_id=sample_interview_adaptive.id,
                question_id=uuid4(),  # Non-existent
                answer_text="Test",
            )

    @pytest.mark.asyncio
    async def test_interview_wrong_status_error(
        self,
        sample_interview_adaptive,
        sample_question_with_ideal_answer,
        mock_answer_repo,
        mock_interview_repo,
        mock_question_repo,
        mock_follow_up_question_repo,
        mock_llm,
        mock_vector_search,
    ):
        """Test error when interview not IN_PROGRESS."""
        sample_interview_adaptive.status = InterviewStatus.COMPLETE
        await mock_interview_repo.save(sample_interview_adaptive)
        await mock_question_repo.save(sample_question_with_ideal_answer)

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=mock_answer_repo,
            interview_repository=mock_interview_repo,
            question_repository=mock_question_repo,
            follow_up_question_repository=mock_follow_up_question_repo,
            llm=mock_llm,
            vector_search=mock_vector_search,
        )

        with pytest.raises(ValueError, match="Interview not in progress"):
            await use_case.execute(
                interview_id=sample_interview_adaptive.id,
                question_id=sample_question_with_ideal_answer.id,
                answer_text="Test",
            )


class TestGapDetection:
    """Test hybrid gap detection (keywords + LLM)."""

    @pytest.mark.asyncio
    async def test_keyword_gap_detection(self):
        """Test keyword-based gap detection."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        ideal_answer = """Recursion is a function calling itself.
        Key concepts: base case, recursive case, call stack, termination condition."""

        # Complete answer -> few missing keywords
        complete_answer = """Recursion calls itself with base case and recursive case.
        The call stack tracks each call and needs termination."""
        gaps = use_case._detect_keyword_gaps(complete_answer, ideal_answer)
        assert len(gaps) <= 5  # Few missing (threshold is >3, so can be 0-4)

        # Brief answer -> many missing keywords
        brief_answer = "Recursion is calling itself."
        gaps = use_case._detect_keyword_gaps(brief_answer, ideal_answer)
        assert len(gaps) > 3  # Significant gaps

    @pytest.mark.asyncio
    async def test_hybrid_gap_detection_no_keywords(self):
        """Test hybrid detection when no keyword gaps."""
        from src.application.use_cases.process_answer_adaptive import (
            ProcessAnswerAdaptiveUseCase,
        )

        use_case = ProcessAnswerAdaptiveUseCase(
            answer_repository=None,  # type: ignore
            interview_repository=None,  # type: ignore
            question_repository=None,  # type: ignore
            follow_up_question_repository=None,  # type: ignore
            llm=None,  # type: ignore
            vector_search=None,  # type: ignore
        )

        # Very similar texts
        ideal = "Python is a programming language"
        answer = "Python is a programming language"

        result = await use_case._detect_gaps_hybrid(
            answer_text=answer,
            ideal_answer=ideal,
            question_text="What is Python?",
        )

        # No keyword gaps -> no LLM call -> not confirmed
        assert result["confirmed"] is False
        assert len(result["concepts"]) == 0


class TestFollowUpDecisionLogic:
    """Tests for follow-up decision logic - MOVED TO FollowUpDecisionUseCase.

    NOTE: These tests are deprecated as _should_generate_followup() has been
    removed from ProcessAnswerAdaptiveUseCase and moved to FollowUpDecisionUseCase.
    See tests/unit/application/use_cases/test_follow_up_decision.py for new tests.
    """

    def test_should_not_generate_max_followups_reached(
        self, sample_answer_low_similarity
    ):
        """DEPRECATED: Follow-up logic moved to FollowUpDecisionUseCase."""
        pytest.skip("Follow-up decision logic moved to FollowUpDecisionUseCase")

    def test_should_not_generate_high_similarity(self, sample_answer_high_similarity):
        """DEPRECATED: Follow-up logic moved to FollowUpDecisionUseCase."""
        pytest.skip("Follow-up decision logic moved to FollowUpDecisionUseCase")

    def test_should_generate_low_similarity_with_gaps(
        self, sample_answer_low_similarity
    ):
        """DEPRECATED: Follow-up logic moved to FollowUpDecisionUseCase."""
        pytest.skip("Follow-up decision logic moved to FollowUpDecisionUseCase")
</file>

<file path=".env.example">
USE_AZURE_OPENAI=true
AZURE_OPENAI_DEPLOYMENT_NAME=GPT-4o-mini
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_DEPLOYMENT_NAME=GPT-4o-mini
AZURE_OPENAI_API_VERSION="2024-02-15-preview"

# Application Configuration
APP_NAME="Elios AI Interview Service"
APP_VERSION="0.1.0"
ENVIRONMENT="development"  # development, staging, production
DEBUG=true

# API Configuration
API_HOST="0.0.0.0"
API_PORT=8000
API_PREFIX="/api"

# LLM Provider Configuration
LLM_PROVIDER="openai"  # openai, claude, llama

# OpenAI Configuration
OPENAI_API_KEY=""
OPENAI_MODEL="gpt-4"
OPENAI_TEMPERATURE=0.7

# Anthropic Claude Configuration (alternative)
# ANTHROPIC_API_KEY="sk-ant-your-anthropic-key-here"
# ANTHROPIC_MODEL="claude-3-sonnet-20240229"

# Vector Database Configuration
VECTOR_DB_PROVIDER="pinecone"  # pinecone, weaviate, chroma

# Pinecone Configuration
PINECONE_API_KEY=""
PINECONE_ENVIRONMENT="us-east-1"
PINECONE_INDEX_NAME="elios-interviews"

# PostgreSQL Configuration
POSTGRES_HOST=""
POSTGRES_PORT=5432
POSTGRES_USER=""
POSTGRES_PASSWORD=""
POSTGRES_DB=""

# Speech Services Configuration
AZURE_SPEECH_KEY="your-azure-speech-key-here"
AZURE_SPEECH_REGION="eastus"

# File Storage Configuration
UPLOAD_DIR="./uploads"
CV_DIR="./uploads/cvs"
AUDIO_DIR="./uploads/audio"

# Interview Configuration
MAX_QUESTIONS_PER_INTERVIEW=10
MIN_PASSING_SCORE=60.0
QUESTION_TIMEOUT_SECONDS=300

# Logging Configuration
LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT="json"  # json or text

# Mock Adapters Configuration
# Set to false to use real external services (Pinecone, OpenAI, etc.)
# In .env or .env.local
USE_MOCK_LLM=false
USE_MOCK_VECTOR_SEARCH=true
USE_MOCK_CV_ANALYZER=true
USE_MOCK_STT=true
USE_MOCK_TTS=true
USE_MOCK_ANALYTICS=true
</file>

<file path=".gitignore">
# Created by .ignore support plugin (hsz.mobi)
### Python template
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*,cover
.hypothesis/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# IPython Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# dotenv
.env
.env.local

# virtualenv
venv/
ENV/

# Spyder project settings
.spyderproject

# Rope project settings
.ropeproject
### VirtualEnv template
# Virtualenv
# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/
[Bb]in
[Ii]nclude
[Ll]ib
[Ll]ib64
[Ll]ocal
[Ss]cripts
pyvenv.cfg
.venv
pip-selfcheck.json

### JetBrains template
# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider
# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839

# User-specific stuff
.idea/**/workspace.xml
.idea/**/tasks.xml
.idea/**/usage.statistics.xml
.idea/**/dictionaries
.idea/**/shelf

# AWS User-specific
.idea/**/aws.xml

# Generated files
.idea/**/contentModel.xml

# Sensitive or high-churn files
.idea/**/dataSources/
.idea/**/dataSources.ids
.idea/**/dataSources.local.xml
.idea/**/sqlDataSources.xml
.idea/**/dynamic.xml
.idea/**/uiDesigner.xml
.idea/**/dbnavigator.xml

# Gradle
.idea/**/gradle.xml
.idea/**/libraries

# Gradle and Maven with auto-import
# When using Gradle or Maven with auto-import, you should exclude module files,
# since they will be recreated, and may cause churn.  Uncomment if using
# auto-import.
# .idea/artifacts
# .idea/compiler.xml
# .idea/jarRepositories.xml
# .idea/modules.xml
# .idea/*.iml
# .idea/modules
# *.iml
# *.ipr

# CMake
cmake-build-*/

# Mongo Explorer plugin
.idea/**/mongoSettings.xml

# File-based project format
*.iws

# IntelliJ
out/

# mpeltonen/sbt-idea plugin
.idea_modules/

# JIRA plugin
atlassian-ide-plugin.xml

# Cursive Clojure plugin
.idea/replstate.xml

# SonarLint plugin
.idea/sonarlint/

# Crashlytics plugin (for Android Studio and IntelliJ)
com_crashlytics_export_strings.xml
crashlytics.properties
crashlytics-build.properties
fabric.properties

# Editor-based Rest Client
.idea/httpRequests

# Android studio 3.1+ serialized cache file
.idea/caches/build_file_checksums.ser

# idea folder
.idea

# Elios-specific uploads and data
uploads/
*.pdf
*.doc
*.docx
*.wav
*.mp3

# Database files
*.db
*.sqlite3

# Pytest cache
.pytest_cache/

# MyPy cache
.mypy_cache/

.opencode/

.cursor/
</file>

<file path="src/adapters/llm/azure_openai_adapter.py">
"""Azure OpenAI LLM adapter implementation."""

import json
import re
from typing import Any
from uuid import UUID

from openai import AsyncAzureOpenAI

from ...domain.models.answer import AnswerEvaluation
from ...domain.models.evaluation import FollowUpEvaluationContext
from ...domain.models.question import Question
from ...domain.ports.llm_port import LLMPort


class AzureOpenAIAdapter(LLMPort):
    """Azure OpenAI implementation of LLM port.

    This adapter encapsulates all Azure OpenAI-specific logic, making it easy
    to swap for another LLM provider without touching domain logic.
    """

    def __init__(
        self,
        api_key: str,
        azure_endpoint: str,
        api_version: str,
        deployment_name: str,
        temperature: float = 0.7,
    ):
        """Initialize Azure OpenAI adapter.

        Args:
            api_key: Azure OpenAI API key
            azure_endpoint: Azure OpenAI endpoint URL (e.g., "https://your-resource.openai.azure.com/")
            api_version: Azure API version (e.g., "2024-02-15-preview")
            deployment_name: Azure deployment name (not model name)
            temperature: Sampling temperature (default: 0.7)
        """
        self.client = AsyncAzureOpenAI(
            api_key=api_key,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
        )
        self.model = deployment_name  # Azure uses deployment names instead of model names
        self.temperature = temperature

    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
        exemplars: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate an interview question using Azure OpenAI.

        Args:
            context: Interview context
            skill: Target skill to test
            difficulty: Question difficulty level
            exemplars: Optional list of similar questions for inspiration

        Returns:
            Generated question text
        """
        system_prompt = """You are an expert technical interviewer.
        Generate a clear, relevant interview question based on the context provided."""

        user_prompt = f"""
        Generate a {difficulty} difficulty interview question to test: {skill}

        Context:
        - Candidate's background: {context.get('cv_summary', 'Not provided')}
        - Previous topics covered: {context.get('covered_topics', [])}
        - Interview stage: {context.get('stage', 'early')}
        """

        # Add exemplars if provided
        if exemplars:
            user_prompt += "\n\nSimilar questions for inspiration (do NOT copy exactly):\n"
            for i, ex in enumerate(exemplars[:3], 1):  # Limit to 3 exemplars
                user_prompt += f"{i}. \"{ex.get('text', '')}\" ({ex.get('difficulty', 'UNKNOWN')})\n"
            user_prompt += "\nGenerate a NEW question inspired by the style and structure above.\n"

        user_prompt += "\nReturn only the question text, no additional explanation."

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self.temperature,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    @staticmethod
    def _extract_json_from_markdown(content: str) -> str:
        """Extract JSON from markdown code fences if present.

        Args:
            content: Content that may contain markdown-wrapped JSON

        Returns:
            Clean JSON string
        """
        if not content:
            return "{}"

        # Remove markdown code fences (```json ... ``` or ``` ... ```)
        content = re.sub(r'^```(?:json)?\s*\n', '', content.strip(), flags=re.MULTILINE)
        content = re.sub(r'\n```\s*$', '', content.strip(), flags=re.MULTILINE)

        return content.strip()

    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
        followup_context: FollowUpEvaluationContext | None = None,
    ) -> AnswerEvaluation:
        """Evaluate an answer using Azure OpenAI.

        Args:
            question: The question that was asked
            answer_text: Candidate's answer
            context: Additional context
            followup_context: Optional context for follow-up evaluation with previous attempts

        Returns:
            Evaluation results
        """
        system_prompt = """You are an expert technical interviewer evaluating candidate answers.
        Provide objective, constructive feedback with specific scores."""

        user_prompt = f"""
        Question: {question.text}
        Question Type: {question.question_type}
        Difficulty: {question.difficulty}
        Expected Skills: {', '.join(question.skills)}

        Candidate's Answer: {answer_text}

        {"Ideal Answer: " + question.ideal_answer if question.ideal_answer else ""}
        """

        # Add follow-up context if this is a follow-up question
        if followup_context:
            user_prompt += f"""

        **FOLLOW-UP CONTEXT** (Attempt #{followup_context.attempt_number}):
        This is a follow-up question after {followup_context.attempt_number - 1} previous attempt(s).

        Previous Scores: {', '.join(f'{score:.1f}' for score in followup_context.previous_scores)}
        Average Previous Score: {followup_context.average_previous_score:.1f}

        Persistent Gaps from Previous Attempts:
        {chr(10).join(f'  - {concept}' for concept in followup_context.get_persistent_gap_concepts())}

        When evaluating this follow-up answer:
        1. Focus on whether the candidate addressed the persistent gaps above
        2. Check if they demonstrate learning from previous feedback
        3. Evaluate both new content AND correction of previous gaps
        4. Be stricter if they repeat the same mistakes (this is attempt #{followup_context.attempt_number})
        """

        user_prompt += """

        Evaluate this answer and provide:
        1. Overall score (0-100)
        2. Completeness score (0-1)
        3. Relevance score (0-1)
        4. Sentiment (confident/uncertain/nervous)
        5. 2-3 strengths
        6. 2-3 weaknesses
        7. 2-3 improvement suggestions
        8. Brief reasoning for the score

        Return as JSON with keys: score, completeness, relevance, sentiment, strengths, weaknesses, improvements, reasoning
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Lower temperature for more consistent evaluation
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        content = self._extract_json_from_markdown(content)
        result = json.loads(content)

        return AnswerEvaluation(
            score=float(result.get("score", 0)),
            semantic_similarity=0.0,  # Will be calculated by vector search
            completeness=float(result.get("completeness", 0)),
            relevance=float(result.get("relevance", 0)),
            sentiment=result.get("sentiment"),
            reasoning=result.get("reasoning"),
            strengths=result.get("strengths", []),
            weaknesses=result.get("weaknesses", []),
            improvement_suggestions=result.get("improvements", []),
        )

    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive feedback report.

        Args:
            interview_id: ID of the interview
            questions: All questions asked
            answers: All answers with evaluations

        Returns:
            Formatted feedback report
        """
        system_prompt = """You are an expert career coach providing comprehensive interview feedback.
        Create a detailed, actionable report that helps candidates improve."""

        # Prepare interview summary
        qa_pairs = []
        for i, (q, a) in enumerate(zip(questions, answers)):
            qa_pairs.append(
                f"Q{i+1}: {q.text}\n"
                f"Answer Score: {a.get('evaluation', {}).get('score', 'N/A')}\n"
                f"Evaluation: {a.get('evaluation', {}).get('reasoning', 'N/A')}\n"
            )

        user_prompt = f"""
        Generate a comprehensive interview feedback report for interview {interview_id}.

        Interview Performance:
        {chr(10).join(qa_pairs)}

        Include:
        1. Overall Performance Summary
        2. Key Strengths (with examples)
        3. Areas for Improvement (with specific guidance)
        4. Skill-by-Skill Breakdown
        5. Actionable Next Steps

        Be encouraging but honest. Provide specific examples and actionable advice.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
        )

        content = response.choices[0].message.content
        return content or ""

    async def summarize_cv(self, cv_text: str) -> str:
        """Generate a summary of a CV.

        Args:
            cv_text: Extracted CV text

        Returns:
            Summary of the CV
        """
        system_prompt = """You are an expert recruiter analyzing candidate CVs.
        Create concise, informative summaries."""

        user_prompt = f"""
        Summarize this CV in 3-4 sentences, highlighting:
        - Key technical skills and experience
        - Years of experience and seniority level
        - Notable projects or achievements

        CV:
        {cv_text[:2000]}  # Limit to first 2000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.5,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract skills from CV text using Azure OpenAI.

        Args:
            text: CV text to analyze

        Returns:
            List of extracted skills with metadata
        """
        system_prompt = """You are an expert at extracting structured information from CVs.
        Identify technical skills, soft skills, and tools mentioned."""

        user_prompt = f"""
        Extract all skills from this CV text. For each skill, identify:
        - name: The skill name
        - category: "technical", "soft", or "language"
        - proficiency: "beginner", "intermediate", or "expert" (infer from context)

        Return as JSON array with keys: name, category, proficiency

        CV Text:
        {text[:3000]}  # Limit to first 3000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        content = self._extract_json_from_markdown(content)
        result = json.loads(content)
        skills: list[dict[str, str]] = result.get("skills", [])
        return skills

    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate ideal answer for a question.

        Args:
            question_text: The interview question
            context: CV summary, skills, etc.

        Returns:
            Ideal answer text (150-300 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer creating reference answers.
        Generate comprehensive, technically accurate ideal answers."""

        user_prompt = f"""
        Question: {question_text}

        Candidate Background:
        - Summary: {context.get('summary', 'Not provided')}
        - Key Skills: {', '.join(context.get('skills', [])[:5])}
        - Experience: {context.get('experience', 'Not specified')} years

        Generate an ideal answer for this interview question. The answer should:
        - Be 150-300 words
        - Demonstrate expert-level understanding
        - Cover key concepts comprehensively
        - Include practical examples if relevant
        - Be technically accurate

        Output only the ideal answer text.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Low for consistency
            max_tokens=500,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate rationale explaining why answer is ideal.

        Args:
            question_text: The question
            ideal_answer: The ideal answer

        Returns:
            Rationale text (50-100 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer explaining evaluation criteria.
        Explain why an answer demonstrates mastery."""

        user_prompt = f"""
        Question: {question_text}
        Ideal Answer: {ideal_answer}

        Explain WHY this is an ideal answer in 50-100 words. Focus on:
        - What key concepts are covered
        - Why this demonstrates mastery
        - What would be missing in a weaker answer

        Output only the rationale text.
        """

        # Note: For rationale, we use the same deployment/model
        # In Azure, you might have separate deployments for different models
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=200,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Detect concept gaps using Azure OpenAI with JSON mode.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with concept gap analysis
        """
        prompt = f"""
Question: {question_text}
Ideal Answer: {ideal_answer}
Candidate Answer: {answer_text}
Potential missing keywords: {', '.join(keyword_gaps[:10])}

Analyze and identify:
1. Key concepts in ideal answer missing from candidate answer
2. Whether missing keywords represent real conceptual gaps

Return as JSON:
- "concepts": list of missing concepts
- "confirmed": boolean
- "severity": "minor" | "moderate" | "major"
"""

        system_prompt = """You are an expert technical interviewer analyzing completeness.
Identify real conceptual gaps, not just missing synonyms."""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        content = self._extract_json_from_markdown(content)
        result = json.loads(content)

        return {
            "concepts": result.get("concepts", []),
            "keywords": keyword_gaps[:5],
            "confirmed": result.get("confirmed", False),
            "severity": result.get("severity", "minor"),
        }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
        cumulative_gaps: list[str] | None = None,
        previous_follow_ups: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate follow-up question using Azure OpenAI with cumulative context.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question (or latest follow-up)
            missing_concepts: List of concepts missing from current answer
            severity: Gap severity
            order: Follow-up order in sequence
            cumulative_gaps: All unique gaps accumulated across follow-up cycle
            previous_follow_ups: Previous follow-up questions and answers for context

        Returns:
            Follow-up question text
        """
        # Build cumulative context if available
        cumulative_context = ""
        if cumulative_gaps and len(cumulative_gaps) > 0:
            cumulative_context = f"\nAll Missing Concepts (cumulative): {', '.join(cumulative_gaps)}"

        # Build previous follow-ups context if available
        previous_context = ""
        if previous_follow_ups and len(previous_follow_ups) > 0:
            previous_context = "\n\nPrevious Follow-ups:"
            for i, fu in enumerate(previous_follow_ups, 1):
                previous_context += f"\n  #{i}: {fu.get('question', 'N/A')}"
                previous_context += f"\n      Answer: {fu.get('answer', 'N/A')[:100]}..."

        prompt = f"""
Original Question: {parent_question}
Latest Answer: {answer_text}
Current Missing Concepts: {', '.join(missing_concepts)}
Gap Severity: {severity}{cumulative_context}{previous_context}

Generate focused follow-up question (#{order}) addressing the most critical missing concepts.
The question should:
- Be specific and concise
- Prioritize concepts: {', '.join(missing_concepts[:2])}
- Avoid repeating previous follow-up questions
- Be progressively more targeted (this is follow-up #{order} of max 3)

Return only the question text.
"""

        system_prompt = """You are an expert technical interviewer generating adaptive follow-ups.
Ask questions that probe specific missing concepts while considering the full interview context."""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_tokens=150,
        )

        content = response.choices[0].message.content
        return content.strip() if content else "Can you elaborate on that?"

    async def generate_interview_recommendations(
        self,
        context: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Generate personalized interview recommendations using Azure OpenAI.

        Args:
            context: Interview context including:
                - interview_id: str
                - total_answers: int
                - gap_progression: dict
                - evaluations: list[dict]

        Returns:
            Dict with strengths, weaknesses, study topics, and technique tips
        """
        evaluations = context.get("evaluations", [])
        gap_progression = context.get("gap_progression", {})

        # Build evaluation summary
        eval_summary = "\n".join(
            [
                f"- Question {i+1}: Score {e['score']:.1f}/100"
                f"\n  Strengths: {', '.join(e.get('strengths', []))}"
                f"\n  Weaknesses: {', '.join(e.get('weaknesses', []))}"
                for i, e in enumerate(evaluations)
            ]
        )

        prompt = f"""
Interview Performance Analysis

Total Questions Answered: {len(evaluations)}
Gap Progression:
- Questions with Follow-ups: {gap_progression.get('questions_with_followups', 0)}
- Gaps Filled: {gap_progression.get('gaps_filled', 0)}
- Gaps Remaining: {gap_progression.get('gaps_remaining', 0)}

Detailed Evaluations:
{eval_summary}

Generate personalized interview feedback in JSON format with these exact keys:
{{
    "strengths": ["strength 1", "strength 2", ...],  // 3-5 specific strengths
    "weaknesses": ["weakness 1", "weakness 2", ...],  // 3-5 specific weaknesses
    "study_topics": ["topic 1", "topic 2", ...],  // 3-7 specific topics to study
    "technique_tips": ["tip 1", "tip 2", ...]  // 2-5 interview technique improvements
}}

Make recommendations:
- Specific and actionable (not generic)
- Based on actual performance data
- Prioritized by impact
- Constructive and encouraging

Return ONLY valid JSON."""

        system_prompt = """You are an expert interview coach analyzing candidate performance.
Provide specific, data-driven recommendations that help candidates improve."""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=800,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content
        if not content:
            # Fallback recommendations
            return {
                "strengths": ["Good technical foundation"],
                "weaknesses": ["Could provide more detail in answers"],
                "study_topics": ["Review core concepts"],
                "technique_tips": ["Practice explaining concepts clearly"],
            }

        try:
            content = self._extract_json_from_markdown(content)
            recommendations = json.loads(content)
            return recommendations
        except json.JSONDecodeError:
            # Fallback if JSON parsing fails
            return {
                "strengths": ["Good technical foundation"],
                "weaknesses": ["Could provide more detail in answers"],
                "study_topics": ["Review core concepts"],
                "technique_tips": ["Practice explaining concepts clearly"],
            }
</file>

<file path="src/domain/ports/__init__.py">
"""Domain ports (interfaces) package."""

from .analytics_port import AnalyticsPort
from .answer_repository_port import AnswerRepositoryPort
from .candidate_repository_port import CandidateRepositoryPort
from .cv_analysis_repository_port import CVAnalysisRepositoryPort
from .cv_analyzer_port import CVAnalyzerPort
from .evaluation_repository_port import EvaluationRepositoryPort
from .follow_up_question_repository_port import FollowUpQuestionRepositoryPort
from .interview_repository_port import InterviewRepositoryPort
from .llm_port import LLMPort
from .question_repository_port import QuestionRepositoryPort
from .speech_to_text_port import SpeechToTextPort
from .text_to_speech_port import TextToSpeechPort
from .vector_search_port import VectorSearchPort

__all__ = [
    "LLMPort",
    "VectorSearchPort",
    "QuestionRepositoryPort",
    "FollowUpQuestionRepositoryPort",
    "CandidateRepositoryPort",
    "InterviewRepositoryPort",
    "AnswerRepositoryPort",
    "EvaluationRepositoryPort",
    "CVAnalysisRepositoryPort",
    "CVAnalyzerPort",
    "SpeechToTextPort",
    "TextToSpeechPort",
    "AnalyticsPort",
]
</file>

<file path="src/domain/ports/vector_search_port.py">
"""Vector search port interface."""

from abc import ABC, abstractmethod
from typing import Any, Dict
from uuid import UUID


class VectorSearchPort(ABC):
    """Interface for vector database operations.

    This port abstracts vector storage and semantic search, allowing easy
    switching between Pinecone, Weaviate, ChromaDB, etc.
    """

    @abstractmethod
    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a question's vector embedding.

        Args:
            question_id: Unique question identifier
            embedding: Vector embedding
            metadata: Additional metadata (skills, tags, difficulty, etc.)
        """
        pass

    @abstractmethod
    async def store_cv_embedding(
        self,
        cv_analysis_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Store a CV analysis vector embedding.

        Args:
            cv_analysis_id: Unique CV analysis identifier
            embedding: Vector embedding
            metadata: Additional metadata (skills, experience, etc.)
        """
        pass

    @abstractmethod
    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Find similar questions using semantic search.

        Args:
            query_embedding: Query vector (e.g., from CV or previous context)
            top_k: Number of results to return
            filters: Optional filters (e.g., difficulty, skills)

        Returns:
            List of similar questions with similarity scores
        """
        pass

    @abstractmethod
    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Calculate similarity between answer and reference answers.

        Args:
            answer_embedding: Candidate's answer embedding
            reference_embeddings: Reference answer embeddings

        Returns:
            Similarity score (0-1)
        """
        pass

    @abstractmethod
    async def get_embedding(
        self,
        text: str,
    ) -> list[float]:
        """Generate embedding for text.

        Args:
            text: Text to embed

        Returns:
            Vector embedding
        """
        pass

    @abstractmethod
    async def delete_embeddings(
        self,
        ids: list[UUID],
    ) -> None:
        """Delete embeddings by IDs.

        Args:
            ids: List of IDs to delete
        """
        pass
</file>

<file path="README.md">
# Elios AI Interview Service

**An AI-powered mock interview platform that helps candidates prepare for technical interviews through personalized CV analysis, adaptive question generation, and real-time answer evaluation.**

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ðŸ“– Overview

Elios AI Interview Service leverages **Large Language Models (LLMs)** and **vector databases** to deliver intelligent, personalized mock interview experiences. The platform analyzes candidate CVs, generates relevant questions, evaluates answers in real-time, and provides comprehensive feedback to help candidates improve their interview performance.

### Key Features

- **ðŸŽ¯ CV Analysis**: Extract skills, experience, and education from resumes
- **ðŸ¤– Adaptive Questions**: Generate personalized interview questions using vector-based exemplar retrieval
- **ðŸ“Š Real-Time Evaluation**: Multi-dimensional answer assessment with instant feedback
- **ðŸ’¬ Voice & Text Support**: Conduct interviews via text chat or voice (planned)
- **ðŸ“ˆ Comprehensive Reports**: Detailed performance analysis with actionable recommendations
- **ðŸ”„ Swappable AI Providers**: Easy integration of OpenAI, Claude, or Llama

### Technology Stack

- **Backend**: Python 3.11+, FastAPI, Pydantic
- **Database**: PostgreSQL (Neon), SQLAlchemy 2.0 (async)
- **AI/ML**: OpenAI GPT-4, Pinecone Vector Database
- **Architecture**: Clean Architecture (Hexagonal/Ports & Adapters)
- **Testing**: pytest, pytest-asyncio
- **Code Quality**: ruff, black, mypy

### Main flows

#### 1. Preparation Phase (Scan CV & Generate Topics)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant CVAnalyzer as ðŸ“„ CV Analyzer Component
    participant VectorDB as ðŸ§  Vector Database
    participant AIEngine as ðŸ¤– AI Interviewer Engine

    Candidate->>ChatUI: Upload CV file
    ChatUI->>CVAnalyzer: Send CV for analysis
    CVAnalyzer->>VectorDB: Generate & store CV embeddings
    VectorDB-->>CVAnalyzer: Confirm embeddings stored
    CVAnalyzer-->>ChatUI: Return extracted skills & suggested topics
    ChatUI-->>Candidate: Display preparation summary
    ChatUI->>AIEngine: Notify readiness (skills, topics)
    AIEngine-->>ChatUI: Acknowledged
```

#### 2. Interview Phase (Real-time Q&A)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant AIEngine as ðŸ¤– AI Interviewer Engine
    participant VectorDB as ðŸ§  Vector Database
    participant QBank as ðŸ“š Question Bank Service
    participant STT as ðŸŽ¤ Speech-to-Text
    participant TTS as ðŸ—£ï¸ Text-to-Speech
    participant Analytics as ðŸ“Š Analytics & Feedback Service

    %% --- Start Interview ---
    Candidate->>ChatUI: Start interview
    ChatUI->>AIEngine: Request first question
    AIEngine->>VectorDB: Query similar question embeddings (based on CV topics)
    VectorDB-->>AIEngine: Return question candidates
    AIEngine->>QBank: Fetch selected question
    QBank-->>AIEngine: Return question details
    AIEngine-->>ChatUI: Send question text
    ChatUI-->>TTS: Convert question text to speech
    TTS-->>Candidate: Play AI voice question

    %% --- Candidate answers ---
    Candidate->>STT: Speak answer
    STT-->>AIEngine: Send transcript text
    AIEngine->>VectorDB: Compare answer embeddings & evaluate quality
    VectorDB-->>AIEngine: Return similarity & semantic score
    AIEngine->>Analytics: Send answer evaluation (score, sentiment, reasoning)
    Analytics-->>AIEngine: Acknowledged

    alt More questions remain
        AIEngine->>VectorDB: Retrieve next suitable question
        VectorDB-->>AIEngine: Return next question candidate
        AIEngine-->>ChatUI: Send next question
        ChatUI-->>TTS: Convert to speech & play
        TTS-->>Candidate: Play next question
    else Interview finished
        AIEngine-->>ChatUI: Notify interview end
    end

```

#### 3. Final Stage (Evaluation & Reporting)

```mermaid
sequenceDiagram
    actor Candidate
    participant ChatUI as Chat UI / Frontend
    participant AIEngine as ðŸ¤– AI Interviewer Engine
    participant Analytics as ðŸ“Š Analytics & Feedback Service

    AIEngine->>Analytics: Send final interview summary (scores, metrics, transcript)
    Analytics->>Analytics: Aggregate results & generate report
    Analytics-->>AIEngine: Acknowledged

    AIEngine-->>ChatUI: Notify interview completion
    ChatUI->>Analytics: Request final feedback report
    Analytics-->>ChatUI: Return detailed feedback & improvement suggestions
    ChatUI-->>Candidate: Display performance summary & insights

```

---

## ðŸ—ï¸ Architecture

This project follows **Clean Architecture** (Hexagonal/Ports & Adapters): Domain Layer (pure business logic) â†’ Application Layer (use cases) â†’ Adapters Layer (external services) â†’ Infrastructure Layer (config, DI).

ðŸ“š **[Full Architecture Details â†’](docs/system-architecture.md)**

---

## ðŸš€ Quick Start

### âš¡ 5-Minute Setup

**Just want to run it?** Copy and paste these commands:

```bash
# Setup environment and install dependencies
python -m venv venv && venv\Scripts\activate && pip install -e ".[dev]"

# Configure and run migrations
cp .env.example .env.local && alembic upgrade head

# Start the server
python -m src.main
```

Then visit: **http://localhost:8000/docs**

âš ï¸ **Note**: Edit `.env.local` with your API keys before full functionality works.

---

### ðŸ“‹ Detailed Setup Instructions

#### Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- PostgreSQL database (or Neon account)
- OpenAI API key
- Pinecone API key

#### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/elios/elios-ai-service.git
   cd EliosAIService
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv

   # Windows
   venv\Scripts\activate

   # Linux/macOS
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -e ".[dev]"
   ```

4. **Configure environment variables**
   ```bash
   cp .env.example .env.local
   ```

   Edit `.env.local` with your credentials:
   ```env
   # Database
   DATABASE_URL=postgresql://user:password@host:5432/elios_interviews

   # LLM Provider
   LLM_PROVIDER=openai
   OPENAI_API_KEY=sk-your-api-key-here
   OPENAI_MODEL=gpt-4

   # Vector Database
   VECTOR_DB_PROVIDER=pinecone
   PINECONE_API_KEY=your-pinecone-api-key
   PINECONE_INDEX_NAME=elios-questions
   ```

5. **Run database migrations**
   ```bash
   alembic upgrade head
   ```

6. **Verify database setup**
   ```bash
   python scripts/verify_db.py
   ```

7. **Start the server**
   ```bash
   python -m src.main
   ```

   Server runs at: http://localhost:8000

   API Documentation: http://localhost:8000/docs

---

## ðŸ“– Documentation

### For Users
- **[Project Overview & PDR](docs/project-overview-pdr.md)** - Product requirements, features, and roadmap
- **[Database Setup Guide](DATABASE_SETUP.md)** - Comprehensive database configuration
- **[Environment Setup Guide](ENV_SETUP.md)** - Environment configuration best practices

### For Developers
- **[System Architecture](docs/system-architecture.md)** - Detailed architecture documentation
- **[Codebase Summary](docs/codebase-summary.md)** - Project structure and tech stack
- **[Code Standards](docs/code-standards.md)** - Coding conventions and best practices
- **[CLAUDE.md](CLAUDE.md)** - Development guidelines for AI assistants

---

## ðŸ§ª Development

### Mock Adapters for Testing

**Mock adapters** simulate external services without API costs or network latency. Enabled by default in development.

**Available Mocks** (6 total):
- `MockLLMAdapter` - Simulates OpenAI/LLM responses
- `MockVectorSearchAdapter` - In-memory vector search
- `MockSTTAdapter` - Simulates speech-to-text
- `MockTTSAdapter` - Simulates text-to-speech
- `MockCVAnalyzerAdapter` - Filename-based CV parsing
- `MockAnalyticsAdapter` - In-memory performance tracking

**Configuration**:
```env
# .env.local
USE_MOCK_ADAPTERS=true   # Use mocks (default, fast tests)
USE_MOCK_ADAPTERS=false  # Use real services (requires API keys)
```

**Benefits**:
- Tests run 10x faster (~5s vs ~30s)
- No API costs during development
- No network dependency
- Deterministic test results

**Note**: Repositories (PostgreSQL) intentionally NOT mocked - use real database for data integrity tests.

### Running Tests

```bash
# Run all tests (with mocks enabled by default)
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test types
pytest tests/unit/         # Unit tests only
pytest tests/integration/  # Integration tests only
pytest tests/e2e/          # End-to-end tests only

# Test with real adapters (requires API keys)
USE_MOCK_ADAPTERS=false pytest
```

### Code Quality

```bash
# Format code
black src/

# Lint code
ruff check src/
ruff check --fix src/  # Auto-fix issues

# Type checking
mypy src/

# Run all checks
black src/ && ruff check src/ && mypy src/
```

### Database Operations

```bash
# Create new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback one migration
alembic downgrade -1

# View migration history
alembic history

# Verify database
python scripts/verify_db.py
```

---

## ðŸŽ¯ Usage Example

### 1. Create a Candidate

```python
import httpx

async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8000/api/candidates",
        json={
            "name": "John Doe",
            "email": "john.doe@example.com"
        }
    )
    candidate = response.json()
    print(f"Created candidate: {candidate['id']}")
```

### 2. Upload and Analyze CV

```python
async with httpx.AsyncClient() as client:
    with open("resume.pdf", "rb") as cv_file:
        response = await client.post(
            "http://localhost:8000/api/cv/upload",
            files={"file": cv_file},
            data={"candidate_id": candidate['id']}
        )
    cv_analysis = response.json()
    print(f"Skills found: {cv_analysis['skills']}")
```

### 3. Start Interview

```python
async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8000/api/interviews",
        json={
            "candidate_id": candidate['id'],
            "cv_analysis_id": cv_analysis['id']
        }
    )
    interview = response.json()
    print(f"Interview ready with {len(interview['question_ids'])} questions")
```

### 4. Submit Answer

```python
async with httpx.AsyncClient() as client:
    response = await client.post(
        f"http://localhost:8000/api/interviews/{interview['id']}/answers",
        json={
            "question_id": interview['question_ids'][0],
            "answer_text": "My answer here..."
        }
    )
    evaluation = response.json()
    print(f"Score: {evaluation['score']}/100")
    print(f"Feedback: {evaluation['feedback']}")
```

---

## ðŸ“¦ Project Structure

```
EliosAIService/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ domain/              # Core business logic (5 models, 11 ports)
â”‚   â”œâ”€â”€ application/         # Use cases
â”‚   â”œâ”€â”€ adapters/            # External service implementations
â”‚   â””â”€â”€ infrastructure/      # Config, DI, database
â”œâ”€â”€ alembic/                 # Database migrations
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ tests/                   # Test suites
```

ðŸ“š **[Complete Structure â†’](docs/codebase-summary.md)**

---

## ðŸ”§ Configuration

Configuration is managed through environment variables with the following priority:

1. `.env.local` (highest priority, gitignored)
2. `.env` (can be committed, template)
3. System environment variables
4. Pydantic defaults

### Key Configuration Sections

- **Application**: Name, version, environment
- **LLM Provider**: OpenAI, Claude, or Llama configuration
- **Vector Database**: Pinecone, Weaviate, or ChromaDB settings
- **PostgreSQL**: Database connection and credentials
- **Speech Services**: Azure STT, Edge TTS (planned)
- **Interview Settings**: Question count, scoring, timeouts

See [ENV_SETUP.md](ENV_SETUP.md) for detailed configuration guide.

---

## ðŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines (coming soon).

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following our [Code Standards](docs/code-standards.md)
4. Run tests and quality checks
5. Commit using [Conventional Commits](https://www.conventionalcommits.org/)
6. Push to your branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Commit Message Format

```
<type>(<scope>): <subject>

Examples:
feat(domain): add Interview aggregate with state management
fix(persistence): handle NULL metadata in answer mapper
docs: update API documentation for CV upload endpoint
```

---

## ðŸ—ºï¸ Roadmap

### Phase 1: Foundation (Current - v0.1.0)
- âœ… Domain models and ports
- âœ… PostgreSQL persistence layer
- âœ… OpenAI LLM adapter
- âœ… Pinecone vector adapter
- âœ… Database migrations
- ðŸ”„ REST API implementation
- ðŸ”„ CV processing adapters

### Phase 2: Core Features (v0.2.0 - v0.5.0)
- â³ Voice interview support
- â³ Advanced question generation
- â³ Interview analytics
- â³ Performance benchmarks
- â³ Frontend integration

### Phase 3: Intelligence Enhancement (v0.6.0 - v0.8.0)
- â³ Multi-LLM support (Claude, Llama)
- â³ Behavioral question analysis
- â³ Personality insights
- â³ Skill gap analysis

### Phase 4: Scale & Polish (v0.9.0 - v1.0.0)
- â³ Multi-language support
- â³ Team/organization features
- â³ Mobile app support
- â³ Production deployment

See [Project Overview & PDR](docs/project-overview-pdr.md) for detailed roadmap.

---

## ðŸ“Š Current Status

**Version**: 0.1.0 (Foundation Phase)

**Implemented**:
- âœ… Clean Architecture structure
- âœ… Domain models (5 entities)
- âœ… Repository ports (5 interfaces)
- âœ… PostgreSQL persistence (5 repositories)
- âœ… OpenAI LLM adapter
- âœ… Pinecone vector adapter
- âœ… Async SQLAlchemy 2.0 with Alembic
- âœ… Configuration management
- âœ… Dependency injection container
- âœ… Use cases (AnalyzeCV, StartInterview)
- âœ… Health check API endpoint

**In Progress**:
- ðŸ”„ Complete REST API
- ðŸ”„ CV processing adapters
- ðŸ”„ WebSocket chat handler

**Planned**:
- â³ Authentication & authorization
- â³ Comprehensive testing
- â³ API documentation
- â³ Docker deployment

---

## ðŸ›¡ï¸ Security

- API keys stored in environment variables (never committed)
- SQL injection prevention via parameterized queries
- Input validation with Pydantic
- HTTPS enforcement (production)
- Data encryption at rest (Neon built-in)
- GDPR compliance considerations

Report security vulnerabilities to: security@elios.ai

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ðŸ™ Acknowledgments

- **OpenAI** for GPT-4 and Embeddings API
- **Pinecone** for vector database
- **FastAPI** for the excellent web framework
- **Neon** for serverless PostgreSQL
- **Pydantic** for data validation
- **SQLAlchemy** for ORM

---

## ðŸ“ž Contact

- **Website**: https://elios.ai
- **Email**: contact@elios.ai
- **Issues**: [GitHub Issues](https://github.com/elios/elios-ai-service/issues)
- **Discussions**: [GitHub Discussions](https://github.com/elios/elios-ai-service/discussions)

---

## â­ Support

If you find this project helpful, please consider giving it a star on GitHub! It helps others discover the project and motivates continued development.

---

**Built with â¤ï¸ using Clean Architecture principles**
</file>

<file path="src/adapters/api/websocket/session_orchestrator.py">
"""Interview session orchestrator - stateless coordinator for interview flow.

REFACTORED: Removed dual state machines (SessionState + InterviewStatus).
Now relies solely on domain Interview entity for state management.
Orchestrator loads fresh state from DB before each operation.
"""

import base64
import logging
from datetime import datetime
from typing import Any
from uuid import UUID

from fastapi import WebSocket

from ....application.use_cases.complete_interview import CompleteInterviewUseCase
from ....application.use_cases.follow_up_decision import FollowUpDecisionUseCase
from ....application.use_cases.get_next_question import GetNextQuestionUseCase
from ....application.use_cases.process_answer_adaptive import (
    ProcessAnswerAdaptiveUseCase,
)
from ....domain.models.answer import Answer
from ....domain.models.follow_up_question import FollowUpQuestion
from ....domain.ports.answer_repository_port import AnswerRepositoryPort
from ....domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from ....domain.ports.interview_repository_port import InterviewRepositoryPort
from ....domain.ports.question_repository_port import QuestionRepositoryPort
from ....infrastructure.database.session import get_async_session

logger = logging.getLogger(__name__)


class InterviewSessionOrchestrator:
    """Stateless orchestrator for interview session flow.

    This orchestrator coordinates interview flow by delegating state management
    to the domain Interview entity:
    - Loads fresh interview state from DB before operations
    - Uses domain methods for state transitions (start, ask_followup, proceed_to_next_question)
    - No in-memory state tracking (stateless)
    - WebSocket message coordination and TTS generation

    The domain Interview entity is the single source of truth for all state.
    """

    def __init__(
        self,
        interview_id: UUID,
        websocket: WebSocket,
        container: Any,
    ):
        """Initialize session orchestrator.

        Args:
            interview_id: Interview UUID
            websocket: WebSocket connection
            container: Dependency injection container
        """
        self.interview_id = interview_id
        self.websocket = websocket
        self.container = container
        self.created_at = datetime.utcnow()
        self.last_activity = datetime.utcnow()

        logger.info(
            f"Session orchestrator created for interview {interview_id}",
            extra={"interview_id": str(interview_id)},
        )

    async def start_session(self) -> None:
        """Start interview session by sending first question.

        Delegates state management to domain Interview entity.

        Raises:
            ValueError: If interview not found, already started, or no questions
        """
        async for session in get_async_session():
            interview_repo = self.container.interview_repository_port(session)
            question_repo = self.container.question_repository_port(session)
            tts = self.container.text_to_speech_port()

            # Load fresh interview state from DB
            interview = await interview_repo.get_by_id(self.interview_id)
            if not interview:
                logger.error(f"Interview {self.interview_id} not found")
                await self._send_error("INTERVIEW_NOT_FOUND", "Interview not found")
                raise ValueError(f"Interview {self.interview_id} not found")

            # Get first question
            use_case = GetNextQuestionUseCase(
                interview_repository=interview_repo,
                question_repository=question_repo,
            )
            question = await use_case.execute(self.interview_id)

            if not question:
                logger.error(f"No questions available for interview {self.interview_id}")
                await self._send_error("NO_QUESTIONS", "No questions available")
                raise ValueError(f"No questions available for interview {self.interview_id}")

            # Use domain method for state transition (IDLE â†’ QUESTIONING)
            interview.start()
            await interview_repo.update(interview)

            # Generate TTS audio
            audio_bytes = await tts.synthesize_speech(question.text)
            audio_data = base64.b64encode(audio_bytes).decode("utf-8")

            # Send question message
            await self._send_message(
                {
                    "type": "question",
                    "question_id": str(question.id),
                    "text": question.text,
                    "question_type": question.question_type,
                    "difficulty": question.difficulty,
                    "index": interview.current_question_index,
                    "total": len(interview.question_ids),
                    "audio_data": audio_data,
                }
            )

            logger.info(
                f"Started interview, sent first question: {question.id}",
                extra={
                    "interview_id": str(self.interview_id),
                    "question_id": str(question.id),
                    "status": interview.status.value,
                },
            )
            break

    async def handle_answer(self, answer_text: str) -> None:
        """Handle answer based on interview state from DB.

        Loads fresh interview state to determine if answering main question or follow-up.

        Args:
            answer_text: Candidate's answer text

        Raises:
            ValueError: If called in invalid state
        """
        async for session in get_async_session():
            interview_repo = self.container.interview_repository_port(session)

            # Load fresh state from DB
            interview = await self._get_interview_or_raise(interview_repo)

            # Route based on domain status
            from ....domain.models.interview import InterviewStatus

            if interview.status == InterviewStatus.QUESTIONING:
                await self._handle_main_question_answer(answer_text)
            elif interview.status == InterviewStatus.FOLLOW_UP:
                await self._handle_followup_answer(answer_text)
            else:
                raise ValueError(
                    f"Cannot handle answer in status {interview.status}. "
                    f"Expected QUESTIONING or FOLLOW_UP"
                )
            break

    async def _handle_main_question_answer(self, answer_text: str) -> None:
        """Handle answer to main question.

        Flow:
        1. Load interview state, get current question ID
        2. Process answer (triggers QUESTIONING â†’ EVALUATING via domain)
        3. Make follow-up decision
        4. Either send follow-up OR next main question OR complete

        Args:
            answer_text: Candidate's answer text
        """
        async for session in get_async_session():
            interview_repo = self.container.interview_repository_port(session)
            question_repo = self.container.question_repository_port(session)
            answer_repo = self.container.answer_repository_port(session)
            follow_up_repo = self.container.follow_up_question_repository(session)

            # Load fresh interview state
            interview = await self._get_interview_or_raise(interview_repo)
            interview.mark_evaluating()
            await interview_repo.update(interview)

            # Get current question ID from interview
            current_question_id = interview.get_current_question_id()
            if not current_question_id:
                raise ValueError("No current question in interview")

            # Process answer with adaptive evaluation (triggers state transition inside use case)
            use_case = ProcessAnswerAdaptiveUseCase(
                answer_repository=answer_repo,
                interview_repository=interview_repo,
                question_repository=question_repo,
                follow_up_question_repository=follow_up_repo,
                llm=self.container.llm_port(),
                vector_search=self.container.vector_search_port(),
            )

            answer, has_more = await use_case.execute(
                interview_id=self.interview_id,
                question_id=current_question_id,
                answer_text=answer_text,
            )

            # Send evaluation
            await self._send_evaluation(answer)

            # Reload interview (state may have changed in use case)
            interview = await self._get_interview_or_raise(interview_repo)

            # Make follow-up decision
            decision_use_case = FollowUpDecisionUseCase(
                answer_repository=answer_repo,
                follow_up_question_repository=follow_up_repo,
            )

            decision = await decision_use_case.execute(
                interview_id=self.interview_id,
                parent_question_id=current_question_id,  # Current question is the parent
                latest_answer=answer,
            )

            logger.info(
                f"Follow-up decision: needs={decision['needs_followup']}, "
                f"reason='{decision['reason']}', count={decision['follow_up_count']}"
            )

            # If follow-up needed, generate and send
            if decision["needs_followup"]:
                await self._generate_and_send_followup(
                    answer, decision, question_repo, follow_up_repo, interview_repo, current_question_id
                )
                break

            # No follow-up needed - send next main question or complete
            if has_more:
                await self._send_next_main_question(interview_repo, question_repo)
            else:
                await self._complete_interview(
                    interview_repo, answer_repo, question_repo, follow_up_repo
                )

            break

    async def _handle_followup_answer(self, answer_text: str) -> None:
        """Handle answer to follow-up question.

        Flow:
        1. Load interview state, get parent question ID and last follow-up ID
        2. Process answer (triggers FOLLOW_UP â†’ EVALUATING via domain)
        3. Make follow-up decision again (may generate another follow-up)
        4. Either send another follow-up OR next main question OR complete

        Args:
            answer_text: Candidate's answer text
        """
        async for session in get_async_session():
            interview_repo = self.container.interview_repository_port(session)
            question_repo = self.container.question_repository_port(session)
            answer_repo = self.container.answer_repository_port(session)
            follow_up_repo = self.container.follow_up_question_repository(session)

            # Load fresh interview state
            interview = await self._get_interview_or_raise(interview_repo)
            interview.mark_evaluating()
            await interview_repo.update(interview)

            # Get parent question ID and last follow-up ID from interview
            parent_question_id = interview.current_parent_question_id
            if not parent_question_id:
                raise ValueError("No parent question tracked in interview")

            # Last follow-up is the most recent in adaptive_follow_ups list
            if not interview.adaptive_follow_ups:
                raise ValueError("No follow-up questions tracked in interview")
            current_followup_id = interview.adaptive_follow_ups[-1]

            # Process answer (triggers state transition inside use case)
            use_case = ProcessAnswerAdaptiveUseCase(
                answer_repository=answer_repo,
                interview_repository=interview_repo,
                question_repository=question_repo,
                follow_up_question_repository=follow_up_repo,
                llm=self.container.llm_port(),
                vector_search=self.container.vector_search_port(),
            )

            answer, has_more = await use_case.execute(
                interview_id=self.interview_id,
                question_id=current_followup_id,
                answer_text=answer_text,
            )

            # Send evaluation
            await self._send_evaluation(answer)

            # Reload interview
            interview = await self._get_interview_or_raise(interview_repo)

            # Make follow-up decision (using parent question ID)
            decision_use_case = FollowUpDecisionUseCase(
                answer_repository=answer_repo,
                follow_up_question_repository=follow_up_repo,
            )

            decision = await decision_use_case.execute(
                interview_id=self.interview_id,
                parent_question_id=parent_question_id,
                latest_answer=answer,
            )

            logger.info(
                f"Follow-up decision (after follow-up): needs={decision['needs_followup']}, "
                f"reason='{decision['reason']}', count={decision['follow_up_count']}"
            )

            # If another follow-up needed, generate and send
            if decision["needs_followup"]:
                await self._generate_and_send_followup(
                    answer, decision, question_repo, follow_up_repo, interview_repo, parent_question_id
                )
                break

            # No more follow-ups - send next main question or complete
            if has_more:
                await self._send_next_main_question(interview_repo, question_repo)
            else:
                await self._complete_interview(
                    interview_repo, answer_repo, question_repo, follow_up_repo
                )

            break

    async def _generate_and_send_followup(
        self,
        answer: Answer,
        decision: dict[str, Any],
        question_repo: QuestionRepositoryPort,
        follow_up_repo: FollowUpQuestionRepositoryPort,
        interview_repo: InterviewRepositoryPort,
        parent_question_id: UUID,
    ) -> None:
        """Generate and send follow-up question using domain methods.

        Args:
            answer: Latest answer entity
            decision: Follow-up decision dict
            question_repo: Question repository
            follow_up_repo: Follow-up question repository
            interview_repo: Interview repository
            parent_question_id: UUID of parent question
        """
        # Get parent question for context
        parent_question = await question_repo.get_by_id(parent_question_id)

        # Generate follow-up question with cumulative context
        follow_up_text = await self.container.llm_port().generate_followup_question(
            parent_question=parent_question.text if parent_question else "Unknown",
            answer_text=answer.text,
            missing_concepts=decision["cumulative_gaps"],
            severity=answer.gaps.get("severity", "moderate") if answer.gaps else "moderate",
            order=decision["follow_up_count"] + 1,
            cumulative_gaps=decision["cumulative_gaps"],
        )

        # Create and save follow-up question entity
        follow_up = FollowUpQuestion(
            parent_question_id=parent_question_id,
            interview_id=self.interview_id,
            text=follow_up_text,
            generated_reason=decision["reason"],
            order_in_sequence=decision["follow_up_count"] + 1,
        )
        await follow_up_repo.save(follow_up)

        # Use domain method to track follow-up (triggers EVALUATING â†’ FOLLOW_UP)
        interview = await self._get_interview_or_raise(interview_repo)
        interview.ask_followup(follow_up.id, parent_question_id)
        await interview_repo.update(interview)

        # Send follow-up question with audio
        tts = self.container.text_to_speech_port()
        audio_bytes = await tts.synthesize_speech(follow_up.text)
        audio_data = base64.b64encode(audio_bytes).decode("utf-8")

        await self._send_message(
            {
                "type": "follow_up_question",
                "question_id": str(follow_up.id),
                "parent_question_id": str(parent_question_id),
                "text": follow_up.text,
                "generated_reason": follow_up.generated_reason,
                "order_in_sequence": follow_up.order_in_sequence,
                "audio_data": audio_data,
            }
        )

        logger.info(
            f"Sent follow-up #{follow_up.order_in_sequence}",
            extra={
                "interview_id": str(self.interview_id),
                "follow_up_id": str(follow_up.id),
                "parent_question_id": str(parent_question_id),
            }
        )

    async def _send_next_main_question(
        self,
        interview_repo: InterviewRepositoryPort,
        question_repo: QuestionRepositoryPort,
    ) -> None:
        """Send next main question using domain methods.

        Args:
            interview_repo: Interview repository
            question_repo: Question repository
        """
        # Load interview and use domain method (EVALUATING â†’ QUESTIONING, resets counters)
        interview = await self._get_interview_or_raise(interview_repo)

        interview.proceed_to_next_question()
        await interview_repo.update(interview)

        # Get next question
        use_case = GetNextQuestionUseCase(
            interview_repository=interview_repo,
            question_repository=question_repo,
        )
        question = await use_case.execute(self.interview_id)

        if not question:
            logger.warning(f"No more questions for interview {self.interview_id}")
            await self._complete_interview(interview_repo, None)
            return

        # Reload interview for updated index
        interview = await self._get_interview_or_raise(interview_repo)

        # Generate TTS audio
        tts = self.container.text_to_speech_port()
        audio_bytes = await tts.synthesize_speech(question.text)
        audio_data = base64.b64encode(audio_bytes).decode("utf-8")

        # Send question message
        await self._send_message(
            {
                "type": "question",
                "question_id": str(question.id),
                "text": question.text,
                "question_type": question.question_type,
                "difficulty": question.difficulty,
                "index": interview.current_question_index if interview else 0,
                "total": len(interview.question_ids) if interview else 0,
                "audio_data": audio_data,
            }
        )

        logger.info(
            f"Sent next main question: {question.id}",
            extra={
                "interview_id": str(self.interview_id),
                "question_id": str(question.id),
                "status": interview.status.value if interview else "unknown",
            }
        )

    async def _complete_interview(
        self,
        interview_repo: InterviewRepositoryPort,
        answer_repo: AnswerRepositoryPort | None,
        question_repo: QuestionRepositoryPort | None = None,
        follow_up_repo: FollowUpQuestionRepositoryPort | None = None,
    ) -> None:
        """Complete interview using domain methods, generate summary, send results.

        State transition handled by CompleteInterviewUseCase.

        Args:
            interview_repo: Interview repository
            answer_repo: Answer repository (optional)
            question_repo: Question repository (optional, for summary generation)
            follow_up_repo: Follow-up question repository (optional, for summary)
        """

        # Get LLM for summary generation
        llm = self.container.llm_port()

        # Complete interview with summary generation
        complete_use_case = CompleteInterviewUseCase(
            interview_repository=interview_repo,
            answer_repository=answer_repo,
            question_repository=question_repo,
            follow_up_question_repository=follow_up_repo,
            llm=llm,
        )
        interview, summary = await complete_use_case.execute(
            self.interview_id, generate_summary=True
        )

        # Send summary message to client
        if summary:
            await self._send_message(
                {
                    "type": "interview_complete",
                    "interview_id": summary["interview_id"],
                    "overall_score": summary["overall_score"],
                    "theoretical_score_avg": summary["theoretical_score_avg"],
                    "speaking_score_avg": summary["speaking_score_avg"],
                    "total_questions": summary["total_questions"],
                    "total_follow_ups": summary["total_follow_ups"],
                    "gap_progression": summary["gap_progression"],
                    "strengths": summary["strengths"],
                    "weaknesses": summary["weaknesses"],
                    "study_recommendations": summary["study_recommendations"],
                    "technique_tips": summary["technique_tips"],
                    "completion_time": summary["completion_time"],
                    "feedback_url": f"/api/interviews/{self.interview_id}/feedback",
                }
            )
        else:
            # Fallback if summary generation failed
            overall_score = 0.0
            if answer_repo:
                answers = await answer_repo.get_by_interview_id(self.interview_id)
                if answers:
                    overall_score = (
                        sum(a.evaluation.score for a in answers if a.evaluation)
                        / len(answers)
                    )

            await self._send_message(
                {
                    "type": "interview_complete",
                    "interview_id": str(interview.id),
                    "overall_score": overall_score,
                    "total_questions": len(interview.question_ids),
                    "feedback_url": f"/api/interviews/{self.interview_id}/feedback",
                }
            )

        logger.info(f"Interview {self.interview_id} completed with summary")

    async def _send_evaluation(self, answer: Answer) -> None:
        """Send evaluation message.

        Args:
            answer: Answer entity with evaluation
        """
        eval_message = {
            "type": "evaluation",
            "answer_id": str(answer.id),
            "score": answer.evaluation.score,
            "feedback": answer.evaluation.reasoning,
            "strengths": answer.evaluation.strengths,
            "weaknesses": answer.evaluation.weaknesses,
            "similarity_score": answer.similarity_score,
            "gaps": answer.gaps,
        }

        await self._send_message(eval_message)

    async def _send_message(self, message: dict) -> None:
        """Send message to client via WebSocket.

        Args:
            message: Message dict to send
        """
        from .connection_manager import manager

        await manager.send_message(self.interview_id, message)

    async def _send_error(self, code: str, message: str) -> None:
        """Send error message to client.

        Args:
            code: Error code
            message: Error message
        """
        await self._send_message(
            {
                "type": "error",
                "code": code,
                "message": message,
            }
        )

    async def _get_interview_or_raise(
        self, interview_repo: InterviewRepositoryPort
    ) -> Any:
        """Get interview by ID or raise ValueError if not found.

        Args:
            interview_repo: Interview repository

        Returns:
            Interview entity

        Raises:
            ValueError: If interview not found
        """
        interview = await interview_repo.get_by_id(self.interview_id)
        if not interview:
            raise ValueError(f"Interview {self.interview_id} not found")
        return interview

    async def get_state(self) -> dict[str, Any]:
        """Get current session state from DB (stateless).

        Loads fresh interview state from database.

        Returns:
            State dict with interview data from DB
        """
        async for session in get_async_session():
            interview_repo = self.container.interview_repository_port(session)
            interview = await self._get_interview_or_raise(interview_repo)

            if not interview:
                return {
                    "interview_id": str(self.interview_id),
                    "status": "NOT_FOUND",
                    "created_at": self.created_at.isoformat(),
                    "last_activity": self.last_activity.isoformat(),
                }

            return {
                "interview_id": str(self.interview_id),
                "status": interview.status.value,
                "current_question_id": str(interview.get_current_question_id()) if interview.get_current_question_id() else None,
                "parent_question_id": str(interview.current_parent_question_id) if interview.current_parent_question_id else None,
                "followup_count": interview.current_followup_count,
                "progress": f"{interview.current_question_index}/{len(interview.question_ids)}",
                "created_at": self.created_at.isoformat(),
                "last_activity": self.last_activity.isoformat(),
            }

        # Fallback (should not reach here)
        return {
            "interview_id": str(self.interview_id),
            "error": "Failed to retrieve state",
        }
</file>

<file path="src/adapters/persistence/models.py">
"""SQLAlchemy models for persistence.

These models represent the database schema and map domain entities
to database tables using SQLAlchemy ORM.
"""

from datetime import datetime
from uuid import UUID

from sqlalchemy import (
    Boolean,
    DateTime,
    Float,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
)
from sqlalchemy import (
    Enum as SQLEnum,
)
from sqlalchemy.dialects.postgresql import ARRAY, JSONB
from sqlalchemy.dialects.postgresql import UUID as PGUUID
from sqlalchemy.orm import Mapped, mapped_column, relationship

from ...domain.models.interview import InterviewStatus
from ...domain.models.question import DifficultyLevel, QuestionType
from ...infrastructure.database.base import Base


class CandidateModel(Base):
    """SQLAlchemy model for Candidate entity."""

    __tablename__ = "candidates"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    name: Mapped[str] = mapped_column(String(255), nullable=False)
    email: Mapped[str] = mapped_column(String(255), unique=True, nullable=False, index=True)
    cv_file_path: Mapped[str | None] = mapped_column(String(500), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    interviews: Mapped[list["InterviewModel"]] = relationship(
        "InterviewModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )
    cv_analyses: Mapped[list["CVAnalysisModel"]] = relationship(
        "CVAnalysisModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="candidate",
        cascade="all, delete-orphan",
    )

    __table_args__ = (
        Index("idx_candidates_email", "email"),
        Index("idx_candidates_created_at", "created_at"),
    )


class QuestionModel(Base):
    """SQLAlchemy model for Question entity."""

    __tablename__ = "questions"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    text: Mapped[str] = mapped_column(Text, nullable=False)
    question_type: Mapped[str] = mapped_column(
        SQLEnum(QuestionType, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    difficulty: Mapped[str] = mapped_column(
        SQLEnum(DifficultyLevel, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    skills: Mapped[list[str]] = mapped_column(ARRAY(String(100)), nullable=False, default=[])
    tags: Mapped[list[str]] = mapped_column(ARRAY(String(100)), nullable=False, default=[])
    evaluation_criteria: Mapped[str | None] = mapped_column(Text, nullable=True)
    version: Mapped[int] = mapped_column(Integer, nullable=False, default=1)
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)

    # Pre-planning fields for adaptive interviews
    ideal_answer: Mapped[str | None] = mapped_column(Text, nullable=True)
    rationale: Mapped[str | None] = mapped_column(Text, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="question",
    )

    __table_args__ = (
        Index("idx_questions_type", "question_type"),
        Index("idx_questions_difficulty", "difficulty"),
        Index("idx_questions_skills", "skills", postgresql_using="gin"),
        Index("idx_questions_tags", "tags", postgresql_using="gin"),
    )


class InterviewModel(Base):
    """SQLAlchemy model for Interview entity."""

    __tablename__ = "interviews"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    status: Mapped[str] = mapped_column(
        SQLEnum(InterviewStatus, native_enum=False, length=50),
        nullable=False,
        index=True,
    )
    cv_analysis_id: Mapped[UUID | None] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("cv_analyses.id", ondelete="SET NULL"),
        nullable=True,
    )
    question_ids: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )
    answer_ids: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )
    current_question_index: Mapped[int] = mapped_column(Integer, nullable=False, default=0)

    # NEW: Pre-planning metadata for adaptive interviews
    plan_metadata: Mapped[dict] = mapped_column(JSONB, nullable=False, default={})
    adaptive_follow_ups: Mapped[list[UUID]] = mapped_column(
        ARRAY(PGUUID(as_uuid=True)),
        nullable=False,
        default=[],
    )

    # NEW: Follow-up tracking for current session
    current_parent_question_id: Mapped[UUID | None] = mapped_column(
        PGUUID(as_uuid=True),
        nullable=True,
        default=None,
    )
    current_followup_count: Mapped[int] = mapped_column(
        Integer,
        nullable=False,
        default=0,
    )

    started_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    completed_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    updated_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="interviews",
    )
    cv_analysis: Mapped["CVAnalysisModel | None"] = relationship(
        "CVAnalysisModel",
        foreign_keys=[cv_analysis_id],
    )
    answers: Mapped[list["AnswerModel"]] = relationship(
        "AnswerModel",
        back_populates="interview",
        cascade="all, delete-orphan",
    )

    __table_args__ = (
        Index("idx_interviews_candidate_id", "candidate_id"),
        Index("idx_interviews_status", "status"),
        Index("idx_interviews_created_at", "created_at"),
    )


class AnswerModel(Base):
    """SQLAlchemy model for Answer entity."""

    __tablename__ = "answers"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    interview_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("interviews.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    question_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("questions.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    text: Mapped[str] = mapped_column(Text, nullable=False)
    is_voice: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False)
    audio_file_path: Mapped[str | None] = mapped_column(String(500), nullable=True)
    duration_seconds: Mapped[float | None] = mapped_column(Float, nullable=True)
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)
    answer_metadata: Mapped[dict] = mapped_column("metadata", JSONB, nullable=False, default={})

    # NEW: Link to Evaluation entity (Phase 1 refactoring)
    evaluation_id: Mapped[UUID | None] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("evaluations.id", ondelete="SET NULL"),
        nullable=True,
        index=True,
    )

    # OLD fields (keep for backward compatibility during migration)
    # These will be dropped after migration completes
    evaluation: Mapped[dict | None] = mapped_column(JSONB, nullable=True)
    similarity_score: Mapped[float | None] = mapped_column(Float, nullable=True)
    gaps: Mapped[dict | None] = mapped_column(JSONB, nullable=True)
    evaluated_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    interview: Mapped["InterviewModel"] = relationship(
        "InterviewModel",
        back_populates="answers",
    )
    question: Mapped["QuestionModel"] = relationship(
        "QuestionModel",
        back_populates="answers",
    )
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="answers",
    )

    __table_args__ = (
        Index("idx_answers_interview_id", "interview_id"),
        Index("idx_answers_question_id", "question_id"),
        Index("idx_answers_candidate_id", "candidate_id"),
        Index("idx_answers_created_at", "created_at"),
    )


class CVAnalysisModel(Base):
    """SQLAlchemy model for CV Analysis entity."""

    __tablename__ = "cv_analyses"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    candidate_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("candidates.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    cv_file_path: Mapped[str] = mapped_column(String(500), nullable=False)
    extracted_text: Mapped[str] = mapped_column(Text, nullable=False)
    skills: Mapped[list[dict]] = mapped_column(JSONB, nullable=False, default=[])
    work_experience_years: Mapped[float | None] = mapped_column(Float, nullable=True)
    education_level: Mapped[str | None] = mapped_column(String(100), nullable=True)
    suggested_topics: Mapped[list[str]] = mapped_column(
        ARRAY(String(200)),
        nullable=False,
        default=[],
    )
    suggested_difficulty: Mapped[str] = mapped_column(String(50), nullable=False, default="medium")
    embedding: Mapped[list[float] | None] = mapped_column(ARRAY(Float), nullable=True)
    summary: Mapped[str | None] = mapped_column(Text, nullable=True)
    cv_metadata: Mapped[dict] = mapped_column("metadata", JSONB, nullable=False, default={})
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    candidate: Mapped["CandidateModel"] = relationship(
        "CandidateModel",
        back_populates="cv_analyses",
    )

    __table_args__ = (
        Index("idx_cv_analyses_candidate_id", "candidate_id"),
        Index("idx_cv_analyses_created_at", "created_at"),
    )


class FollowUpQuestionModel(Base):
    """SQLAlchemy model for FollowUpQuestion entity."""

    __tablename__ = "follow_up_questions"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    parent_question_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("questions.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    interview_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("interviews.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    text: Mapped[str] = mapped_column(Text, nullable=False)
    generated_reason: Mapped[str] = mapped_column(Text, nullable=False)
    order_in_sequence: Mapped[int] = mapped_column(Integer, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    __table_args__ = (
        Index("idx_follow_up_questions_parent_question_id", "parent_question_id"),
        Index("idx_follow_up_questions_interview_id", "interview_id"),
        Index("idx_follow_up_questions_created_at", "created_at"),
    )


class EvaluationModel(Base):
    """SQLAlchemy model for Evaluation entity."""

    __tablename__ = "evaluations"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    answer_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("answers.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    question_id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), nullable=False, index=True)
    interview_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("interviews.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    # Scores
    raw_score: Mapped[float] = mapped_column(Float, nullable=False)
    penalty: Mapped[float] = mapped_column(Float, nullable=False, server_default="0")
    final_score: Mapped[float] = mapped_column(Float, nullable=False)
    similarity_score: Mapped[float | None] = mapped_column(Float, nullable=True)

    # LLM evaluation details
    completeness: Mapped[float] = mapped_column(Float, nullable=False)
    relevance: Mapped[float] = mapped_column(Float, nullable=False)
    sentiment: Mapped[str | None] = mapped_column(String(50), nullable=True)
    reasoning: Mapped[str | None] = mapped_column(Text, nullable=True)
    strengths: Mapped[list[str]] = mapped_column(
        ARRAY(Text), nullable=False, server_default="{}"
    )
    weaknesses: Mapped[list[str]] = mapped_column(
        ARRAY(Text), nullable=False, server_default="{}"
    )
    improvement_suggestions: Mapped[list[str]] = mapped_column(
        ARRAY(Text), nullable=False, server_default="{}"
    )

    # Follow-up context
    attempt_number: Mapped[int] = mapped_column(Integer, nullable=False, server_default="1")
    parent_evaluation_id: Mapped[UUID | None] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("evaluations.id", ondelete="SET NULL"),
        nullable=True,
        index=True,
    )

    # Timestamps
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)
    evaluated_at: Mapped[datetime | None] = mapped_column(DateTime, nullable=True)

    # Relationships
    gaps: Mapped[list["EvaluationGapModel"]] = relationship(
        "EvaluationGapModel",
        back_populates="evaluation",
        cascade="all, delete-orphan",
    )

    __table_args__ = (
        Index("idx_evaluations_answer_id", "answer_id"),
        Index("idx_evaluations_question_id", "question_id"),
        Index("idx_evaluations_interview_id", "interview_id"),
        Index("idx_evaluations_parent_id", "parent_evaluation_id"),
        Index("idx_evaluations_attempt_number", "attempt_number"),
    )


class EvaluationGapModel(Base):
    """SQLAlchemy model for EvaluationGap entity."""

    __tablename__ = "evaluation_gaps"

    id: Mapped[UUID] = mapped_column(PGUUID(as_uuid=True), primary_key=True)
    evaluation_id: Mapped[UUID] = mapped_column(
        PGUUID(as_uuid=True),
        ForeignKey("evaluations.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    concept: Mapped[str] = mapped_column(Text, nullable=False)
    severity: Mapped[str] = mapped_column(String(20), nullable=False, server_default="'moderate'")
    resolved: Mapped[bool] = mapped_column(Boolean, nullable=False, server_default="false")
    created_at: Mapped[datetime] = mapped_column(DateTime, nullable=False)

    # Relationships
    evaluation: Mapped["EvaluationModel"] = relationship(
        "EvaluationModel",
        back_populates="gaps",
    )

    __table_args__ = (
        Index("idx_evaluation_gaps_evaluation_id", "evaluation_id"),
        Index("idx_evaluation_gaps_resolved", "resolved"),
    )
</file>

<file path="src/adapters/vector_db/chroma_adapter.py">
import os
import chromadb
import asyncio
import numpy as np
from uuid import UUID
from typing import Any, Dict
import json
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from ...domain.ports.vector_search_port import VectorSearchPort

load_dotenv()

CHROMA_PATH = os.path.join(os.path.dirname(__file__), "..", "chroma_db")
CV_COLLECTION_NAME = "cv_embedding"
QUESTION_COLLECTION_NAME = "question_embedding"


chromaDB_client = chromadb.PersistentClient(path=CHROMA_PATH)
cv_collection = chromaDB_client.get_or_create_collection(name=CV_COLLECTION_NAME, namespace="cv_process")
question_collection = chromaDB_client.get_or_create_collection(name=QUESTION_COLLECTION_NAME, namespace="question_process")

embedding_client = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key = os.getenv("TEXT_EMBEDDING_API_KEY"),
    dimensions=1536,
    max_retries=3,
    request_timeout=30
)

class ChromaAdapter(VectorSearchPort):
    """Interface for vector database operations.

    This port abstracts vector storage and semantic search, allowing easy
    switching between Pinecone, Weaviate, ChromaDB, etc.
    """

    def __init__(self):
        pass
    
    """
    This method stores a CV analysis vector embedding.
    """
    async def store_cv_embedding(self, cv_analysis_id, embedding, metadatas):
        try:
            cv_collection.add(
                ids=[cv_analysis_id],
                embeddings=[embedding],
                metadatas=[metadatas]
            )
        except Exception as e:
            print(f"Error storing CV embedding: {e}")

    """
    This method generates an embedding for the given text, in detail for this application is summarized info of a CV.
    """
    async def get_embedding(self, text):
        cleaned_text = text.strip()
        loop = asyncio.get_event_loop()
        try:
            embedding = await loop.run_in_executor(
                None,
                lambda: embedding_client.embed_query(cleaned_text)
            )
            return embedding
        except Exception as e:
            print(f"Error generating embedding: {e.__cause__}")
            return None

    """
    This method deletes embeddings by their IDs.
    """        
    async def delete_embeddings(self, ids):
        valid_ids = [id.strip() for id in ids if id and id.strip()]

        loop = asyncio.get_event_loop()
        try:
            await loop.run_in_executor(None, lambda: cv_collection.delete(ids=valid_ids))
        except Exception as e:
            print(f"Error deleting embeddings: {e.__cause__}")

    """
    This method finds similar questions based on a query embedding.
    """
    async def find_similar_questions(self, query_embedding, top_k=5):

        loop = asyncio.get_event_loop()
        include_metadata = True
        try:
            raw_results = await loop.run_in_executor(
            None,
            lambda: cv_collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["metadatas", "distances", "documents"] if include_metadata else ["distances"]
                )
            )
            ids = raw_results.get("ids", [[]])[0]
            distances = raw_results.get("distances", [[]])[0]
            metadatas = raw_results.get("metadatas", [[]])[0]
            results = []
            for i, doc_id in enumerate(ids):
                if i >= len(distances):
                    break
                distance = distances[i]
                # Convert distance â†’ similarity (cosine: 0= giá»‘ng nháº¥t â†’ 1=khÃ¡c nháº¥t)
                similarity_score = 1.0 - distance

                item = {
                    "id": doc_id,
                    "score": round(similarity_score, 4),
                    "distance": round(distance, 4)
                }
                if include_metadata and i < len(metadatas):
                    item["metadata"] = metadatas[i]

                results.append(item)
            return results
        except Exception as e:
            print(f"Error finding similar questions: {e}")
            return None
        
    """
    This method calculate similarity between answer and given answer
    """
    async def find_similar_answers(self, answer_embedding: list[float], reference_embeddings: list[list[float]]):
        results = question_collection.query(
        query_embeddings=reference_embeddings,
        n_results=1,                            
        where={"answer": {"$exists": True}},    
        include=["embeddings", "metadatas"]
        )

        embeddings_nested = results["embeddings"]
        embeddings = [
            embedding
            for embedding_list in embeddings_nested
            for embedding in embedding_list
        ]

        ref_array = np.array(embeddings, dtype=np.float32)
        ans_array = np.array(answer_embedding, dtype=np.float32).reshape(1, -1)
        ref_norms = np.linalg.norm(ref_array, axis=1, keepdims=True)
        ans_norm = np.linalg.norm(ans_array)

        if ans_norm == 0 or np.any(ref_norms == 0):
            return 0.0

        ref_normalized = ref_array / ref_norms
        ans_normalized = ans_array / ans_norm

        similarities = np.dot(ref_normalized, ans_normalized.T).flatten()

        return float(np.max(similarities))
        
    async def store_question_embedding(self, question_id: UUID, embedding: list[float], metadatas: dict[str, Any]):
        try:
            question_collection.add(
                ids=[question_id],
                embeddings=[embedding],
                metadatas=[metadatas]
            )
        except Exception as e:
            print(f"Error storing question embedding: {e}")
</file>

<file path="src/application/use_cases/__init__.py">
"""Use cases package."""

from .analyze_cv import AnalyzeCVUseCase
from .generate_summary import GenerateSummaryUseCase
from .plan_interview import PlanInterviewUseCase
from .process_answer_adaptive import ProcessAnswerAdaptiveUseCase

__all__ = [
    "AnalyzeCVUseCase",
    "GenerateSummaryUseCase",
    "PlanInterviewUseCase",
    "ProcessAnswerAdaptiveUseCase",
]
</file>

<file path="src/domain/models/answer.py">
"""Answer domain model."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class AnswerEvaluation(BaseModel):
    """Represents the evaluation of an answer.

    This is a value object containing evaluation metrics.
    """

    score: float = Field(ge=0.0, le=100.0)  # Score out of 100
    semantic_similarity: float = Field(ge=0.0, le=1.0)  # Similarity to reference
    completeness: float = Field(ge=0.0, le=1.0)  # How complete the answer is
    relevance: float = Field(ge=0.0, le=1.0)  # How relevant to the question
    sentiment: str | None = None  # e.g., "confident", "uncertain"
    reasoning: str | None = None  # AI explanation of the evaluation
    strengths: list[str] = Field(default_factory=list)
    weaknesses: list[str] = Field(default_factory=list)
    improvement_suggestions: list[str] = Field(default_factory=list)

    def is_passing(self, threshold: float = 60.0) -> bool:
        """Check if answer meets passing threshold.

        Args:
            threshold: Minimum score to pass (default: 60.0)

        Returns:
            True if score meets threshold, False otherwise
        """
        return self.score >= threshold


class Answer(BaseModel):
    """Represents a candidate's answer to a question.

    This is an entity in the interview domain.
    NOTE: Evaluation moved to separate Evaluation entity (linked via evaluation_id).
    """

    id: UUID = Field(default_factory=uuid4)
    interview_id: UUID
    question_id: UUID
    candidate_id: UUID
    text: str  # The actual answer text
    is_voice: bool = False  # Whether answer was given via voice
    audio_file_path: str | None = None  # If voice answer
    duration_seconds: float | None = None  # Time taken to answer
    embedding: list[float] | None = None  # Vector embedding of answer
    metadata: dict[str, Any] = Field(default_factory=dict)  # Additional context

    # UPDATED: Link to separate Evaluation entity (Phase 1 refactoring)
    evaluation_id: UUID | None = None  # FK to evaluations table

    # REMOVED: evaluation, similarity_score, gaps, speaking_score, overall_score
    # These fields now exist in Evaluation entity

    # KEEP: Voice metrics (will be stored in Evaluation entity in future)
    voice_metrics: dict[str, float] | None = None  # Voice quality metrics from STT

    created_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False

    def is_evaluated(self) -> bool:
        """Check if answer has been evaluated.

        Returns:
            True if evaluation_id is set, False otherwise
        """
        return self.evaluation_id is not None

    def is_complete(self) -> bool:
        """Check if answer is considered complete.

        Returns:
            True if answer has content
        """
        return bool(self.text and len(self.text.strip()) > 0)

    def has_voice_metrics(self) -> bool:
        """Check if voice metrics are available.

        Returns:
            True if voice metrics exist
        """
        return self.voice_metrics is not None and len(self.voice_metrics) > 0

    def get_voice_metrics(self) -> dict[str, float] | None:
        """Get voice metrics if available.

        Returns:
            Voice metrics dict or None
        """
        return self.voice_metrics
</file>

<file path="src/domain/models/question.py">
"""Question domain model."""

from datetime import datetime
from enum import Enum
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class QuestionType(str, Enum):
    """Question type enumeration."""

    TECHNICAL = "technical"
    BEHAVIORAL = "behavioral"
    SITUATIONAL = "situational"


class DifficultyLevel(str, Enum):
    """Question difficulty level."""

    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"


class Question(BaseModel):
    """Represents an interview question.

    Questions are value objects in the interview domain.
    They contain metadata for semantic search and categorization.
    """

    id: UUID = Field(default_factory=uuid4)
    text: str
    question_type: QuestionType
    difficulty: DifficultyLevel
    skills: list[str] = Field(default_factory=list)  # e.g., ["Python", "OOP"]
    tags: list[str] = Field(default_factory=list)  # e.g., ["algorithms", "data-structures"]
    evaluation_criteria: str | None = None
    version: int = 1
    embedding: list[float] | None = None  # Vector embedding for semantic search

    # Pre-planning fields for adaptive interviews
    ideal_answer: str | None = None  # Reference answer for similarity scoring and evaluation
    rationale: str | None = None  # Explanation of why this question is suitable for the candidate

    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        pass

    def has_skill(self, skill: str) -> bool:
        """Check if question tests a specific skill.

        Args:
            skill: Skill name to check

        Returns:
            True if skill is tested, False otherwise
        """
        return skill.lower() in [s.lower() for s in self.skills]

    def has_tag(self, tag: str) -> bool:
        """Check if question has a specific tag.

        Args:
            tag: Tag to check

        Returns:
            True if tag exists, False otherwise
        """
        return tag.lower() in [t.lower() for t in self.tags]

    def is_suitable_for_difficulty(self, max_difficulty: DifficultyLevel) -> bool:
        """Check if question difficulty is appropriate.

        Args:
            max_difficulty: Maximum allowed difficulty

        Returns:
            True if suitable, False otherwise
        """
        difficulty_order = {
            DifficultyLevel.EASY: 1,
            DifficultyLevel.MEDIUM: 2,
            DifficultyLevel.HARD: 3,
        }
        return difficulty_order[self.difficulty] <= difficulty_order[max_difficulty]

    def has_ideal_answer(self) -> bool:
        """Check if question has ideal answer for similarity scoring.

        Returns:
            True if ideal_answer is present and non-empty
        """
        return self.ideal_answer is not None and len(self.ideal_answer.strip()) > 10

    @property
    def is_planned(self) -> bool:
        """Check if question is part of pre-planned interview.

        Returns:
            True if has ideal_answer and rationale
        """
        return self.has_ideal_answer() and self.rationale is not None
</file>

<file path="src/adapters/api/rest/interview_routes.py">
"""Interview REST API endpoints."""

from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from ....application.dto.interview_dto import (
    InterviewResponse,
    PlanInterviewRequest,
    PlanningStatusResponse,
    QuestionResponse,
)
from ....application.use_cases.get_next_question import GetNextQuestionUseCase
from ....application.use_cases.plan_interview import PlanInterviewUseCase
from ....domain.models.interview import InterviewStatus
from ....infrastructure.config.settings import get_settings
from ....infrastructure.database.session import get_async_session
from ....infrastructure.dependency_injection.container import get_container

router = APIRouter(prefix="/interviews", tags=["Interviews"])


@router.get(
    "/{interview_id}",
    response_model=InterviewResponse,
    summary="Get interview details",
)
async def get_interview(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get interview by ID.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Interview details

    Raises:
        HTTPException: If interview not found
    """
    container = get_container()
    settings = get_settings()

    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    base_url = settings.ws_base_url
    return InterviewResponse.from_domain(interview, base_url)


@router.put(
    "/{interview_id}/start",
    response_model=InterviewResponse,
    summary="Start interview (move to IN_PROGRESS)",
)
async def start_interview(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Start interview session.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Updated interview details

    Raises:
        HTTPException: If interview not found or invalid state
    """
    container = get_container()
    settings = get_settings()

    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    try:
        interview.start()
        updated = await interview_repo.update(interview)

        base_url = settings.ws_base_url
        return InterviewResponse.from_domain(updated, base_url)
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


@router.get(
    "/{interview_id}/questions/current",
    response_model=QuestionResponse,
    summary="Get current question",
)
async def get_current_question(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get current unanswered question.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Current question details

    Raises:
        HTTPException: If interview not found or no more questions
    """
    container = get_container()

    use_case = GetNextQuestionUseCase(
        interview_repository=container.interview_repository_port(session),
        question_repository=container.question_repository_port(session),
    )

    try:
        question = await use_case.execute(interview_id)
        if not question:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No more questions available",
            )

        # Get interview for context
        interview = await container.interview_repository_port(
            session
        ).get_by_id(interview_id)

        if not interview:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Interview {interview_id} not found",
            )

        return QuestionResponse(
            id=question.id,
            text=question.text,
            question_type=question.question_type.value,
            difficulty=question.difficulty.value,
            index=interview.current_question_index,
            total=len(interview.question_ids),
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


# NEW: Adaptive Planning Endpoints
@router.post(
    "/plan",
    response_model=PlanningStatusResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Plan interview with adaptive questions",
)
async def plan_interview(
    request: PlanInterviewRequest,
    session: AsyncSession = Depends(get_async_session),
):
    """Plan interview by generating n questions with ideal answers.

    This endpoint triggers the pre-planning phase:
    1. Calculates n based on skill diversity (max 5)
    2. Generates n questions with ideal_answer + rationale
    3. Returns interview with status=PREPARING (async process)

    Args:
        request: Planning request with cv_analysis_id and candidate_id
        session: Database session

    Returns:
        Planning status with interview_id

    Raises:
        HTTPException: If CV analysis not found
    """
    try:
        container = get_container()

        # Validate CV analysis exists
        cv_analysis_repo = container.cv_analysis_repository_port(session)
        cv_analysis = await cv_analysis_repo.get_by_id(request.cv_analysis_id)
        if not cv_analysis:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"CV analysis {request.cv_analysis_id} not found",
            )

        # Execute planning use case
        use_case = PlanInterviewUseCase(
            llm=container.llm_port(),
            vector_search=container.vector_search_port(),
            cv_analysis_repo=cv_analysis_repo,
            interview_repo=container.interview_repository_port(session),
            question_repo=container.question_repository_port(session),
        )

        interview = await use_case.execute(
            cv_analysis_id=request.cv_analysis_id,
            candidate_id=request.candidate_id,
        )

        return PlanningStatusResponse(
            interview_id=interview.id,
            status=interview.status.value,
            planned_question_count=interview.planned_question_count,
            plan_metadata=interview.plan_metadata,
            message=f"Interview planned with {interview.planned_question_count} questions",
        )

    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=str(e)
        ) from e


@router.get(
    "/{interview_id}/plan",
    response_model=PlanningStatusResponse,
    summary="Get interview planning status",
)
async def get_planning_status(
    interview_id: UUID,
    session: AsyncSession = Depends(get_async_session),
):
    """Get interview planning status.

    Args:
        interview_id: Interview UUID
        session: Database session

    Returns:
        Planning status details

    Raises:
        HTTPException: If interview not found
    """
    container = get_container()
    interview_repo = container.interview_repository_port(session)
    interview = await interview_repo.get_by_id(interview_id)

    if not interview:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Interview {interview_id} not found",
        )

    # Determine message based on status
    if interview.status == InterviewStatus.IDLE:
        message = f"Interview ready with {interview.planned_question_count} questions"
    elif interview.status == InterviewStatus.QUESTIONING or interview.status == InterviewStatus.EVALUATING:
        message = "Interview started"
    elif interview.status == InterviewStatus.COMPLETE:
        message = "Interview completed"
    else:
        message = f"Interview status: {interview.status.value}"

    return PlanningStatusResponse(
        interview_id=interview.id,
        status=interview.status.value,
        planned_question_count=interview.planned_question_count,
        plan_metadata=interview.plan_metadata,
        message=message,
    )
</file>

<file path="src/adapters/mock/mock_llm_adapter.py">
"""Mock LLM adapter for development and testing."""

import random
from typing import Any
from uuid import UUID

from ...domain.models.answer import AnswerEvaluation
from ...domain.models.evaluation import FollowUpEvaluationContext
from ...domain.models.question import Question
from ...domain.ports.llm_port import LLMPort


class MockLLMAdapter(LLMPort):
    """Mock LLM adapter that returns realistic but fake responses.

    This adapter simulates LLM behavior for development and testing
    without requiring actual API calls to external services.
    """

    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
        exemplars: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate mock question.

        Args:
            context: Interview context
            skill: Target skill to test
            difficulty: Question difficulty level
            exemplars: Optional list of similar questions (for testing)

        Returns:
            Mock question text
        """
        base_question = f"Mock question about {skill} at {difficulty} difficulty?"

        # Indicate exemplars were provided (for testing purposes)
        if exemplars:
            base_question += f" [Generated with {len(exemplars)} exemplar(s)]"

        return base_question

    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
        followup_context: FollowUpEvaluationContext | None = None,
    ) -> AnswerEvaluation:
        """Generate mock evaluation with realistic scores."""
        # Adjust score based on follow-up context (if provided)
        if followup_context:
            # Mock: Lower scores for later attempts (simulate declining patience)
            base_score = random.uniform(65.0, 85.0)
            attempt_penalty = (followup_context.attempt_number - 1) * 5
            score = max(50.0, base_score - attempt_penalty)
        else:
            # Random score between 70-95 for realistic feel
            score = random.uniform(70.0, 95.0)

        # Simulate different evaluation patterns based on score
        if score >= 85:
            strengths = [
                "Clear and comprehensive explanation",
                "Good use of examples",
                "Strong technical understanding",
            ]
            weaknesses = ["Could provide more edge case handling"]
            improvements = ["Consider discussing performance implications"]
            sentiment = "confident"
        elif score >= 75:
            strengths = [
                "Solid understanding of concepts",
                "Relevant examples provided",
            ]
            weaknesses = [
                "Missing some technical details",
                "Could be more structured",
            ]
            improvements = [
                "Add more specific examples",
                "Elaborate on implementation details",
            ]
            sentiment = "positive"
        else:
            strengths = ["Basic understanding demonstrated"]
            weaknesses = [
                "Lacks depth",
                "Missing key concepts",
                "Limited examples",
            ]
            improvements = [
                "Study the fundamentals more thoroughly",
                "Provide concrete examples",
                "Explain reasoning more clearly",
            ]
            sentiment = "uncertain"

        return AnswerEvaluation(
            score=score,
            semantic_similarity=random.uniform(0.7, 0.95),
            completeness=random.uniform(0.7, 0.95),
            relevance=random.uniform(0.8, 1.0),
            sentiment=sentiment,
            reasoning=f"Mock evaluation: Answer demonstrates {sentiment} understanding of {question.text[:50]}...",
            strengths=strengths,
            weaknesses=weaknesses,
            improvement_suggestions=improvements,
        )

    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate mock feedback report."""
        avg_score = sum(a.get("score", 75) for a in answers) / len(answers) if answers else 75
        return f"""
Mock Feedback Report for Interview {interview_id}

Overall Performance: {avg_score:.1f}/100

Questions Answered: {len(answers)} of {len(questions)}

Strengths:
- Good understanding of fundamental concepts
- Clear communication skills
- Relevant examples provided

Areas for Improvement:
- Dive deeper into technical details
- Practice explaining complex concepts
- Add more real-world examples

Recommendations:
- Review advanced topics in your field
- Practice mock interviews
- Study best practices and design patterns
"""

    async def summarize_cv(self, cv_text: str) -> str:
        """Generate mock CV summary."""
        return "Mock CV summary: Experienced professional with strong technical skills in software development."

    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract mock skills."""
        return [
            {"name": "Python", "category": "programming", "proficiency": "expert"},
            {"name": "FastAPI", "category": "framework", "proficiency": "advanced"},
            {"name": "PostgreSQL", "category": "database", "proficiency": "intermediate"},
        ]

    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate mock ideal answer."""
        return f"""Mock ideal answer for '{question_text[:50]}...':
This demonstrates comprehensive understanding of the concept with clear explanation,
relevant examples, and practical application. The answer covers all key aspects
including fundamental principles, real-world use cases, and potential edge cases."""

    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate mock rationale."""
        return """This answer demonstrates mastery by covering fundamental concepts,
providing practical examples, and explaining the reasoning behind technical choices.
A weaker answer would miss these comprehensive details."""

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Mock gap detection based on answer length.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with concept gap analysis
        """
        # Simple heuristic: short answers have gaps
        word_count = len(answer_text.split())

        if word_count < 30:
            # Simulate gaps for short answers
            return {
                "concepts": keyword_gaps[:2] if keyword_gaps else ["depth", "examples"],
                "keywords": keyword_gaps[:5],
                "confirmed": True,
                "severity": "moderate",
            }
        else:
            # Good answer, no gaps
            return {
                "concepts": [],
                "keywords": [],
                "confirmed": False,
                "severity": "minor",
            }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
        cumulative_gaps: list[str] | None = None,
        previous_follow_ups: list[dict[str, Any]] | None = None,
    ) -> str:
        """Mock follow-up question generation with cumulative context.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question (or latest follow-up)
            missing_concepts: List of concepts missing from current answer
            severity: Gap severity
            order: Follow-up order in sequence
            cumulative_gaps: All unique gaps accumulated across follow-up cycle
            previous_follow_ups: Previous follow-up questions and answers for context

        Returns:
            Follow-up question text
        """
        # Use cumulative gaps if available, otherwise current missing concepts
        target_concepts = cumulative_gaps if cumulative_gaps else missing_concepts
        concepts_str = ', '.join(target_concepts[:2]) if target_concepts else "that concept"

        # Add order context to make questions unique per iteration
        if order == 1:
            return f"Can you elaborate more on {concepts_str}? Please provide specific examples."
        elif order == 2:
            return f"Let's dive deeper into {concepts_str}. Can you explain the underlying principles?"
        else:
            return f"Final question on {concepts_str}: How would you apply this in a real-world scenario?"

    async def generate_interview_recommendations(
        self,
        context: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Generate mock personalized recommendations.

        Args:
            context: Interview context with evaluations and gap progression

        Returns:
            Dict with strengths, weaknesses, study topics, and technique tips
        """
        evaluations = context.get("evaluations", [])
        gap_progression = context.get("gap_progression", {})

        # Calculate average score from evaluations
        avg_score = (
            sum(e["score"] for e in evaluations) / len(evaluations)
            if evaluations
            else 75.0
        )

        # Generate recommendations based on score
        if avg_score >= 85:
            strengths = [
                "Exceptional understanding of core concepts",
                "Strong analytical and problem-solving skills",
                "Excellent communication and explanation abilities",
                "Good use of real-world examples and context",
            ]
            weaknesses = [
                "Could explore more edge cases in answers",
                "Consider discussing performance trade-offs more explicitly",
            ]
            study_topics = [
                "Advanced system design patterns",
                "Performance optimization techniques",
                "Security best practices",
            ]
            technique_tips = [
                "Continue your clear and structured communication style",
                "Consider adding more visual diagrams when explaining concepts",
            ]
        elif avg_score >= 70:
            strengths = [
                "Solid understanding of fundamental concepts",
                "Good ability to explain technical topics",
                "Relevant examples provided in most answers",
            ]
            weaknesses = [
                "Some technical depth missing in complex topics",
                "Could improve answer structure and organization",
                "Occasionally missed key concepts in follow-up questions",
            ]
            study_topics = [
                "Deep dive into data structures and algorithms",
                "Practice system design scenarios",
                "Review concurrency and threading concepts",
                "Study testing strategies and best practices",
            ]
            technique_tips = [
                "Use the STAR method (Situation, Task, Action, Result) for answering",
                "Practice explaining concepts at multiple levels of detail",
                "Slow down pace to ensure clarity in responses",
            ]
        else:
            strengths = [
                "Shows basic understanding of core concepts",
                "Willing to tackle challenging questions",
            ]
            weaknesses = [
                "Lacks depth in technical explanations",
                "Missing critical concepts in several answers",
                "Limited use of examples and practical applications",
                "Answer structure needs improvement",
            ]
            study_topics = [
                "Review fundamental programming concepts thoroughly",
                "Practice basic data structures and algorithms",
                "Study common design patterns",
                "Build small projects to reinforce learning",
                "Review language-specific best practices",
            ]
            technique_tips = [
                "Practice explaining concepts out loud before answering",
                "Use pen and paper to diagram ideas during preparation",
                "Structure answers: state the concept, explain it, give an example",
                "Take time to think before responding - silence is acceptable",
                "Ask clarifying questions if prompt is unclear",
            ]

        # Add gap-specific recommendations
        if gap_progression.get("gaps_remaining", 0) > 3:
            study_topics.append(
                "Focus on concepts that remained unclear after follow-up questions"
            )

        return {
            "strengths": strengths,
            "weaknesses": weaknesses,
            "study_topics": study_topics,
            "technique_tips": technique_tips,
        }
</file>

<file path="tests/conftest.py">
"""Pytest configuration and fixtures."""

from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

import pytest

from src.domain.models.answer import Answer, AnswerEvaluation
from src.domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from src.domain.models.follow_up_question import FollowUpQuestion
from src.domain.models.interview import Interview, InterviewStatus
from src.domain.models.question import DifficultyLevel, Question, QuestionType


@pytest.fixture
def sample_cv_analysis() -> CVAnalysis:
    """Sample CV analysis for testing."""
    return CVAnalysis(
        candidate_id=uuid4(),
        cv_file_path="/path/to/cv.pdf",
        extracted_text="Sample CV text with Python, FastAPI, PostgreSQL, and Docker experience",
        summary="Experienced Python developer with 5 years of experience",
        skills=[
            ExtractedSkill(skill="Python", category="technical", proficiency="expert"),
            ExtractedSkill(skill="FastAPI", category="technical", proficiency="intermediate"),
            ExtractedSkill(skill="PostgreSQL", category="technical", proficiency="intermediate"),
            ExtractedSkill(skill="Docker", category="technical", proficiency="beginner"),
        ],
        work_experience_years=5,
        education_level="Bachelor's Degree",
    )


@pytest.fixture
def sample_question_with_ideal_answer() -> Question:
    """Sample question with ideal answer for adaptive testing."""
    return Question(
        text="Explain the concept of recursion in programming",
        question_type=QuestionType.TECHNICAL,
        difficulty=DifficultyLevel.MEDIUM,
        skills=["Python", "Algorithms"],
        ideal_answer="""Recursion is a programming technique where a function calls itself
        to solve a problem by breaking it down into smaller subproblems. Key concepts include:
        1) Base case: A condition that stops the recursion
        2) Recursive case: The function calling itself with modified parameters
        3) Stack management: Each call is added to the call stack
        Example: Fibonacci sequence, factorial calculation, tree traversal""",
        rationale="""This answer demonstrates mastery by covering the fundamental concepts
        (base case, recursive case), explaining the mechanism (call stack), and providing
        concrete examples. A weaker answer would miss these comprehensive details.""",
    )


@pytest.fixture
def sample_question_without_ideal_answer() -> Question:
    """Sample question without ideal answer (legacy mode)."""
    return Question(
        text="Tell me about a challenging project you worked on",
        question_type=QuestionType.BEHAVIORAL,
        difficulty=DifficultyLevel.EASY,
        skills=["Communication"],
    )


@pytest.fixture
def sample_interview_adaptive(sample_cv_analysis: CVAnalysis) -> Interview:
    """Sample adaptive interview with plan_metadata."""
    interview = Interview(
        candidate_id=sample_cv_analysis.candidate_id,
        status=InterviewStatus.IDLE,
        cv_analysis_id=sample_cv_analysis.id,
    )
    interview.plan_metadata = {
        "n": 3,
        "generated_at": datetime.utcnow().isoformat(),
        "strategy": "adaptive_planning_v1",
        "cv_summary": sample_cv_analysis.summary,
    }
    interview.question_ids = [uuid4(), uuid4(), uuid4()]
    # Start interview to set status to IN_PROGRESS
    interview.start()
    return interview


@pytest.fixture
def sample_interview_legacy() -> Interview:
    """Sample legacy interview without plan_metadata."""
    return Interview(
        candidate_id=uuid4(),
        status=InterviewStatus.QUESTIONING,
    )


@pytest.fixture
def sample_answer_high_similarity(sample_question_with_ideal_answer: Question) -> Answer:
    """Sample answer with high similarity (>= 80%)."""
    answer = Answer(
        interview_id=uuid4(),
        question_id=sample_question_with_ideal_answer.id,
        candidate_id=uuid4(),
        text="""Recursion is when a function calls itself to solve problems.
        It needs a base case to stop and a recursive case to continue.
        The call stack tracks each call. Examples include factorial and Fibonacci.""",
        is_voice=False,
        similarity_score=0.85,
        gaps={"concepts": [], "keywords": [], "confirmed": False},
    )
    answer.evaluate(
        AnswerEvaluation(
            score=85.0,
            semantic_similarity=0.85,
            completeness=0.9,
            relevance=0.95,
            sentiment="confident",
            reasoning="Strong answer covering key concepts",
            strengths=["Clear explanation", "Good examples"],
            weaknesses=["Could add more detail on stack management"],
            improvement_suggestions=["Explain stack overflow scenarios"],
        )
    )
    return answer


@pytest.fixture
def sample_answer_low_similarity(sample_question_with_ideal_answer: Question) -> Answer:
    """Sample answer with low similarity (< 80%) - should trigger follow-up."""
    answer = Answer(
        interview_id=uuid4(),
        question_id=sample_question_with_ideal_answer.id,
        candidate_id=uuid4(),
        text="Recursion is a function that calls itself.",
        is_voice=False,
        similarity_score=0.45,
        gaps={
            "concepts": ["base case", "recursive case", "call stack"],
            "keywords": ["base", "stack", "parameters"],
            "confirmed": True,
            "severity": "major",
        },
    )
    answer.evaluate(
        AnswerEvaluation(
            score=55.0,
            semantic_similarity=0.45,
            completeness=0.4,
            relevance=0.8,
            sentiment="uncertain",
            reasoning="Answer is too brief and missing key concepts",
            strengths=["Correct basic definition"],
            weaknesses=["Missing base case", "No examples", "Lacks depth"],
            improvement_suggestions=[
                "Explain base case and recursive case",
                "Provide examples",
                "Discuss call stack",
            ],
        )
    )
    return answer


@pytest.fixture
def sample_follow_up_question(sample_question_with_ideal_answer: Question) -> FollowUpQuestion:
    """Sample follow-up question."""
    return FollowUpQuestion(
        parent_question_id=sample_question_with_ideal_answer.id,
        interview_id=uuid4(),
        text="Can you explain what a base case is in recursion and why it's important?",
        generated_reason="Missing concepts: base case, termination condition",
        order_in_sequence=1,
    )


# Mock repository fixtures
class MockQuestionRepository:
    """Mock question repository for testing."""

    def __init__(self) -> None:
        self.questions: dict[UUID, Question] = {}

    async def save(self, question: Question | FollowUpQuestion) -> Question | FollowUpQuestion:
        self.questions[question.id] = question  # type: ignore
        return question

    async def get_by_id(self, question_id: UUID) -> Question | None:
        return self.questions.get(question_id)  # type: ignore


class MockInterviewRepository:
    """Mock interview repository for testing."""

    def __init__(self) -> None:
        self.interviews: dict[UUID, Interview] = {}

    async def save(self, interview: Interview) -> Interview:
        self.interviews[interview.id] = interview
        return interview

    async def update(self, interview: Interview) -> Interview:
        self.interviews[interview.id] = interview
        return interview

    async def get_by_id(self, interview_id: UUID) -> Interview | None:
        return self.interviews.get(interview_id)


class MockAnswerRepository:
    """Mock answer repository for testing."""

    def __init__(self) -> None:
        self.answers: dict[UUID, Answer] = {}

    async def save(self, answer: Answer) -> Answer:
        self.answers[answer.id] = answer
        return answer

    async def get_by_interview_id(self, interview_id: UUID) -> list[Answer]:
        return [a for a in self.answers.values() if a.interview_id == interview_id]


class MockFollowUpQuestionRepository:
    """Mock follow-up question repository for testing."""

    def __init__(self) -> None:
        self.follow_ups: dict[UUID, list[FollowUpQuestion]] = {}

    async def save(self, follow_up: FollowUpQuestion) -> FollowUpQuestion:
        parent_id = follow_up.parent_question_id
        if parent_id not in self.follow_ups:
            self.follow_ups[parent_id] = []
        self.follow_ups[parent_id].append(follow_up)
        return follow_up

    async def get_by_parent_question_id(
        self, parent_question_id: UUID
    ) -> list[FollowUpQuestion]:
        return self.follow_ups.get(parent_question_id, [])


class MockCVAnalysisRepository:
    """Mock CV analysis repository for testing."""

    def __init__(self) -> None:
        self.analyses: dict[UUID, CVAnalysis] = {}

    async def save(self, cv_analysis: CVAnalysis) -> CVAnalysis:
        self.analyses[cv_analysis.id] = cv_analysis
        return cv_analysis

    async def get_by_id(self, cv_analysis_id: UUID) -> CVAnalysis | None:
        return self.analyses.get(cv_analysis_id)


class MockVectorSearch:
    """Mock vector search for testing."""

    async def get_embedding(self, text: str) -> list[float]:
        """Return mock embedding."""
        # Simple hash-based mock embedding
        hash_val = hash(text.lower()[:50])
        return [float((hash_val >> i) & 1) for i in range(128)]

    async def find_similar_questions(
        self,
        query_embedding: list[float],
        top_k: int = 5,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """Return mock similar questions."""
        # Return empty list by default (simulating empty vector DB)
        # Tests can override this behavior
        return []

    async def store_question_embedding(
        self,
        question_id: UUID,
        embedding: list[float],
        metadata: dict[str, Any],
    ) -> None:
        """Mock embedding storage (no-op)."""
        pass

    async def find_similar_answers(
        self,
        answer_embedding: list[float],
        reference_embeddings: list[list[float]],
    ) -> float:
        """Return mock similarity score based on text length."""
        # Simple mock: longer answers get higher similarity
        return 0.85 if len(answer_embedding) > 100 else 0.45


class MockLLM:
    """Mock LLM for testing."""

    async def evaluate_answer(
        self, question: Question, answer_text: str, context: dict[str, Any]
    ) -> AnswerEvaluation:
        """Return mock evaluation."""
        score = 85.0 if len(answer_text) > 50 else 55.0
        return AnswerEvaluation(
            score=score,
            semantic_similarity=score / 100,
            completeness=score / 100,
            relevance=0.9,
            sentiment="confident" if score > 70 else "uncertain",
            reasoning="Mock evaluation",
            strengths=["Good understanding"],
            weaknesses=["Could be more detailed"],
            improvement_suggestions=["Add examples"],
        )

    async def generate_question(
        self, context: dict[str, Any], skill: str, difficulty: str, exemplars: list[dict[str, Any]] | None = None
    ) -> str:
        """Return mock question."""
        base = f"Mock question about {skill} at {difficulty} level"
        if exemplars:
            base += f" [with {len(exemplars)} exemplars]"
        return base

    async def generate_ideal_answer(
        self, question_text: str, context: dict[str, Any]
    ) -> str:
        """Return mock ideal answer."""
        return f"Mock ideal answer for: {question_text[:50]}..."

    async def generate_rationale(
        self, question_text: str, ideal_answer: str
    ) -> str:
        """Return mock rationale."""
        return "Mock rationale explaining why this is an ideal answer"

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Mock gap detection based on answer length."""
        # Simple heuristic: short answers have gaps
        word_count = len(answer_text.split())

        if word_count < 30:
            # Simulate gaps for short answers
            return {
                "concepts": keyword_gaps[:2] if keyword_gaps else ["depth", "examples"],
                "keywords": keyword_gaps[:5],
                "confirmed": True,
                "severity": "moderate",
            }
        else:
            # Good answer, no gaps
            return {
                "concepts": [],
                "keywords": [],
                "confirmed": False,
                "severity": "minor",
            }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
    ) -> str:
        """Mock follow-up question generation."""
        concepts_str = ', '.join(missing_concepts[:2]) if missing_concepts else "that concept"
        return f"Can you elaborate more on {concepts_str}? Please provide specific examples."

    async def generate_interview_recommendations(
        self,
        context: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Mock interview recommendations generation."""
        return {
            "strengths": ["Clear communication", "Good problem-solving", "Strong technical knowledge"],
            "weaknesses": ["Could provide more examples", "Needs to elaborate on concepts"],
            "study_topics": ["Advanced algorithms", "System design patterns", "Best practices"],
            "technique_tips": ["Speak more slowly", "Use concrete examples", "Structure answers better"],
        }


@pytest.fixture
def mock_question_repo() -> MockQuestionRepository:
    """Mock question repository fixture."""
    return MockQuestionRepository()


@pytest.fixture
def mock_interview_repo() -> MockInterviewRepository:
    """Mock interview repository fixture."""
    return MockInterviewRepository()


@pytest.fixture
def mock_answer_repo() -> MockAnswerRepository:
    """Mock answer repository fixture."""
    return MockAnswerRepository()


@pytest.fixture
def mock_cv_analysis_repo() -> MockCVAnalysisRepository:
    """Mock CV analysis repository fixture."""
    return MockCVAnalysisRepository()


@pytest.fixture
def mock_vector_search() -> MockVectorSearch:
    """Mock vector search fixture."""
    return MockVectorSearch()


@pytest.fixture
def mock_llm() -> MockLLM:
    """Mock LLM fixture."""
    return MockLLM()


@pytest.fixture
def mock_follow_up_question_repo() -> MockFollowUpQuestionRepository:
    """Mock follow-up question repository fixture."""
    return MockFollowUpQuestionRepository()
</file>

<file path="src/domain/models/interview.py">
"""Interview domain model."""

from datetime import datetime
from enum import Enum
from typing import Any
from uuid import UUID, uuid4

from pydantic import BaseModel, Field


class InterviewStatus(str, Enum):
    """Interview status enumeration."""

    PLANNING = "PLANNING"  # Interview in planning process
    IDLE = "IDLE"  # Waiting to start questioning
    QUESTIONING = "QUESTIONING"  # Asking a question
    EVALUATING = "EVALUATING"  # Evaluating received answer(s)
    FOLLOW_UP = "FOLLOW_UP"  # Awaiting follow-up response
    COMPLETE = "COMPLETE"  # Interview finished
    CANCELLED = "CANCELLED"  # Interview cancelled


class Interview(BaseModel):
    """Represents an interview session.

    This is the core aggregate root for the interview domain.
    It encapsulates all interview-related business logic.
    """

    # State transition rules
    VALID_TRANSITIONS: dict[InterviewStatus, list[InterviewStatus]] = {
        InterviewStatus.PLANNING: [InterviewStatus.IDLE, InterviewStatus.CANCELLED],
        InterviewStatus.IDLE: [InterviewStatus.QUESTIONING, InterviewStatus.CANCELLED],
        InterviewStatus.QUESTIONING: [InterviewStatus.EVALUATING, InterviewStatus.CANCELLED],
        InterviewStatus.EVALUATING: [
            InterviewStatus.FOLLOW_UP,
            InterviewStatus.QUESTIONING,
            InterviewStatus.COMPLETE,
            InterviewStatus.CANCELLED,
        ],
        InterviewStatus.FOLLOW_UP: [InterviewStatus.EVALUATING, InterviewStatus.CANCELLED],
        InterviewStatus.COMPLETE: [],  # Terminal state
        InterviewStatus.CANCELLED: [],  # Terminal state
    }

    id: UUID = Field(default_factory=uuid4)
    candidate_id: UUID
    status: InterviewStatus = InterviewStatus.IDLE
    cv_analysis_id: UUID | None = None
    question_ids: list[UUID] = Field(default_factory=list)
    answer_ids: list[UUID] = Field(default_factory=list)
    current_question_index: int = 0

    # NEW: Pre-planning metadata for adaptive interviews
    plan_metadata: dict[str, Any] = Field(default_factory=dict)  # {n, generated_at, strategy}
    adaptive_follow_ups: list[UUID] = Field(default_factory=list)  # Follow-up question IDs

    # NEW: Follow-up tracking for current session
    current_parent_question_id: UUID | None = None
    current_followup_count: int = 0

    started_at: datetime | None = None
    completed_at: datetime | None = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    class Config:
        """Pydantic configuration."""

        frozen = False

    def transition_to(self, new_status: InterviewStatus) -> None:
        """Validate and perform state transition.

        Args:
            new_status: Target status to transition to

        Raises:
            ValueError: If transition is invalid
        """
        if new_status not in self.VALID_TRANSITIONS.get(self.status, []):
            raise ValueError(
                f"Invalid transition: {self.status} â†’ {new_status}. "
                f"Valid transitions from {self.status}: {self.VALID_TRANSITIONS.get(self.status, [])}"
            )
        self.status = new_status
        self.updated_at = datetime.utcnow()

    def mark_idle(self, cv_analysis_id: UUID) -> None:
        """Mark interview as idle after planning is complete.
        """
        self.transition_to(InterviewStatus.IDLE)
        self.updated_at = datetime.utcnow()

    def start(self) -> None:
        """Start the interview.

        Raises:
            ValueError: If interview is not ready to start
        """

        self.transition_to(InterviewStatus.QUESTIONING)
        now = datetime.utcnow()
        self.started_at = now
        self.updated_at = now

    def mark_evaluating(self) -> None:
        """Mark interview as evaluating after an answer is added.
        """
        self.transition_to(InterviewStatus.EVALUATING)
        self.updated_at = datetime.utcnow()

    def complete(self) -> None:
        """Complete the interview.

        Raises:
            ValueError: If interview is not ready to complete
        """

        self.transition_to(InterviewStatus.COMPLETE)
        now = datetime.utcnow()
        self.completed_at = now
        self.updated_at = now

    def cancel(self) -> None:
        """Cancel the interview."""
        self.transition_to(InterviewStatus.CANCELLED)
        self.updated_at = datetime.utcnow()

    def add_question(self, question_id: UUID) -> None:
        """Add a question to the interview.

        Args:
            question_id: ID of the question to add
        """
        self.question_ids.append(question_id)
        self.updated_at = datetime.utcnow()

    def add_answer(self, answer_id: UUID) -> None:
        """Add an answer to the interview.

        Args:
            answer_id: ID of the answer to add
        """
        self.answer_ids.append(answer_id)
        self.current_question_index += 1
        self.updated_at = datetime.utcnow()

    def has_more_questions(self) -> bool:
        """Check if there are more questions to ask.

        Returns:
            True if more questions remain, False otherwise
        """
        return self.current_question_index < len(self.question_ids)

    def get_current_question_id(self) -> UUID | None:
        """Get the current question ID.

        Returns:
            Current question ID or None if no questions remain
        """
        if self.has_more_questions():
            return self.question_ids[self.current_question_index]
        return None

    def get_progress_percentage(self) -> float:
        """Calculate interview progress percentage.

        Returns:
            Progress as a percentage (0-100)
        """
        if not self.question_ids:
            return 0.0
        return (self.current_question_index / len(self.question_ids)) * 100

    def is_active(self) -> bool:
        """Check if interview is currently active.

        Returns:
            True if interview is in progress, False otherwise
        """
        return self.status in {
            InterviewStatus.QUESTIONING,
            InterviewStatus.EVALUATING,
            InterviewStatus.FOLLOW_UP,
        }

    def ask_followup(self, followup_id: UUID, parent_question_id: UUID) -> None:
        """Add follow-up question with count tracking.

        Args:
            followup_id: UUID of follow-up question
            parent_question_id: UUID of main question that spawned this follow-up

        Raises:
            ValueError: If max 3 follow-ups per question exceeded
        """
        # Handle parent question change
        if self.current_parent_question_id != parent_question_id:
            self.current_parent_question_id = parent_question_id
            self.current_followup_count = 1
        else:
            # Same parent, increment counter
            if self.current_followup_count >= 3:
                raise ValueError(
                    f"Max 3 follow-ups per question. Current: {self.current_followup_count}"
                )
            self.current_followup_count += 1

        self.adaptive_follow_ups.append(followup_id)
        self.transition_to(InterviewStatus.FOLLOW_UP)
        self.updated_at = datetime.utcnow()

    def answer_followup(self) -> None:
        """Record follow-up answered, return to evaluation.

        Raises:
            ValueError: If not in FOLLOW_UP state
        """
        if self.status != InterviewStatus.FOLLOW_UP:
            raise ValueError(f"Not in FOLLOW_UP state: {self.status}")
        self.transition_to(InterviewStatus.EVALUATING)
        self.updated_at = datetime.utcnow()

    def can_ask_more_followups(self) -> bool:
        """Check if more follow-ups allowed for current parent question.

        Returns:
            True if more follow-ups can be asked (count < 3), False otherwise
        """
        return self.current_followup_count < 3

    def add_adaptive_followup(self, question_id: UUID) -> None:
        """Add adaptive follow-up question to interview.

        DEPRECATED: Use ask_followup() instead for proper count tracking.

        Args:
            question_id: UUID of follow-up question

        Raises:
            ValueError: If follow-up limit exceeded (max 3 per main question)
        """
        self.adaptive_follow_ups.append(question_id)
        self.transition_to(InterviewStatus.FOLLOW_UP)
        self.updated_at = datetime.utcnow()

    def mark_follow_up_answered(self) -> None:
        """Return to evaluation after a follow-up response.

        DEPRECATED: Use answer_followup() instead.
        """
        if self.status != InterviewStatus.FOLLOW_UP:
            raise ValueError(f"Cannot mark follow-up answered in status: {self.status}")
        self.transition_to(InterviewStatus.EVALUATING)
        self.updated_at = datetime.utcnow()

    def proceed_to_next_question(self) -> None:
        """Move to next question or complete interview.

        Resets follow-up tracking when advancing to next main question.

        Raises:
            ValueError: If not in EVALUATING state
        """
        if self.status != InterviewStatus.EVALUATING:
            raise ValueError(f"Cannot proceed from status: {self.status}")

        # Reset follow-up tracking
        self.current_parent_question_id = None
        self.current_followup_count = 0
        now = datetime.utcnow()

        if self.has_more_questions():
            self.transition_to(InterviewStatus.QUESTIONING)
            self.updated_at = now
        else:
            self.transition_to(InterviewStatus.COMPLETE)
            self.completed_at = now
            self.updated_at = now

    def proceed_after_evaluation(self) -> None:
        """Advance interview after evaluation is complete.

        DEPRECATED: Use proceed_to_next_question() instead for proper counter reset.
        """
        if self.status != InterviewStatus.EVALUATING:
            raise ValueError(f"Cannot proceed from status: {self.status}")

        if self.has_more_questions():
            self.transition_to(InterviewStatus.QUESTIONING)
            return

        self.transition_to(InterviewStatus.COMPLETE)
        now = datetime.utcnow()
        self.completed_at = now
        self.updated_at = now

    def is_planned(self) -> bool:
        """Check if interview has planning metadata.

        Returns:
            True if plan_metadata contains required keys
        """
        return "n" in self.plan_metadata and "generated_at" in self.plan_metadata

    @property
    def planned_question_count(self) -> int:
        """Get number of planned questions.

        Returns:
            Value of n from plan_metadata, or 0 if not planned
        """
        n = self.plan_metadata.get("n", 0)
        return int(n) if n is not None else 0
</file>

<file path="src/domain/ports/llm_port.py">
"""LLM (Large Language Model) port interface."""

from abc import ABC, abstractmethod
from typing import Any
from uuid import UUID

from ..models.answer import AnswerEvaluation
from ..models.evaluation import FollowUpEvaluationContext
from ..models.question import Question


class LLMPort(ABC):
    """Interface for Large Language Model providers.

    This port abstracts LLM interactions, allowing easy switching between
    providers like OpenAI, Claude, Llama, etc.
    """

    @abstractmethod
    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
        exemplars: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate an interview question.

        Args:
            context: Interview context (CV analysis, previous answers, etc.)
            skill: Target skill to test
            difficulty: Question difficulty level
            exemplars: Optional list of similar questions for inspiration.
                      Each dict should contain: 'text', 'skills', 'difficulty', 'similarity_score'.
                      Helps LLM understand desired question style and depth.
                      Default: None (generate without exemplars)

        Returns:
            Generated question text
        """
        pass

    @abstractmethod
    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
        followup_context: FollowUpEvaluationContext | None = None,
    ) -> AnswerEvaluation:
        """Evaluate a candidate's answer.

        Args:
            question: The question that was asked
            answer_text: Candidate's answer
            context: Additional context for evaluation
            followup_context: Optional context for follow-up question evaluation.
                Includes previous evaluations, cumulative gaps, attempt number.
                Used to provide LLM with history and apply attempt-based penalties.

        Returns:
            Evaluation results with score and feedback
        """
        pass

    @abstractmethod
    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive feedback report.

        Args:
            interview_id: ID of the interview
            questions: All questions asked
            answers: All answers with evaluations

        Returns:
            Formatted feedback report
        """
        pass

    @abstractmethod
    async def summarize_cv(self, cv_text: str) -> str:
        """Generate a summary of a CV.

        Args:
            cv_text: Extracted CV text

        Returns:
            Summary of the CV
        """
        pass

    @abstractmethod
    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract skills from CV text using NLP.

        Args:
            text: CV text to analyze

        Returns:
            List of extracted skills with metadata
        """
        pass

    @abstractmethod
    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate ideal answer for a question.

        Args:
            question_text: The interview question
            context: CV summary, skills, etc.

        Returns:
            Ideal answer text (150-300 words)
        """
        pass

    @abstractmethod
    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate rationale explaining why answer is ideal.

        Args:
            question_text: The question
            ideal_answer: The ideal answer

        Returns:
            Rationale text (50-100 words)
        """
        pass

    @abstractmethod
    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Detect missing concepts in answer using LLM.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with keys:
                - concepts: list[str] - Missing key concepts
                - keywords: list[str] - Subset of confirmed missing keywords
                - confirmed: bool - Whether gaps are confirmed
                - severity: str - "minor" | "moderate" | "major"
        """
        pass

    @abstractmethod
    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
        cumulative_gaps: list[str] | None = None,
        previous_follow_ups: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate targeted follow-up question.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question (or latest follow-up)
            missing_concepts: List of concepts missing from current answer
            severity: Gap severity ("minor" | "moderate" | "major")
            order: Follow-up order in sequence (1, 2, 3, ...)
            cumulative_gaps: All unique gaps accumulated across follow-up cycle (optional)
            previous_follow_ups: Previous follow-up questions and answers for context (optional)

        Returns:
            Follow-up question text
        """
        pass

    @abstractmethod
    async def generate_interview_recommendations(
        self,
        context: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Generate personalized interview recommendations.

        Args:
            context: Interview context including:
                - interview_id: str
                - total_answers: int
                - gap_progression: dict (gaps filled, remaining, etc.)
                - evaluations: list[dict] (scores, strengths, weaknesses per answer)

        Returns:
            Dict with keys:
                - strengths: list[str] (top 3-5 strengths)
                - weaknesses: list[str] (top 3-5 weaknesses)
                - study_topics: list[str] (topic-specific study recommendations)
                - technique_tips: list[str] (voice, pacing, structure tips)
        """
        pass
</file>

<file path="src/adapters/llm/openai_adapter.py">
"""OpenAI LLM adapter implementation."""

import json
from typing import Any
from uuid import UUID

from openai import AsyncOpenAI

from ...domain.models.answer import AnswerEvaluation
from ...domain.models.evaluation import FollowUpEvaluationContext
from ...domain.models.question import Question
from ...domain.ports.llm_port import LLMPort


class OpenAIAdapter(LLMPort):
    """OpenAI implementation of LLM port.

    This adapter encapsulates all OpenAI-specific logic, making it easy
    to swap for another LLM provider without touching domain logic.
    """

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4",
        temperature: float = 0.7,
    ):
        """Initialize OpenAI adapter.

        Args:
            api_key: OpenAI API key
            model: Model to use (default: gpt-4)
            temperature: Sampling temperature (default: 0.7)
        """
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature

    async def generate_question(
        self,
        context: dict[str, Any],
        skill: str,
        difficulty: str,
        exemplars: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate an interview question using OpenAI.

        Args:
            context: Interview context
            skill: Target skill to test
            difficulty: Question difficulty level
            exemplars: Optional list of similar questions for inspiration

        Returns:
            Generated question text
        """
        system_prompt = """You are an expert technical interviewer.
        Generate a clear, relevant interview question based on the context provided."""

        user_prompt = f"""
        Generate a {difficulty} difficulty interview question to test: {skill}

        Context:
        - Candidate's background: {context.get('cv_summary', 'Not provided')}
        - Previous topics covered: {context.get('covered_topics', [])}
        - Interview stage: {context.get('stage', 'early')}
        """

        # Add exemplars if provided
        if exemplars:
            user_prompt += "\n\nSimilar questions for inspiration (do NOT copy exactly):\n"
            for i, ex in enumerate(exemplars[:3], 1):  # Limit to 3 exemplars
                user_prompt += f"{i}. \"{ex.get('text', '')}\" ({ex.get('difficulty', 'UNKNOWN')})\n"
            user_prompt += "\nGenerate a NEW question inspired by the style and structure above.\n"

        user_prompt += "\nReturn only the question text, no additional explanation."

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self.temperature,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def evaluate_answer(
        self,
        question: Question,
        answer_text: str,
        context: dict[str, Any],
        followup_context: FollowUpEvaluationContext | None = None,
    ) -> AnswerEvaluation:
        """Evaluate an answer using OpenAI.

        Args:
            question: The question that was asked
            answer_text: Candidate's answer
            context: Additional context
            followup_context: Optional context for follow-up evaluation with previous attempts

        Returns:
            Evaluation results
        """
        system_prompt = """You are an expert technical interviewer evaluating candidate answers.
        Provide objective, constructive feedback with specific scores."""

        user_prompt = f"""
        Question: {question.text}
        Question Type: {question.question_type}
        Difficulty: {question.difficulty}
        Expected Skills: {', '.join(question.skills)}

        Candidate's Answer: {answer_text}

        {"Ideal Answer: " + question.ideal_answer if question.ideal_answer else ""}
        """

        # Add follow-up context if this is a follow-up question
        if followup_context:
            user_prompt += f"""

        **FOLLOW-UP CONTEXT** (Attempt #{followup_context.attempt_number}):
        This is a follow-up question after {followup_context.attempt_number - 1} previous attempt(s).

        Previous Scores: {', '.join(f'{score:.1f}' for score in followup_context.previous_scores)}
        Average Previous Score: {followup_context.average_previous_score:.1f}

        Persistent Gaps from Previous Attempts:
        {chr(10).join(f'  - {concept}' for concept in followup_context.get_persistent_gap_concepts())}

        When evaluating this follow-up answer:
        1. Focus on whether the candidate addressed the persistent gaps above
        2. Check if they demonstrate learning from previous feedback
        3. Evaluate both new content AND correction of previous gaps
        4. Be stricter if they repeat the same mistakes (this is attempt #{followup_context.attempt_number})
        """

        user_prompt += """

        Evaluate this answer and provide:
        1. Overall score (0-100)
        2. Completeness score (0-1)
        3. Relevance score (0-1)
        4. Sentiment (confident/uncertain/nervous)
        5. 2-3 strengths
        6. 2-3 weaknesses
        7. 2-3 improvement suggestions
        8. Brief reasoning for the score

        Return as JSON with keys: score, completeness, relevance, sentiment, strengths, weaknesses, improvements, reasoning
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Lower temperature for more consistent evaluation
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)

        return AnswerEvaluation(
            score=float(result.get("score", 0)),
            semantic_similarity=0.0,  # Will be calculated by vector search
            completeness=float(result.get("completeness", 0)),
            relevance=float(result.get("relevance", 0)),
            sentiment=result.get("sentiment"),
            reasoning=result.get("reasoning"),
            strengths=result.get("strengths", []),
            weaknesses=result.get("weaknesses", []),
            improvement_suggestions=result.get("improvements", []),
        )

    async def generate_feedback_report(
        self,
        interview_id: UUID,
        questions: list[Question],
        answers: list[dict[str, Any]],
    ) -> str:
        """Generate comprehensive feedback report.

        Args:
            interview_id: ID of the interview
            questions: All questions asked
            answers: All answers with evaluations

        Returns:
            Formatted feedback report
        """
        system_prompt = """You are an expert career coach providing comprehensive interview feedback.
        Create a detailed, actionable report that helps candidates improve."""

        # Prepare interview summary
        qa_pairs = []
        for i, (q, a) in enumerate(zip(questions, answers)):
            qa_pairs.append(
                f"Q{i+1}: {q.text}\n"
                f"Answer Score: {a.get('evaluation', {}).get('score', 'N/A')}\n"
                f"Evaluation: {a.get('evaluation', {}).get('reasoning', 'N/A')}\n"
            )

        user_prompt = f"""
        Generate a comprehensive interview feedback report for interview {interview_id}.

        Interview Performance:
        {chr(10).join(qa_pairs)}

        Include:
        1. Overall Performance Summary
        2. Key Strengths (with examples)
        3. Areas for Improvement (with specific guidance)
        4. Skill-by-Skill Breakdown
        5. Actionable Next Steps

        Be encouraging but honest. Provide specific examples and actionable advice.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
        )

        content = response.choices[0].message.content
        return content or ""

    async def summarize_cv(self, cv_text: str) -> str:
        """Generate a summary of a CV.

        Args:
            cv_text: Extracted CV text

        Returns:
            Summary of the CV
        """
        system_prompt = """You are an expert recruiter analyzing candidate CVs.
        Create concise, informative summaries."""

        user_prompt = f"""
        Summarize this CV in 3-4 sentences, highlighting:
        - Key technical skills and experience
        - Years of experience and seniority level
        - Notable projects or achievements

        CV:
        {cv_text[:2000]}  # Limit to first 2000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.5,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def extract_skills_from_text(self, text: str) -> list[dict[str, str]]:
        """Extract skills from CV text using OpenAI.

        Args:
            text: CV text to analyze

        Returns:
            List of extracted skills with metadata
        """
        system_prompt = """You are an expert at extracting structured information from CVs.
        Identify technical skills, soft skills, and tools mentioned."""

        user_prompt = f"""
        Extract all skills from this CV text. For each skill, identify:
        - name: The skill name
        - category: "technical", "soft", or "language"
        - proficiency: "beginner", "intermediate", or "expert" (infer from context)

        Return as JSON array with keys: name, category, proficiency

        CV Text:
        {text[:3000]}  # Limit to first 3000 chars
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)
        skills: list[dict[str, str]] = result.get("skills", [])
        return skills

    async def generate_ideal_answer(
        self,
        question_text: str,
        context: dict[str, Any],
    ) -> str:
        """Generate ideal answer for a question.

        Args:
            question_text: The interview question
            context: CV summary, skills, etc.

        Returns:
            Ideal answer text (150-300 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer creating reference answers.
        Generate comprehensive, technically accurate ideal answers."""

        user_prompt = f"""
        Question: {question_text}

        Candidate Background:
        - Summary: {context.get('summary', 'Not provided')}
        - Key Skills: {', '.join(context.get('skills', [])[:5])}
        - Experience: {context.get('experience', 'Not specified')} years

        Generate an ideal answer for this interview question. The answer should:
        - Be 150-300 words
        - Demonstrate expert-level understanding
        - Cover key concepts comprehensively
        - Include practical examples if relevant
        - Be technically accurate

        Output only the ideal answer text.
        """

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,  # Low for consistency
            max_tokens=500,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def generate_rationale(
        self,
        question_text: str,
        ideal_answer: str,
    ) -> str:
        """Generate rationale explaining why answer is ideal.

        Args:
            question_text: The question
            ideal_answer: The ideal answer

        Returns:
            Rationale text (50-100 words)

        Raises:
            Exception: If generation fails
        """
        system_prompt = """You are an expert technical interviewer explaining evaluation criteria.
        Explain why an answer demonstrates mastery."""

        user_prompt = f"""
        Question: {question_text}
        Ideal Answer: {ideal_answer}

        Explain WHY this is an ideal answer in 50-100 words. Focus on:
        - What key concepts are covered
        - Why this demonstrates mastery
        - What would be missing in a weaker answer

        Output only the rationale text.
        """

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",  # Cheaper model for rationale
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=200,
        )

        content = response.choices[0].message.content
        return content.strip() if content else ""

    async def detect_concept_gaps(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Detect concept gaps using OpenAI with JSON mode.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question that was asked
            keyword_gaps: Potential missing keywords from keyword analysis

        Returns:
            Dict with concept gap analysis
        """
        prompt = f"""
Question: {question_text}
Ideal Answer: {ideal_answer}
Candidate Answer: {answer_text}
Potential missing keywords: {', '.join(keyword_gaps[:10])}

Analyze and identify:
1. Key concepts in ideal answer missing from candidate answer
2. Whether missing keywords represent real conceptual gaps

Return as JSON:
- "concepts": list of missing concepts
- "confirmed": boolean
- "severity": "minor" | "moderate" | "major"
"""

        system_prompt = """You are an expert technical interviewer analyzing completeness.
Identify real conceptual gaps, not just missing synonyms."""

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content or "{}"
        result = json.loads(content)

        return {
            "concepts": result.get("concepts", []),
            "keywords": keyword_gaps[:5],
            "confirmed": result.get("confirmed", False),
            "severity": result.get("severity", "minor"),
        }

    async def generate_followup_question(
        self,
        parent_question: str,
        answer_text: str,
        missing_concepts: list[str],
        severity: str,
        order: int,
        cumulative_gaps: list[str] | None = None,
        previous_follow_ups: list[dict[str, Any]] | None = None,
    ) -> str:
        """Generate follow-up question using OpenAI with cumulative context.

        Args:
            parent_question: Original question text
            answer_text: Candidate's answer to parent question (or latest follow-up)
            missing_concepts: List of concepts missing from current answer
            severity: Gap severity
            order: Follow-up order in sequence
            cumulative_gaps: All unique gaps accumulated across follow-up cycle
            previous_follow_ups: Previous follow-up questions and answers for context

        Returns:
            Follow-up question text
        """
        # Build cumulative context if available
        cumulative_context = ""
        if cumulative_gaps and len(cumulative_gaps) > 0:
            cumulative_context = f"\nAll Missing Concepts (cumulative): {', '.join(cumulative_gaps)}"

        # Build previous follow-ups context if available
        previous_context = ""
        if previous_follow_ups and len(previous_follow_ups) > 0:
            previous_context = "\n\nPrevious Follow-ups:"
            for i, fu in enumerate(previous_follow_ups, 1):
                previous_context += f"\n  #{i}: {fu.get('question', 'N/A')}"
                previous_context += f"\n      Answer: {fu.get('answer', 'N/A')[:100]}..."

        prompt = f"""
Original Question: {parent_question}
Latest Answer: {answer_text}
Current Missing Concepts: {', '.join(missing_concepts)}
Gap Severity: {severity}{cumulative_context}{previous_context}

Generate focused follow-up question (#{order}) addressing the most critical missing concepts.
The question should:
- Be specific and concise
- Prioritize concepts: {', '.join(missing_concepts[:2])}
- Avoid repeating previous follow-up questions
- Be progressively more targeted (this is follow-up #{order} of max 3)

Return only the question text.
"""

        system_prompt = """You are an expert technical interviewer generating adaptive follow-ups.
Ask questions that probe specific missing concepts while considering the full interview context."""

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_tokens=150,
        )

        content = response.choices[0].message.content
        return content.strip() if content else "Can you elaborate on that?"

    async def generate_interview_recommendations(
        self,
        context: dict[str, Any],
    ) -> dict[str, list[str]]:
        """Generate personalized interview recommendations using OpenAI.

        Args:
            context: Interview context including:
                - interview_id: str
                - total_answers: int
                - gap_progression: dict
                - evaluations: list[dict]

        Returns:
            Dict with strengths, weaknesses, study topics, and technique tips
        """
        evaluations = context.get("evaluations", [])
        gap_progression = context.get("gap_progression", {})

        # Build evaluation summary
        eval_summary = "\n".join(
            [
                f"- Question {i+1}: Score {e['score']:.1f}/100"
                f"\n  Strengths: {', '.join(e.get('strengths', []))}"
                f"\n  Weaknesses: {', '.join(e.get('weaknesses', []))}"
                for i, e in enumerate(evaluations)
            ]
        )

        prompt = f"""
Interview Performance Analysis

Total Questions Answered: {len(evaluations)}
Gap Progression:
- Questions with Follow-ups: {gap_progression.get('questions_with_followups', 0)}
- Gaps Filled: {gap_progression.get('gaps_filled', 0)}
- Gaps Remaining: {gap_progression.get('gaps_remaining', 0)}

Detailed Evaluations:
{eval_summary}

Generate personalized interview feedback in JSON format with these exact keys:
{{
    "strengths": ["strength 1", "strength 2", ...],  // 3-5 specific strengths
    "weaknesses": ["weakness 1", "weakness 2", ...],  // 3-5 specific weaknesses
    "study_topics": ["topic 1", "topic 2", ...],  // 3-7 specific topics to study
    "technique_tips": ["tip 1", "tip 2", ...]  // 2-5 interview technique improvements
}}

Make recommendations:
- Specific and actionable (not generic)
- Based on actual performance data
- Prioritized by impact
- Constructive and encouraging

Return ONLY valid JSON."""

        system_prompt = """You are an expert interview coach analyzing candidate performance.
Provide specific, data-driven recommendations that help candidates improve."""

        response = await self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
            max_tokens=800,
            response_format={"type": "json_object"},
        )

        content = response.choices[0].message.content
        if not content:
            # Fallback recommendations
            return {
                "strengths": ["Good technical foundation"],
                "weaknesses": ["Could provide more detail in answers"],
                "study_topics": ["Review core concepts"],
                "technique_tips": ["Practice explaining concepts clearly"],
            }

        try:
            recommendations = json.loads(content)
            return recommendations
        except json.JSONDecodeError:
            # Fallback if JSON parsing fails
            return {
                "strengths": ["Good technical foundation"],
                "weaknesses": ["Could provide more detail in answers"],
                "study_topics": ["Review core concepts"],
                "technique_tips": ["Practice explaining concepts clearly"],
            }
</file>

<file path="src/adapters/persistence/mappers.py">
"""Mappers to convert between domain models and SQLAlchemy models.

These mappers handle the translation between the domain layer
(Pydantic models) and the persistence layer (SQLAlchemy models).
"""


from ...domain.models.answer import Answer, AnswerEvaluation
from ...domain.models.candidate import Candidate
from ...domain.models.cv_analysis import CVAnalysis, ExtractedSkill
from ...domain.models.follow_up_question import FollowUpQuestion
from ...domain.models.interview import Interview, InterviewStatus
from ...domain.models.question import DifficultyLevel, Question, QuestionType
from .models import (
    AnswerModel,
    CandidateModel,
    CVAnalysisModel,
    FollowUpQuestionModel,
    InterviewModel,
    QuestionModel,
)


class CandidateMapper:
    """Mapper for Candidate domain model and CandidateModel database model."""

    @staticmethod
    def to_domain(db_model: CandidateModel) -> Candidate:
        """Convert database model to domain model.

        Args:
            db_model: SQLAlchemy model instance

        Returns:
            Domain model instance
        """
        return Candidate(
            id=db_model.id,
            name=db_model.name,
            email=db_model.email,
            cv_file_path=db_model.cv_file_path,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Candidate) -> CandidateModel:
        """Convert domain model to database model.

        Args:
            domain_model: Domain model instance

        Returns:
            SQLAlchemy model instance
        """
        return CandidateModel(
            id=domain_model.id,
            name=domain_model.name,
            email=domain_model.email,
            cv_file_path=domain_model.cv_file_path,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: CandidateModel, domain_model: Candidate) -> None:
        """Update database model from domain model.

        Args:
            db_model: SQLAlchemy model to update
            domain_model: Domain model with new data
        """
        db_model.name = domain_model.name
        db_model.email = domain_model.email
        db_model.cv_file_path = domain_model.cv_file_path
        db_model.updated_at = domain_model.updated_at


class QuestionMapper:
    """Mapper for Question domain model and QuestionModel database model."""

    @staticmethod
    def to_domain(db_model: QuestionModel) -> Question:
        """Convert database model to domain model."""
        return Question(
            id=db_model.id,
            text=db_model.text,
            question_type=QuestionType(db_model.question_type),
            difficulty=DifficultyLevel(db_model.difficulty),
            skills=list(db_model.skills) if db_model.skills else [],
            tags=list(db_model.tags) if db_model.tags else [],
            evaluation_criteria=db_model.evaluation_criteria,
            version=db_model.version,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            ideal_answer=db_model.ideal_answer,
            rationale=db_model.rationale,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Question) -> QuestionModel:
        """Convert domain model to database model."""
        return QuestionModel(
            id=domain_model.id,
            text=domain_model.text,
            question_type=domain_model.question_type.value,
            difficulty=domain_model.difficulty.value,
            skills=domain_model.skills,
            tags=domain_model.tags,
            evaluation_criteria=domain_model.evaluation_criteria,
            version=domain_model.version,
            embedding=domain_model.embedding,
            ideal_answer=domain_model.ideal_answer,
            rationale=domain_model.rationale,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: QuestionModel, domain_model: Question) -> None:
        """Update database model from domain model."""
        db_model.text = domain_model.text
        db_model.question_type = domain_model.question_type.value
        db_model.difficulty = domain_model.difficulty.value
        db_model.skills = domain_model.skills
        db_model.tags = domain_model.tags
        db_model.evaluation_criteria = domain_model.evaluation_criteria
        db_model.version = domain_model.version
        db_model.embedding = domain_model.embedding
        db_model.ideal_answer = domain_model.ideal_answer
        db_model.rationale = domain_model.rationale
        db_model.updated_at = domain_model.updated_at


class InterviewMapper:
    """Mapper for Interview domain model and InterviewModel database model."""

    @staticmethod
    def to_domain(db_model: InterviewModel) -> Interview:
        """Convert database model to domain model."""
        return Interview(
            id=db_model.id,
            candidate_id=db_model.candidate_id,
            status=InterviewStatus(db_model.status),
            cv_analysis_id=db_model.cv_analysis_id,
            question_ids=list(db_model.question_ids) if db_model.question_ids else [],
            answer_ids=list(db_model.answer_ids) if db_model.answer_ids else [],
            current_question_index=db_model.current_question_index,
            plan_metadata=dict(db_model.plan_metadata) if db_model.plan_metadata else {},
            adaptive_follow_ups=list(db_model.adaptive_follow_ups) if db_model.adaptive_follow_ups else [],
            current_parent_question_id=db_model.current_parent_question_id,
            current_followup_count=db_model.current_followup_count,
            started_at=db_model.started_at,
            completed_at=db_model.completed_at,
            created_at=db_model.created_at,
            updated_at=db_model.updated_at,
        )

    @staticmethod
    def to_db_model(domain_model: Interview) -> InterviewModel:
        """Convert domain model to database model."""
        return InterviewModel(
            id=domain_model.id,
            candidate_id=domain_model.candidate_id,
            status=domain_model.status.value,
            cv_analysis_id=domain_model.cv_analysis_id,
            question_ids=domain_model.question_ids,
            answer_ids=domain_model.answer_ids,
            current_question_index=domain_model.current_question_index,
            plan_metadata=domain_model.plan_metadata,
            adaptive_follow_ups=domain_model.adaptive_follow_ups,
            current_parent_question_id=domain_model.current_parent_question_id,
            current_followup_count=domain_model.current_followup_count,
            started_at=domain_model.started_at,
            completed_at=domain_model.completed_at,
            created_at=domain_model.created_at,
            updated_at=domain_model.updated_at,
        )

    @staticmethod
    def update_db_model(db_model: InterviewModel, domain_model: Interview) -> None:
        """Update database model from domain model."""
        db_model.status = domain_model.status.value
        db_model.cv_analysis_id = domain_model.cv_analysis_id
        db_model.question_ids = domain_model.question_ids
        db_model.answer_ids = domain_model.answer_ids
        db_model.current_question_index = domain_model.current_question_index
        db_model.plan_metadata = domain_model.plan_metadata
        db_model.adaptive_follow_ups = domain_model.adaptive_follow_ups
        db_model.current_parent_question_id = domain_model.current_parent_question_id
        db_model.current_followup_count = domain_model.current_followup_count
        db_model.started_at = domain_model.started_at
        db_model.completed_at = domain_model.completed_at
        db_model.updated_at = domain_model.updated_at


class AnswerMapper:
    """Mapper for Answer domain model and AnswerModel database model."""

    @staticmethod
    def to_domain(db_model: AnswerModel) -> Answer:
        """Convert database model to domain model."""
        return Answer(
            id=db_model.id,
            interview_id=db_model.interview_id,
            question_id=db_model.question_id,
            candidate_id=db_model.candidate_id,
            text=db_model.text,
            is_voice=db_model.is_voice,
            audio_file_path=db_model.audio_file_path,
            duration_seconds=db_model.duration_seconds,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            metadata=dict(db_model.answer_metadata) if db_model.answer_metadata else {},
            evaluation_id=db_model.evaluation_id,  # NEW: Link to Evaluation entity
            voice_metrics=None,  # Not persisted yet
            created_at=db_model.created_at,
        )

    @staticmethod
    def to_db_model(domain_model: Answer) -> AnswerModel:
        """Convert domain model to database model."""
        return AnswerModel(
            id=domain_model.id,
            interview_id=domain_model.interview_id,
            question_id=domain_model.question_id,
            candidate_id=domain_model.candidate_id,
            text=domain_model.text,
            is_voice=domain_model.is_voice,
            audio_file_path=domain_model.audio_file_path,
            duration_seconds=domain_model.duration_seconds,
            embedding=domain_model.embedding,
            answer_metadata=domain_model.metadata,
            evaluation_id=domain_model.evaluation_id,  # NEW
            created_at=domain_model.created_at,
        )

    @staticmethod
    def update_db_model(db_model: AnswerModel, domain_model: Answer) -> None:
        """Update database model from domain model."""
        db_model.text = domain_model.text
        db_model.is_voice = domain_model.is_voice
        db_model.audio_file_path = domain_model.audio_file_path
        db_model.duration_seconds = domain_model.duration_seconds

        # Convert evaluation to dict if present
        if domain_model.evaluation:
            db_model.evaluation = domain_model.evaluation.model_dump()
        else:
            db_model.evaluation = None

        db_model.embedding = domain_model.embedding
        db_model.answer_metadata = domain_model.metadata
        db_model.similarity_score = domain_model.similarity_score
        db_model.gaps = domain_model.gaps
        db_model.evaluated_at = domain_model.evaluated_at


class CVAnalysisMapper:
    """Mapper for CVAnalysis domain model and CVAnalysisModel database model."""

    @staticmethod
    def to_domain(db_model: CVAnalysisModel) -> CVAnalysis:
        """Convert database model to domain model."""
        # Convert skills from JSONB to ExtractedSkill objects
        skills = []
        if db_model.skills:
            skills = [ExtractedSkill(**skill_dict) for skill_dict in db_model.skills]

        return CVAnalysis(
            id=db_model.id,
            candidate_id=db_model.candidate_id,
            cv_file_path=db_model.cv_file_path,
            extracted_text=db_model.extracted_text,
            skills=skills,
            work_experience_years=db_model.work_experience_years,
            education_level=db_model.education_level,
            suggested_topics=(
                list(db_model.suggested_topics) if db_model.suggested_topics else []
            ),
            suggested_difficulty=db_model.suggested_difficulty,
            embedding=list(db_model.embedding) if db_model.embedding else None,
            summary=db_model.summary,
            metadata=dict(db_model.cv_metadata) if db_model.cv_metadata else {},
            created_at=db_model.created_at,
        )

    @staticmethod
    def to_db_model(domain_model: CVAnalysis) -> CVAnalysisModel:
        """Convert domain model to database model."""
        # Convert ExtractedSkill objects to dicts for JSONB storage
        skills_dicts = [skill.model_dump() for skill in domain_model.skills]

        return CVAnalysisModel(
            id=domain_model.id,
            candidate_id=domain_model.candidate_id,
            cv_file_path=domain_model.cv_file_path,
            extracted_text=domain_model.extracted_text,
            skills=skills_dicts,
            work_experience_years=domain_model.work_experience_years,
            education_level=domain_model.education_level,
            suggested_topics=domain_model.suggested_topics,
            suggested_difficulty=domain_model.suggested_difficulty,
            embedding=domain_model.embedding,
            summary=domain_model.summary,
            cv_metadata=domain_model.metadata,
            created_at=domain_model.created_at,
        )

    @staticmethod
    def update_db_model(db_model: CVAnalysisModel, domain_model: CVAnalysis) -> None:
        """Update database model from domain model."""
        db_model.cv_file_path = domain_model.cv_file_path
        db_model.extracted_text = domain_model.extracted_text
        db_model.skills = [skill.model_dump() for skill in domain_model.skills]
        db_model.work_experience_years = domain_model.work_experience_years
        db_model.education_level = domain_model.education_level
        db_model.suggested_topics = domain_model.suggested_topics
        db_model.suggested_difficulty = domain_model.suggested_difficulty
        db_model.embedding = domain_model.embedding
        db_model.summary = domain_model.summary
        db_model.cv_metadata = domain_model.metadata


class FollowUpQuestionMapper:
    """Mapper for FollowUpQuestion domain model and FollowUpQuestionModel database model."""

    @staticmethod
    def to_domain(db_model: FollowUpQuestionModel) -> FollowUpQuestion:
        """Convert database model to domain model.

        Args:
            db_model: SQLAlchemy model instance

        Returns:
            FollowUpQuestion domain model
        """
        return FollowUpQuestion(
            id=db_model.id,
            parent_question_id=db_model.parent_question_id,
            interview_id=db_model.interview_id,
            text=db_model.text,
            generated_reason=db_model.generated_reason,
            order_in_sequence=db_model.order_in_sequence,
            created_at=db_model.created_at,
        )

    @staticmethod
    def to_db_model(domain_model: FollowUpQuestion) -> FollowUpQuestionModel:
        """Convert domain model to database model.

        Args:
            domain_model: FollowUpQuestion domain model

        Returns:
            FollowUpQuestionModel SQLAlchemy model
        """
        return FollowUpQuestionModel(
            id=domain_model.id,
            parent_question_id=domain_model.parent_question_id,
            interview_id=domain_model.interview_id,
            text=domain_model.text,
            generated_reason=domain_model.generated_reason,
            order_in_sequence=domain_model.order_in_sequence,
            created_at=domain_model.created_at,
        )

    @staticmethod
    def update_db_model(
        db_model: FollowUpQuestionModel, domain_model: FollowUpQuestion
    ) -> None:
        """Update database model from domain model.

        Args:
            db_model: SQLAlchemy model to update
            domain_model: FollowUpQuestion domain model with new data
        """
        db_model.text = domain_model.text
        db_model.generated_reason = domain_model.generated_reason
        db_model.order_in_sequence = domain_model.order_in_sequence
</file>

<file path="src/infrastructure/dependency_injection/container.py">
"""Dependency injection container.

This module wires up all dependencies and provides them to the application.
It's the only place that knows about concrete implementations.
"""

from functools import lru_cache

from sqlalchemy.ext.asyncio import AsyncSession

# Import adapters
from ...adapters.llm.azure_openai_adapter import AzureOpenAIAdapter
from ...adapters.llm.openai_adapter import OpenAIAdapter

# Import mock adapters
from ...adapters.mock import (
    MockAnalyticsAdapter,
    MockCVAnalyzerAdapter,
    MockLLMAdapter,
    MockSTTAdapter,
    MockTTSAdapter,
    MockVectorSearchAdapter,
)

# Import persistence adapters
from ...adapters.persistence import (
    PostgreSQLAnswerRepository,
    PostgreSQLCandidateRepository,
    PostgreSQLCVAnalysisRepository,
    PostgreSQLEvaluationRepository,
    PostgreSQLFollowUpQuestionRepository,
    PostgreSQLInterviewRepository,
    PostgreSQLQuestionRepository,
)
from ...adapters.vector_db.pinecone_adapter import PineconeAdapter
from ...domain.ports import (
    AnalyticsPort,
    AnswerRepositoryPort,
    CandidateRepositoryPort,
    CVAnalysisRepositoryPort,
    CVAnalyzerPort,
    EvaluationRepositoryPort,
    FollowUpQuestionRepositoryPort,
    InterviewRepositoryPort,
    LLMPort,
    QuestionRepositoryPort,
    SpeechToTextPort,
    TextToSpeechPort,
    VectorSearchPort,
)
from ...infrastructure.config.settings import Settings, get_settings


class Container:
    """Dependency injection container.

    This class is responsible for creating and managing all dependencies.
    It follows the dependency inversion principle by depending on ports
    (interfaces) while providing concrete implementations.
    """

    def __init__(self, settings: Settings):
        """Initialize container with settings.

        Args:
            settings: Application settings
        """
        self.settings = settings
        self._llm_port: LLMPort | None = None
        self._vector_search_port: VectorSearchPort | None = None
        self._stt_port: SpeechToTextPort | None = None
        self._tts_port: TextToSpeechPort | None = None

    def llm_port(self) -> LLMPort:
        """Get LLM port implementation.

        Returns:
            Configured LLM port based on settings

        Raises:
            ValueError: If LLM provider is not supported or not configured
        """
        if self._llm_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_llm:
                self._llm_port = MockLLMAdapter()
            elif self.settings.llm_provider == "openai":
                # Check if using Azure OpenAI
                if self.settings.use_azure_openai:
                    if not self.settings.azure_openai_api_key:
                        raise ValueError("Azure OpenAI API key not configured")
                    if not self.settings.azure_openai_endpoint:
                        raise ValueError("Azure OpenAI endpoint not configured")
                    if not self.settings.azure_openai_deployment_name:
                        raise ValueError("Azure OpenAI deployment name not configured")

                    self._llm_port = AzureOpenAIAdapter(
                        api_key=self.settings.azure_openai_api_key,
                        azure_endpoint=self.settings.azure_openai_endpoint,
                        api_version=self.settings.azure_openai_api_version,
                        deployment_name=self.settings.azure_openai_deployment_name,
                        temperature=self.settings.openai_temperature,
                    )
                else:
                    # Standard OpenAI
                    if not self.settings.openai_api_key:
                        raise ValueError("OpenAI API key not configured")

                    self._llm_port = OpenAIAdapter(
                        api_key=self.settings.openai_api_key,
                        model=self.settings.openai_model,
                        temperature=self.settings.openai_temperature,
                    )
            elif self.settings.llm_provider == "claude":
                if not self.settings.anthropic_api_key:
                    raise ValueError("Anthropic API key not configured")

                # Import Claude adapter when implemented
                # from ...adapters.llm.claude_adapter import ClaudeAdapter
                # self._llm_port = ClaudeAdapter(
                #     api_key=self.settings.anthropic_api_key,
                #     model=self.settings.anthropic_model,
                # )
                raise NotImplementedError("Claude adapter not yet implemented")
            else:
                raise ValueError(f"Unsupported LLM provider: {self.settings.llm_provider}")

        return self._llm_port

    def vector_search_port(self) -> VectorSearchPort:
        """Get vector search port implementation.

        Returns:
            Configured vector search port based on settings

        Raises:
            ValueError: If vector DB provider is not supported or not configured
        """
        if self._vector_search_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_vector_search:
                self._vector_search_port = MockVectorSearchAdapter()
            elif self.settings.vector_db_provider == "pinecone":
                if not self.settings.pinecone_api_key:
                    raise ValueError("Pinecone API key not configured")
                if not self.settings.openai_api_key:
                    raise ValueError("OpenAI API key required for embeddings")

                self._vector_search_port = PineconeAdapter(
                    api_key=self.settings.pinecone_api_key,
                    environment=self.settings.pinecone_environment,
                    index_name=self.settings.pinecone_index_name,
                    openai_api_key=self.settings.openai_api_key,
                )
            elif self.settings.vector_db_provider == "weaviate":
                # Import Weaviate adapter when implemented
                # from ...adapters.vector_db.weaviate_adapter import WeaviateAdapter
                # self._vector_search_port = WeaviateAdapter(...)
                raise NotImplementedError("Weaviate adapter not yet implemented")
            elif self.settings.vector_db_provider == "chroma":
                # Import ChromaDB adapter when implemented
                # from ...adapters.vector_db.chroma_adapter import ChromaAdapter
                # self._vector_search_port = ChromaAdapter(...)
                raise NotImplementedError("ChromaDB adapter not yet implemented")
            else:
                raise ValueError(
                    f"Unsupported vector DB provider: {self.settings.vector_db_provider}"
                )

        return self._vector_search_port

    def question_repository_port(self, session: AsyncSession) -> QuestionRepositoryPort:
        """Get question repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured question repository
        """
        return PostgreSQLQuestionRepository(session)

    def follow_up_question_repository(
        self, session: AsyncSession
    ) -> FollowUpQuestionRepositoryPort:
        """Get follow-up question repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured follow-up question repository
        """
        return PostgreSQLFollowUpQuestionRepository(session)

    def candidate_repository_port(self, session: AsyncSession) -> CandidateRepositoryPort:
        """Get candidate repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured candidate repository
        """
        return PostgreSQLCandidateRepository(session)

    def interview_repository_port(self, session: AsyncSession) -> InterviewRepositoryPort:
        """Get interview repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured interview repository
        """
        return PostgreSQLInterviewRepository(session)

    def answer_repository_port(self, session: AsyncSession) -> AnswerRepositoryPort:
        """Get answer repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured answer repository
        """
        return PostgreSQLAnswerRepository(session)

    def evaluation_repository_port(
        self, session: AsyncSession
    ) -> EvaluationRepositoryPort:
        """Get evaluation repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured evaluation repository
        """
        return PostgreSQLEvaluationRepository(session)

    def cv_analysis_repository_port(
        self, session: AsyncSession
    ) -> CVAnalysisRepositoryPort:
        """Get CV analysis repository port implementation.

        Args:
            session: Async database session

        Returns:
            Configured CV analysis repository
        """
        return PostgreSQLCVAnalysisRepository(session)

    def cv_analyzer_port(self) -> CVAnalyzerPort:
        """Get CV analyzer port implementation.

        Returns:
            Configured CV analyzer

        Raises:
            NotImplementedError: Real implementation pending
        """
        if self.settings.use_mock_cv_analyzer:
            return MockCVAnalyzerAdapter()
        else:
            # TODO: Implement real CV analyzer
            # from ...adapters.cv_processing.spacy_cv_analyzer import SpacyCVAnalyzer
            # return SpacyCVAnalyzer(llm_port=self.llm_port())
            raise NotImplementedError("Real CV analyzer not yet implemented")

    def speech_to_text_port(self) -> SpeechToTextPort:
        """Get speech-to-text port implementation.

        Returns:
            Configured STT service

        Raises:
            ValueError: If Azure Speech API key or region is not configured
        """
        if self._stt_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_stt:
                self._stt_port = MockSTTAdapter()
            else:
                # Use Azure Speech SDK
                from ...adapters.speech.azure_stt_adapter import AzureSpeechToTextAdapter

                if not self.settings.azure_speech_key:
                    raise ValueError("Azure Speech API key not configured")
                if not self.settings.azure_speech_region:
                    raise ValueError("Azure Speech region not configured")

                self._stt_port = AzureSpeechToTextAdapter(
                    api_key=self.settings.azure_speech_key,
                    region=self.settings.azure_speech_region,
                    language=self.settings.azure_speech_language,
                )

        return self._stt_port

    def text_to_speech_port(self) -> TextToSpeechPort:
        """Get text-to-speech port implementation.

        Returns:
            Configured TTS service

        Raises:
            ValueError: If Azure Speech API key or region is not configured
        """
        if self._tts_port is None:
            # Use mock adapter if configured
            if self.settings.use_mock_tts:
                self._tts_port = MockTTSAdapter()
            else:
                # Use Azure Speech SDK
                from ...adapters.speech.azure_tts_adapter import AzureTextToSpeechAdapter

                if not self.settings.azure_speech_key:
                    raise ValueError("Azure Speech API key not configured")
                if not self.settings.azure_speech_region:
                    raise ValueError("Azure Speech region not configured")

                self._tts_port = AzureTextToSpeechAdapter(
                    api_key=self.settings.azure_speech_key,
                    region=self.settings.azure_speech_region,
                    default_voice=self.settings.azure_speech_voice,
                    cache_size=self.settings.azure_speech_cache_size,
                )

        return self._tts_port

    def analytics_port(self) -> AnalyticsPort:
        """Get analytics port implementation.

        Returns:
            Configured analytics service

        Raises:
            NotImplementedError: Real implementation pending
        """
        if self.settings.use_mock_analytics:
            return MockAnalyticsAdapter()
        else:
            # TODO: Implement real analytics service
            # from ...adapters.analytics.analytics_adapter import AnalyticsAdapter
            # return AnalyticsAdapter(database_url=self.settings.database_url)
            raise NotImplementedError("Real analytics adapter not yet implemented")


@lru_cache
def get_container() -> Container:
    """Get cached container instance.

    Returns:
        Container instance with all dependencies configured
    """
    settings = get_settings()
    return Container(settings)
</file>

<file path="src/adapters/api/websocket/interview_handler.py">
"""WebSocket handler for interview sessions (orchestrator-based)."""

import logging
from uuid import UUID

from fastapi import WebSocket, WebSocketDisconnect

from ....infrastructure.dependency_injection.container import get_container
from .connection_manager import manager
from .session_orchestrator import InterviewSessionOrchestrator

logger = logging.getLogger(__name__)


async def handle_interview_websocket(
    websocket: WebSocket,
    interview_id: UUID,
):
    """WebSocket handler for interview session (orchestrator-based).

    Uses InterviewSessionOrchestrator to manage state machine lifecycle.

    Protocol:
        Client â†’ Server: { type: "text_answer", question_id: UUID, answer_text: str }
        Server â†’ Client: { type: "evaluation", ... }
        Server â†’ Client: { type: "question", ... }
        Server â†’ Client: { type: "interview_complete", ... }

    Args:
        websocket: WebSocket connection
        interview_id: Interview UUID
    """
    # Connect
    await manager.connect(interview_id, websocket)

    try:
        container = get_container()

        # Create session orchestrator
        orchestrator = InterviewSessionOrchestrator(
            interview_id=interview_id,
            websocket=websocket,
            container=container,
        )

        # Start session (send first question)
        await orchestrator.start_session()

        # Listen for messages
        while True:
            data = await websocket.receive_json()
            message_type = data.get("type")

            if message_type == "text_answer":
                answer_text = data.get("answer_text", "")
                await orchestrator.handle_answer(answer_text)

            elif message_type == "audio_chunk":
                await handle_audio_chunk(interview_id, data, container)

            elif message_type == "get_next_question":
                logger.warning(
                    "get_next_question deprecated - use orchestrator state machine"
                )
                await manager.send_message(
                    interview_id,
                    {
                        "type": "error",
                        "code": "DEPRECATED_MESSAGE_TYPE",
                        "message": "get_next_question deprecated - orchestrator manages flow",
                    },
                )

            else:
                await manager.send_message(
                    interview_id,
                    {
                        "type": "error",
                        "code": "UNKNOWN_MESSAGE_TYPE",
                        "message": f"Unknown message type: {message_type}",
                    },
                )

    except WebSocketDisconnect:
        manager.disconnect(interview_id)
        logger.info(f"Client disconnected from interview {interview_id}")

    except ValueError as e:
        # State machine validation error
        logger.error(f"State machine error for interview {interview_id}: {e}")
        await manager.send_message(
            interview_id,
            {"type": "error", "code": "INVALID_STATE", "message": str(e)},
        )
        manager.disconnect(interview_id)

    except Exception as e:
        logger.error(
            f"WebSocket error for interview {interview_id}: {e}", exc_info=True
        )
        await manager.send_message(
            interview_id,
            {"type": "error", "code": "INTERNAL_ERROR", "message": str(e)},
        )
        manager.disconnect(interview_id)


# Deprecated functions - now handled by InterviewSessionOrchestrator
# Kept for reference during migration period


async def handle_audio_chunk(interview_id: UUID, data: dict, container):
    """Handle audio chunk from client (for voice answers).

    TODO: Integrate with InterviewSessionOrchestrator in Phase 6 (Voice Integration)

    Args:
        interview_id: Interview UUID
        data: Message data with audio chunk
        container: DI container
    """
    # Mock implementation for now
    await manager.send_message(
        interview_id,
        {
            "type": "transcription",
            "text": "[Mock transcription of audio - Phase 6]",
            "is_final": data.get("is_final", False),
        },
    )
</file>

<file path="src/application/use_cases/process_answer_adaptive.py">
"""Process answer with adaptive follow-up logic - Phase 2 complete."""

import logging
from datetime import datetime
from typing import Any
from uuid import UUID

from ...domain.models.answer import Answer
from ...domain.models.evaluation import Evaluation, ConceptGap, GapSeverity, FollowUpEvaluationContext
from ...domain.models.interview import InterviewStatus
from ...domain.models.question import Question
from ...domain.ports.answer_repository_port import AnswerRepositoryPort
from ...domain.ports.evaluation_repository_port import EvaluationRepositoryPort
from ...domain.ports.follow_up_question_repository_port import (
    FollowUpQuestionRepositoryPort,
)
from ...domain.ports.interview_repository_port import InterviewRepositoryPort
from ...domain.ports.llm_port import LLMPort
from ...domain.ports.question_repository_port import QuestionRepositoryPort
from ...domain.ports.vector_search_port import VectorSearchPort

logger = logging.getLogger(__name__)


class ProcessAnswerAdaptiveUseCase:
    """Process answer with adaptive evaluation and follow-up generation.

    Phase 2 complete:
    - Uses separate Evaluation entity with structured gap tracking
    - Context-aware evaluation for follow-up questions
    - Attempt-based penalty system: 0/-5/-15 for attempts 1/2/3
    - Automatic gap resolution when criteria met
    """

    def __init__(
        self,
        answer_repository: AnswerRepositoryPort,
        evaluation_repository: EvaluationRepositoryPort,
        interview_repository: InterviewRepositoryPort,
        question_repository: QuestionRepositoryPort,
        follow_up_question_repository: FollowUpQuestionRepositoryPort,
        llm: LLMPort,
        vector_search: VectorSearchPort,
    ):
        """Initialize use case with required ports.

        Args:
            answer_repository: Answer storage
            evaluation_repository: Evaluation storage (NEW in Phase 1)
            interview_repository: Interview storage
            question_repository: Question storage
            follow_up_question_repository: Follow-up question storage
            llm: LLM service for evaluation and gap detection
            vector_search: Vector database for similarity calculation
        """
        self.answer_repo = answer_repository
        self.evaluation_repo = evaluation_repository
        self.interview_repo = interview_repository
        self.question_repo = question_repository
        self.follow_up_question_repo = follow_up_question_repository
        self.llm = llm
        self.vector_search = vector_search

    async def execute(
        self,
        interview_id: UUID,
        question_id: UUID,
        answer_text: str,
        audio_file_path: str | None = None,
        voice_metrics: dict[str, float] | None = None,
    ) -> tuple[Answer, Evaluation, bool]:
        """Process answer with adaptive evaluation.

        Phase 2: Context-aware evaluation with penalties and gap resolution.

        For main questions (attempt 1):
        - Standard evaluation, no penalty
        - Detects concept gaps

        For follow-up questions (attempts 2-3):
        - Builds context with previous evaluations and cumulative gaps
        - Passes context to LLM for smarter evaluation
        - Applies penalty: -5 (2nd attempt), -15 (3rd attempt)
        - Resolves gaps if: completeness >= 0.8 OR score >= 80 OR attempt == 3

        Args:
            interview_id: The interview UUID
            question_id: The question UUID (main or follow-up)
            answer_text: The answer text
            audio_file_path: Optional audio file path for voice answers
            voice_metrics: Optional voice quality metrics from STT

        Returns:
            Tuple of (Answer, Evaluation, has_more_questions)

        Raises:
            ValueError: If interview or question not found, or invalid state
        """
        logger.info(
            "Processing adaptive answer",
            extra={"interview_id": str(interview_id), "question_id": str(question_id)},
        )

        # Step 1: Validate interview
        interview = await self.interview_repo.get_by_id(interview_id)
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        if interview.status != InterviewStatus.EVALUATING:
            raise ValueError(f"Interview not in progress: {interview.status}")

        # Step 2: Detect if this is a follow-up question
        is_followup, parent_question_id = await self._is_followup_question(question_id)

        # Step 3: Get question (handles both main and follow-up)
        question = await self._get_question(question_id)

        # Step 4: Build follow-up context if applicable
        followup_context = None
        if is_followup and parent_question_id:
            followup_context = await self._build_followup_context(
                question_id=question_id,
                parent_question_id=parent_question_id,
                parent_ideal_answer=question.ideal_answer or "",
            )

        # Step 5: Create answer (simplified - no embedded evaluation)
        answer = Answer(
            interview_id=interview_id,
            question_id=question_id,
            candidate_id=interview.candidate_id,
            text=answer_text,
            is_voice=bool(audio_file_path),
            audio_file_path=audio_file_path,
            voice_metrics=voice_metrics,
            created_at=datetime.utcnow(),
        )

        # Step 6: Evaluate answer using LLM (with follow-up context if applicable)
        llm_eval = await self.llm.evaluate_answer(
            question=question,
            answer_text=answer_text,
            context={
                "interview_id": str(interview_id),
                "candidate_id": str(interview.candidate_id),
            },
            followup_context=followup_context,
        )

        # Step 7: Calculate similarity (if ideal_answer exists)
        similarity_score = None
        if question.has_ideal_answer() and question.ideal_answer:
            similarity_score = await self._calculate_similarity(
                answer_text, question.ideal_answer
            )
            logger.info(f"Similarity score: {similarity_score:.2f}")

        # Step 6: Detect gaps
        gaps_dict = await self._detect_gaps_hybrid(
            answer_text=answer_text,
            ideal_answer=question.ideal_answer or "",
            question_text=question.text,
        )

        # Step 9: Determine attempt number and parent evaluation
        attempt_number = followup_context.attempt_number if followup_context else 1
        parent_evaluation_id = (
            followup_context.previous_evaluations[0].id
            if followup_context and followup_context.previous_evaluations
            else None
        )

        # Step 10: Create Evaluation entity
        evaluation = Evaluation(
            answer_id=answer.id,  # Will link after saving answer
            question_id=question_id,
            interview_id=interview_id,
            raw_score=llm_eval.score,
            penalty=0.0,  # Will be set by apply_penalty()
            final_score=llm_eval.score,  # Will be recalculated by apply_penalty()
            similarity_score=similarity_score,
            completeness=llm_eval.completeness,
            relevance=llm_eval.relevance,
            sentiment=llm_eval.sentiment,
            reasoning=llm_eval.reasoning,
            strengths=llm_eval.strengths,
            weaknesses=llm_eval.weaknesses,
            improvement_suggestions=llm_eval.improvement_suggestions,
            attempt_number=attempt_number,
            parent_evaluation_id=parent_evaluation_id,
            gaps=[
                ConceptGap(
                    evaluation_id=answer.id,  # Temporary, will be updated
                    concept=concept,
                    severity=self._determine_gap_severity(concept, gaps_dict),
                    resolved=False,
                    created_at=datetime.utcnow(),
                )
                for concept in gaps_dict.get("concepts", [])
            ],
            evaluated_at=datetime.utcnow(),
        )

        # Step 11: Apply penalty based on attempt number
        evaluation.apply_penalty(attempt_number)

        # Step 12: Check if gaps should be resolved
        if evaluation.is_gap_resolved_by_criteria():
            evaluation.resolve_gaps()
            logger.info(
                f"Gaps resolved by criteria: completeness={evaluation.completeness:.2f}, "
                f"final_score={evaluation.final_score:.1f}, attempt={attempt_number}"
            )

        # Step 8: Save answer first (to get ID)
        saved_answer = await self.answer_repo.save(answer)

        # Step 9: Update evaluation with correct answer_id
        evaluation.answer_id = saved_answer.id
        for gap in evaluation.gaps:
            gap.evaluation_id = evaluation.id

        # Step 10: Save evaluation
        saved_evaluation = await self.evaluation_repo.save(evaluation)

        # Step 11: Link answer to evaluation
        saved_answer.evaluation_id = saved_evaluation.id
        saved_answer = await self.answer_repo.save(saved_answer)

        # Step 12: Update interview
        interview.add_answer(saved_answer.id)
        await self.interview_repo.update(interview)

        # Step 13: Check if more questions
        has_more = interview.has_more_questions()

        logger.info(
            f"Answer processed: score={evaluation.final_score:.1f}, "
            f"similarity={similarity_score:.2f if similarity_score else 'N/A'}, "
            f"gaps={len(evaluation.gaps)}, has_more={has_more}"
        )

        return saved_answer, saved_evaluation, has_more

    async def _get_question(self, question_id: UUID) -> Question:
        """Get question (main or follow-up).

        Returns parent question for follow-ups (to access ideal_answer).
        Context building happens separately in _build_followup_context().

        Args:
            question_id: Question UUID

        Returns:
            Question entity (parent if follow-up)

        Raises:
            ValueError: If question not found
        """
        # Try main question first
        question = await self.question_repo.get_by_id(question_id)
        if question:
            return question

        # Try follow-up question
        follow_up = await self.follow_up_question_repo.get_by_id(question_id)
        if not follow_up:
            raise ValueError(f"Question {question_id} not found")

        # Get parent question (for ideal_answer)
        parent = await self.question_repo.get_by_id(follow_up.parent_question_id)
        if not parent:
            raise ValueError(f"Parent question {follow_up.parent_question_id} not found")

        return parent

    async def _calculate_similarity(self, answer_text: str, ideal_answer: str) -> float:
        """Calculate cosine similarity between answer and ideal_answer.

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer

        Returns:
            Similarity score (0-1)
        """
        answer_embedding = await self.vector_search.get_embedding(answer_text)
        ideal_embedding = await self.vector_search.get_embedding(ideal_answer)

        similarity = await self.vector_search.find_similar_answers(
            answer_embedding=answer_embedding,
            reference_embeddings=[ideal_embedding],
        )

        return max(0.01, similarity)  # Avoid zero

    async def _detect_gaps_hybrid(
        self, answer_text: str, ideal_answer: str, question_text: str
    ) -> dict[str, Any]:
        """Detect concept gaps using hybrid approach (keywords + LLM).

        Args:
            answer_text: Candidate's answer
            ideal_answer: Reference ideal answer
            question_text: The question asked

        Returns:
            Gaps dict with detected concepts
        """
        # Step 1: Keyword-based gap detection
        keyword_gaps = self._detect_keyword_gaps(answer_text, ideal_answer)

        # Step 2: If keywords detected gaps, confirm with LLM
        if keyword_gaps:
            llm_gaps = await self._detect_gaps_with_llm(
                answer_text=answer_text,
                ideal_answer=ideal_answer,
                question_text=question_text,
                keyword_gaps=keyword_gaps,
            )
            return llm_gaps
        else:
            return {"concepts": [], "confirmed": False, "severity": "minor"}

    def _detect_keyword_gaps(self, answer_text: str, ideal_answer: str) -> list[str]:
        """Fast keyword-based gap detection."""
        stop_words = {
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "from", "is", "are", "was", "were", "be", "been",
            "being", "have", "has", "had", "do", "does", "did", "will", "would",
            "should", "could", "may", "might", "must", "can", "this", "that",
            "these", "those",
        }

        ideal_words = {
            word.lower().strip('.,!?;:"\'-')
            for word in ideal_answer.split()
            if len(word.strip('.,!?;:"\'-')) > 3
            and word.lower().strip('.,!?;:"\'-') not in stop_words
        }

        answer_words = {
            word.lower().strip('.,!?;:"\'-')
            for word in answer_text.split()
            if len(word.strip('.,!?;:"\'-')) > 3
            and word.lower().strip('.,!?;:"\'-') not in stop_words
        }

        missing = list(ideal_words - answer_words)
        return missing if len(missing) > 3 else []

    async def _detect_gaps_with_llm(
        self,
        answer_text: str,
        ideal_answer: str,
        question_text: str,
        keyword_gaps: list[str],
    ) -> dict[str, Any]:
        """Use LLM to confirm and refine gap detection."""
        return await self.llm.detect_concept_gaps(
            answer_text=answer_text,
            ideal_answer=ideal_answer,
            question_text=question_text,
            keyword_gaps=keyword_gaps,
        )

    def _determine_gap_severity(
        self, concept: str, gaps_dict: dict[str, Any]
    ) -> GapSeverity:
        """Determine gap severity from LLM response.

        Args:
            concept: The missing concept
            gaps_dict: Gaps dictionary from LLM

        Returns:
            GapSeverity enum value
        """
        severity_str = gaps_dict.get("severity", "moderate")
        try:
            return GapSeverity(severity_str.lower())
        except ValueError:
            return GapSeverity.MODERATE

    async def _is_followup_question(self, question_id: UUID) -> tuple[bool, UUID | None]:
        """Check if question_id is a follow-up question.

        Args:
            question_id: Question UUID to check

        Returns:
            Tuple of (is_followup, parent_question_id)
        """
        # Try to get from follow_up_question_repository
        follow_up = await self.follow_up_question_repo.get_by_id(question_id)
        if follow_up:
            return True, follow_up.parent_question_id
        return False, None

    async def _build_followup_context(
        self,
        question_id: UUID,
        parent_question_id: UUID,
        parent_ideal_answer: str,
    ) -> FollowUpEvaluationContext:
        """Build follow-up evaluation context.

        Args:
            question_id: Follow-up question UUID
            parent_question_id: Parent question UUID
            parent_ideal_answer: Ideal answer from parent question

        Returns:
            FollowUpEvaluationContext with previous evaluations and gaps
        """
        # Get all answers for parent question (main + follow-ups)
        # We need to find all evaluations related to this parent question
        parent_answer_ids = []

        # Get main question answer
        main_answers = await self.answer_repo.get_by_question_id(parent_question_id)
        parent_answer_ids.extend([a.id for a in main_answers])

        # Get follow-up answers for this parent
        follow_ups = await self.follow_up_question_repo.get_by_parent_question_id(
            parent_question_id
        )
        for fu in follow_ups:
            fu_answers = await self.answer_repo.get_by_question_id(fu.id)
            parent_answer_ids.extend([a.id for a in fu_answers])

        # Get all evaluations for these answers
        previous_evaluations: list[Evaluation] = []
        for answer_id in parent_answer_ids:
            answer = await self.answer_repo.get_by_id(answer_id)
            if answer and answer.evaluation_id:
                evaluation = await self.evaluation_repo.get_by_id(answer.evaluation_id)
                if evaluation:
                    previous_evaluations.append(evaluation)

        # Sort by created_at to maintain order
        previous_evaluations.sort(key=lambda e: e.created_at)

        # Collect all unresolved gaps from previous evaluations
        cumulative_gaps: list[ConceptGap] = []
        for evaluation in previous_evaluations:
            for gap in evaluation.gaps:
                if not gap.resolved:
                    cumulative_gaps.append(gap)

        # Calculate attempt number (1-based index for this follow-up chain)
        attempt_number = len(previous_evaluations) + 1

        # Extract previous scores
        previous_scores = [e.final_score for e in previous_evaluations]

        return FollowUpEvaluationContext(
            parent_question_id=parent_question_id,
            follow_up_question_id=question_id,
            attempt_number=attempt_number,
            previous_evaluations=previous_evaluations,
            cumulative_gaps=cumulative_gaps,
            previous_scores=previous_scores,
            parent_ideal_answer=parent_ideal_answer,
        )
</file>

<file path="src/infrastructure/config/settings.py">
"""Application settings using Pydantic."""

import os
import re
from functools import lru_cache
from pathlib import Path

from dotenv import load_dotenv, find_dotenv
from pydantic_settings import BaseSettings, SettingsConfigDict

# Load environment variables from .env file
# env_path = find_dotenv()
# print(f"âœ… .env file found: {env_path if env_path else 'None'}")
#
# load_dotenv(env_path)


class Settings(BaseSettings):
    """Application settings loaded from environment variables.

    This uses Pydantic for validation and type safety.
    """

    # Application
    app_name: str = "Elios AI Interview Service"
    app_version: str = "0.1.0"
    environment: str = "development"  # development, staging, production
    debug: bool = True

    # API Configuration
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_prefix: str = "/api"

    # LLM Provider Selection
    llm_provider: str = "openai"  # openai, claude, llama

    # OpenAI Configuration
    openai_api_key: str | None = None
    openai_model: str = "gpt-4"
    openai_temperature: float = 0.7

    # Azure OpenAI Configuration (alternative to standard OpenAI)
    azure_openai_api_key: str | None = None
    azure_openai_endpoint: str | None = None  # e.g., "https://your-resource.openai.azure.com/"
    azure_openai_api_version: str = "2024-02-15-preview"
    azure_openai_deployment_name: str | None = None  # Deployment name, not model name
    use_azure_openai: bool = False  # Flag to enable Azure OpenAI

    # Anthropic Claude Configuration (alternative)
    anthropic_api_key: str | None = None
    anthropic_model: str = "claude-3-sonnet-20240229"

    # Vector Database Selection
    vector_db_provider: str = "pinecone"  # pinecone, weaviate, chroma

    # Pinecone Configuration
    pinecone_api_key: str | None = None
    pinecone_environment: str = "us-east-1"
    pinecone_index_name: str = "elios-interviews"

    # PostgreSQL Configuration
    postgres_host: str = "localhost"
    postgres_port: int = 5432
    postgres_user: str = "elios"
    postgres_password: str = ""
    postgres_db: str = "elios_interviews"
    database_url: str | None = None  # Full DATABASE_URL from environment

    @property
    def async_database_url(self) -> str:
        """Generate async PostgreSQL connection URL.

        Converts postgresql:// to postgresql+asyncpg:// for async support.
        Strips out sslmode and channel_binding parameters (not supported by asyncpg).
        If DATABASE_URL is set in environment, use that; otherwise construct from parts.

        Note: For Neon and other cloud PostgreSQL providers, asyncpg handles SSL
        automatically - no explicit SSL parameters needed.
        """
        # First check if DATABASE_URL is provided directly
        db_url = self.database_url or os.getenv("DATABASE_URL")

        if db_url:
            # Convert postgresql:// to postgresql+asyncpg://
            db_url = re.sub(r'^postgresql:', 'postgresql+asyncpg:', db_url)

            # Strip out SSL parameters that asyncpg doesn't support in URL format
            # asyncpg handles SSL automatically for cloud providers like Neon
            db_url = re.sub(r'\?sslmode=[^&]*', '', db_url)  # Remove sslmode param
            db_url = re.sub(r'&sslmode=[^&]*', '', db_url)   # Remove if not first param
            db_url = re.sub(r'\?channel_binding=[^&]*', '', db_url)  # Remove channel_binding
            db_url = re.sub(r'&channel_binding=[^&]*', '', db_url)   # Remove if not first param
            db_url = re.sub(r'\?&', '?', db_url)  # Clean up malformed query string
            db_url = re.sub(r'\?$', '', db_url)   # Remove trailing ?

            return db_url

        # Otherwise construct from individual parts
        return (
            f"postgresql+asyncpg://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
        )

    # Speech Services (Azure Speech SDK)
    azure_speech_key: str | None = None
    azure_speech_region: str = "eastus"
    azure_speech_language: str = "en-US"
    azure_speech_voice: str = "en-US-AriaNeural"
    azure_speech_cache_size: int = 128  # LRU cache size for TTS

    # File Storage
    upload_dir: str = "./uploads"
    cv_dir: str = "./uploads/cvs"
    audio_dir: str = "./uploads/audio"

    # Interview Configuration
    max_questions_per_interview: int = 10
    min_passing_score: float = 60.0
    question_timeout_seconds: int = 300  # 5 minutes per question

    # Logging
    log_level: str = "INFO"
    log_format: str = "json"  # json or text

    # CORS
    cors_origins: list[str] = ["http://localhost:3000", "http://localhost:5173"]

    # WebSocket Configuration
    ws_host: str = "localhost"
    ws_port: int = 8000
    ws_base_url: str = "ws://localhost:8000"

    # Mock Adapters (for development/testing)
    # Individual flags for each adapter - set to False to use real implementations
    use_mock_llm: bool = True
    use_mock_vector_search: bool = True
    use_mock_cv_analyzer: bool = True
    use_mock_stt: bool = True
    use_mock_tts: bool = True
    use_mock_analytics: bool = True

    model_config = SettingsConfigDict(
        env_file=("../.env.local", "../.env", ".env"),  # Try .env.local first, fallback to .env
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )

    def print_loaded_env_file(self):
        for env_file in self.model_config["env_file"]:
            if Path(env_file).exists():
                print(f"Found {env_file} (will be used if values not already set)")
        print(f"Active environment: {self.environment}")

    def is_production(self) -> bool:
        """Check if running in production."""
        return self.environment == "production"

    def is_development(self) -> bool:
        """Check if running in development."""
        return self.environment == "development"


@lru_cache
def get_settings() -> Settings:
    """Get cached settings instance.

    Returns:
        Settings instance
    """

    settings = Settings()
    settings.print_loaded_env_file()

    return settings
</file>

</files>
